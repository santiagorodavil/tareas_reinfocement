{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Reinforcement Learning: Taller 4**\n",
    "## Estudiantes: Juan Pablo Reyes Fajardo y Santiago Rodríguez Ávila "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RL Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion Auxiliar para diccionarios\n",
    "def key_max(d):\n",
    "        return max(d.items(), key=operator.itemgetter(1))\n",
    "def key_min(d):\n",
    "        return min(d.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Política $\\epsilon$ - Greedy \n",
    "\n",
    "(Útil más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q_, state,epsilon=0.1):\n",
    "    rand = np.random.uniform()\n",
    "    if rand>epsilon:\n",
    "        return key_max(Q_[state])[0],1-epsilon\n",
    "    else:\n",
    "        return key_min(Q_[state])[0],epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretización\n",
    "\n",
    "Inicialmente se realizan múltiples experimentos para determinar límites razonables para las variables a discretizar (aquellas cuyo espacio de observación es infinito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocidades_absolutas_maximas={\"Lineal\":[],\"Angular\":[]}\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "abs_lineal=abs(observation[1])\n",
    "abs_angular=abs(observation[3])\n",
    "\n",
    "for _ in range(int(1e6)):\n",
    "    \n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if abs(observation[1])>abs_lineal:\n",
    "        abs_lineal=abs(observation[1])\n",
    "    if abs(observation[3])>abs_angular:\n",
    "        abs_angular=abs(observation[3])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        velocidades_absolutas_maximas[\"Lineal\"].append(abs_lineal)\n",
    "        velocidades_absolutas_maximas[\"Angular\"].append(abs_angular)\n",
    "\n",
    "env.close()\n",
    "\n",
    "vel_lin_abs_max=np.mean(velocidades_absolutas_maximas[\"Lineal\"])\n",
    "vel_ang_abs_max=np.mean(velocidades_absolutas_maximas[\"Angular\"])\n",
    "print(f'Promedio de velocidad lineal absoluta máxima: \\\n",
    "      {vel_lin_abs_max} \\\n",
    "      \\nPromedio de velocidad angular absoluta máxima: \\\n",
    "      {vel_ang_abs_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Límites del espacio de observación del MDP real\n",
    "cart_high_var = env.observation_space.high\n",
    "cart_low_var = env.observation_space.low\n",
    "\n",
    "# Espacios de observación discretizados\n",
    "observation_space_discrete_400=[np.linspace(cart_low_var[0], cart_high_var[0], num= 5),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 4),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 5),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 4)]\\\n",
    "\n",
    "observation_space_discrete_4096=[np.linspace(cart_low_var[0], cart_high_var[0], num= 8),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 8),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 8),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 8)]\n",
    "\n",
    "# Uso de iteradores para obtener todos los estados a partir del espacio de obsevación discreto\n",
    "states_400=list(itertools.product(*observation_space_discrete_400))\n",
    "states_4096=list(itertools.product(*observation_space_discrete_4096))\n",
    "\n",
    "def init_Q_400():\n",
    "    Q_table={}\n",
    "    for i in states_400:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "\n",
    "def init_Q_4096():\n",
    "    Q_table={}\n",
    "    for i in states_4096:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "         \n",
    "def discretize_400(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_400[i]]\n",
    "        discretizacion[i] = observation_space_discrete_400[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)\n",
    "\n",
    "def discretize_4096(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_4096[i]]\n",
    "        discretizacion[i] = observation_space_discrete_4096[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de Q con Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 Estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_400(gamma,alpha):\n",
    "    terminated=False\n",
    "    observation, info = env.reset()\n",
    "    observation=discretize_400(observation)\n",
    "    while not terminated:\n",
    "        \n",
    "        action,_=eps_greedy(Q,observation,0.1)\n",
    "        \n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        observation_=discretize_400(observation_)\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode='human')\n",
    "Q=init_Q_400()\n",
    "policy=dict.fromkeys(states_400, 0)\n",
    "for i in range(0,10):\n",
    "    Q_Learning_400(0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_4096(gamma,alpha):\n",
    "    terminated=False\n",
    "    observation, info = env.reset()\n",
    "    observation=discretize_4096(observation)\n",
    "    while not terminated:\n",
    "        \n",
    "        action,_=eps_greedy(Q,observation,0.1)\n",
    "        \n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        observation_=discretize_4096(observation_)\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "Q=init_Q_4096()\n",
    "policy=dict.fromkeys(states_4096, 0)\n",
    "    \n",
    "for i in tqdm(range(int(1e6))):\n",
    "    Q_Learning_4096(0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state_4096 in states_4096:\n",
    "    policy[state_4096],_=eps_greedy(Q,tuple(state_4096),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "\n",
    "\n",
    "rwds=[]\n",
    "for _ in range(100):\n",
    "    observation, info = env.reset()\n",
    "    r=0\n",
    "    while True:\n",
    "        action = policy[discretize_4096(observation)]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        r+=reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            rwds.append(r)\n",
    "            break\n",
    "env.close()\n",
    "print(np.mean(rwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100 steps ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44036\\1937588419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mvisualkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayered_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         inputs, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1304\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m         )\n\u001b[0;32m   1306\u001b[0m         \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m         )\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2694\u001b[0m                 \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2696\u001b[1;33m                 \u001b[0mexception_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2697\u001b[0m             )\n\u001b[0;32m   2698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    737\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[1;34m\" but got array with shape \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                         )\n\u001b[0;32m    741\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(2, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=100, visualize=True, verbose=2)\n",
    "\n",
    "visualkeras.layered_view(model)\n",
    "\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36332\\2881083884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python --version'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NumPy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "!python --version\n",
    "print('NumPy', np.__version__)\n",
    "print('Tensorflow', tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: 82.0\n",
      "    82/100000: episode: 1, duration: 1.317s, episode steps: 82, steps per second: 62, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.042 [-0.402, 0.731], loss: 0.457013, mean_absolute_error: 0.505343, mean_q: 0.056611\n",
      "Episode 2 reward: 33.0\n",
      "   115/100000: episode: 2, duration: 0.187s, episode steps: 33, steps per second: 176, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.157 [-0.151, 1.048], loss: 0.404588, mean_absolute_error: 0.475813, mean_q: 0.115570\n",
      "Episode 3 reward: 52.0\n",
      "   167/100000: episode: 3, duration: 0.335s, episode steps: 52, steps per second: 155, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.082 [-0.299, 1.016], loss: 0.384792, mean_absolute_error: 0.473673, mean_q: 0.182906\n",
      "Episode 4 reward: 41.0\n",
      "   208/100000: episode: 4, duration: 0.266s, episode steps: 41, steps per second: 154, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.091 [-0.222, 0.764], loss: 0.370169, mean_absolute_error: 0.485515, mean_q: 0.269427\n",
      "Episode 5 reward: 56.0\n",
      "   264/100000: episode: 5, duration: 0.309s, episode steps: 56, steps per second: 181, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.126 [-0.759, 0.286], loss: 0.356575, mean_absolute_error: 0.504947, mean_q: 0.365103\n",
      "Episode 6 reward: 40.0\n",
      "   304/100000: episode: 6, duration: 0.226s, episode steps: 40, steps per second: 177, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.196, 1.035], loss: 0.335625, mean_absolute_error: 0.528928, mean_q: 0.479357\n",
      "Episode 7 reward: 43.0\n",
      "   347/100000: episode: 7, duration: 0.241s, episode steps: 43, steps per second: 179, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.078 [-0.348, 0.838], loss: 0.317623, mean_absolute_error: 0.559418, mean_q: 0.586636\n",
      "Episode 8 reward: 42.0\n",
      "   389/100000: episode: 8, duration: 0.240s, episode steps: 42, steps per second: 175, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.084 [-0.233, 1.030], loss: 0.291186, mean_absolute_error: 0.598305, mean_q: 0.715429\n",
      "Episode 9 reward: 38.0\n",
      "   427/100000: episode: 9, duration: 0.223s, episode steps: 38, steps per second: 170, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.100 [-0.249, 0.798], loss: 0.251894, mean_absolute_error: 0.634544, mean_q: 0.861490\n",
      "Episode 10 reward: 30.0\n",
      "   457/100000: episode: 10, duration: 0.171s, episode steps: 30, steps per second: 176, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.102 [-0.407, 0.688], loss: 0.233812, mean_absolute_error: 0.686988, mean_q: 1.003904\n",
      "Episode 11 reward: 44.0\n",
      "   501/100000: episode: 11, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.089 [-0.417, 0.731], loss: 0.203271, mean_absolute_error: 0.742617, mean_q: 1.171461\n",
      "Episode 12 reward: 47.0\n",
      "   548/100000: episode: 12, duration: 0.267s, episode steps: 47, steps per second: 176, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.082 [-0.349, 0.817], loss: 0.179066, mean_absolute_error: 0.829654, mean_q: 1.389864\n",
      "Episode 13 reward: 54.0\n",
      "   602/100000: episode: 13, duration: 0.308s, episode steps: 54, steps per second: 175, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.541, 0.838], loss: 0.169004, mean_absolute_error: 0.946398, mean_q: 1.632324\n",
      "Episode 14 reward: 49.0\n",
      "   651/100000: episode: 14, duration: 0.282s, episode steps: 49, steps per second: 174, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.058 [-0.384, 0.816], loss: 0.153078, mean_absolute_error: 1.060400, mean_q: 1.909128\n",
      "Episode 15 reward: 31.0\n",
      "   682/100000: episode: 15, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.082 [-0.404, 0.759], loss: 0.146522, mean_absolute_error: 1.174608, mean_q: 2.144051\n",
      "Episode 16 reward: 24.0\n",
      "   706/100000: episode: 16, duration: 0.141s, episode steps: 24, steps per second: 170, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.405, 1.131], loss: 0.139223, mean_absolute_error: 1.258622, mean_q: 2.325916\n",
      "Episode 17 reward: 58.0\n",
      "   764/100000: episode: 17, duration: 0.323s, episode steps: 58, steps per second: 180, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.370, 0.958], loss: 0.164662, mean_absolute_error: 1.388662, mean_q: 2.577227\n",
      "Episode 18 reward: 55.0\n",
      "   819/100000: episode: 18, duration: 0.308s, episode steps: 55, steps per second: 178, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.038 [-0.591, 1.035], loss: 0.168170, mean_absolute_error: 1.552694, mean_q: 2.931855\n",
      "Episode 19 reward: 25.0\n",
      "   844/100000: episode: 19, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.104 [-0.383, 0.982], loss: 0.192574, mean_absolute_error: 1.693461, mean_q: 3.187111\n",
      "Episode 20 reward: 19.0\n",
      "   863/100000: episode: 20, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.099 [-0.583, 0.937], loss: 0.200099, mean_absolute_error: 1.763675, mean_q: 3.341580\n",
      "Episode 21 reward: 16.0\n",
      "   879/100000: episode: 21, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.379, 0.840], loss: 0.189889, mean_absolute_error: 1.822412, mean_q: 3.504950\n",
      "Episode 22 reward: 20.0\n",
      "   899/100000: episode: 22, duration: 0.148s, episode steps: 20, steps per second: 135, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.419, 0.983], loss: 0.188839, mean_absolute_error: 1.870181, mean_q: 3.617026\n",
      "Episode 23 reward: 16.0\n",
      "   915/100000: episode: 23, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.118 [-0.377, 1.147], loss: 0.201506, mean_absolute_error: 1.943288, mean_q: 3.765450\n",
      "Episode 24 reward: 18.0\n",
      "   933/100000: episode: 24, duration: 0.126s, episode steps: 18, steps per second: 142, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.092 [-0.778, 1.445], loss: 0.271828, mean_absolute_error: 2.044582, mean_q: 3.941034\n",
      "Episode 25 reward: 24.0\n",
      "   957/100000: episode: 25, duration: 0.142s, episode steps: 24, steps per second: 169, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.048 [-0.427, 1.068], loss: 0.373300, mean_absolute_error: 2.155244, mean_q: 4.105679\n",
      "Episode 26 reward: 12.0\n",
      "   969/100000: episode: 26, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.111 [-0.583, 1.228], loss: 0.228514, mean_absolute_error: 2.190855, mean_q: 4.256942\n",
      "Episode 27 reward: 18.0\n",
      "   987/100000: episode: 27, duration: 0.104s, episode steps: 18, steps per second: 174, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.060 [-0.610, 1.165], loss: 0.303589, mean_absolute_error: 2.274472, mean_q: 4.411596\n",
      "Episode 28 reward: 13.0\n",
      "  1000/100000: episode: 28, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.090 [-0.649, 1.288], loss: 0.436419, mean_absolute_error: 2.372814, mean_q: 4.603442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 reward: 15.0\n",
      "  1015/100000: episode: 29, duration: 0.087s, episode steps: 15, steps per second: 171, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.115 [-0.555, 1.244], loss: 0.354892, mean_absolute_error: 2.394433, mean_q: 4.705206\n",
      "Episode 30 reward: 15.0\n",
      "  1030/100000: episode: 30, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.102 [-0.553, 1.169], loss: 0.428847, mean_absolute_error: 2.465823, mean_q: 4.823096\n",
      "Episode 31 reward: 17.0\n",
      "  1047/100000: episode: 31, duration: 0.096s, episode steps: 17, steps per second: 176, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.070 [-0.819, 1.331], loss: 0.468106, mean_absolute_error: 2.566093, mean_q: 4.933160\n",
      "Episode 32 reward: 13.0\n",
      "  1060/100000: episode: 32, duration: 0.093s, episode steps: 13, steps per second: 139, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.094 [-0.647, 1.306], loss: 0.545484, mean_absolute_error: 2.626122, mean_q: 5.069242\n",
      "Episode 33 reward: 16.0\n",
      "  1076/100000: episode: 33, duration: 0.093s, episode steps: 16, steps per second: 172, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.577, 1.220], loss: 0.688816, mean_absolute_error: 2.720528, mean_q: 5.192471\n",
      "Episode 34 reward: 12.0\n",
      "  1088/100000: episode: 34, duration: 0.072s, episode steps: 12, steps per second: 166, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.125 [-0.744, 1.522], loss: 0.432813, mean_absolute_error: 2.761365, mean_q: 5.313078\n",
      "Episode 35 reward: 12.0\n",
      "  1100/100000: episode: 35, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.107 [-0.617, 1.133], loss: 0.626869, mean_absolute_error: 2.836241, mean_q: 5.467601\n",
      "Episode 36 reward: 14.0\n",
      "  1114/100000: episode: 36, duration: 0.083s, episode steps: 14, steps per second: 168, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.084 [-0.814, 1.448], loss: 0.570953, mean_absolute_error: 2.869338, mean_q: 5.540588\n",
      "Episode 37 reward: 15.0\n",
      "  1129/100000: episode: 37, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-0.971, 1.750], loss: 0.617190, mean_absolute_error: 2.962817, mean_q: 5.744807\n",
      "Episode 38 reward: 10.0\n",
      "  1139/100000: episode: 38, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-0.772, 1.472], loss: 1.024627, mean_absolute_error: 3.119527, mean_q: 5.853378\n",
      "Episode 39 reward: 15.0\n",
      "  1154/100000: episode: 39, duration: 0.094s, episode steps: 15, steps per second: 160, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-0.793, 1.395], loss: 0.728393, mean_absolute_error: 3.061856, mean_q: 5.828655\n",
      "Episode 40 reward: 15.0\n",
      "  1169/100000: episode: 40, duration: 0.083s, episode steps: 15, steps per second: 181, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.613, 1.249], loss: 0.547559, mean_absolute_error: 3.118985, mean_q: 6.000878\n",
      "Episode 41 reward: 11.0\n",
      "  1180/100000: episode: 41, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.131 [-0.597, 1.259], loss: 0.792986, mean_absolute_error: 3.215531, mean_q: 6.146585\n",
      "Episode 42 reward: 13.0\n",
      "  1193/100000: episode: 42, duration: 0.089s, episode steps: 13, steps per second: 146, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.089 [-0.840, 1.448], loss: 0.696259, mean_absolute_error: 3.184865, mean_q: 6.220067\n",
      "Episode 43 reward: 11.0\n",
      "  1204/100000: episode: 43, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.144 [-0.754, 1.493], loss: 0.572654, mean_absolute_error: 3.300560, mean_q: 6.422394\n",
      "Episode 44 reward: 13.0\n",
      "  1217/100000: episode: 44, duration: 0.089s, episode steps: 13, steps per second: 147, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.750, 1.376], loss: 0.927710, mean_absolute_error: 3.389425, mean_q: 6.547160\n",
      "Episode 45 reward: 11.0\n",
      "  1228/100000: episode: 45, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.130 [-0.750, 1.421], loss: 0.587833, mean_absolute_error: 3.416085, mean_q: 6.654301\n",
      "Episode 46 reward: 10.0\n",
      "  1238/100000: episode: 46, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.145 [-0.756, 1.486], loss: 1.061575, mean_absolute_error: 3.508929, mean_q: 6.776745\n",
      "Episode 47 reward: 12.0\n",
      "  1250/100000: episode: 47, duration: 0.094s, episode steps: 12, steps per second: 128, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.134 [-0.754, 1.482], loss: 1.012303, mean_absolute_error: 3.525768, mean_q: 6.812667\n",
      "Episode 48 reward: 11.0\n",
      "  1261/100000: episode: 48, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.100 [-1.019, 1.728], loss: 0.836060, mean_absolute_error: 3.563030, mean_q: 6.977400\n",
      "Episode 49 reward: 10.0\n",
      "  1271/100000: episode: 49, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-0.989, 1.628], loss: 1.042871, mean_absolute_error: 3.624328, mean_q: 7.051842\n",
      "Episode 50 reward: 13.0\n",
      "  1284/100000: episode: 50, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.082 [-1.007, 1.684], loss: 0.919763, mean_absolute_error: 3.652867, mean_q: 7.101908\n",
      "Episode 51 reward: 11.0\n",
      "  1295/100000: episode: 51, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-0.951, 1.695], loss: 0.702186, mean_absolute_error: 3.741850, mean_q: 7.305779\n",
      "Episode 52 reward: 13.0\n",
      "  1308/100000: episode: 52, duration: 0.092s, episode steps: 13, steps per second: 141, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.090 [-1.017, 1.657], loss: 1.227448, mean_absolute_error: 3.835110, mean_q: 7.405396\n",
      "Episode 53 reward: 15.0\n",
      "  1323/100000: episode: 53, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.091 [-1.028, 1.748], loss: 1.260143, mean_absolute_error: 3.874955, mean_q: 7.466963\n",
      "Episode 54 reward: 12.0\n",
      "  1335/100000: episode: 54, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-0.990, 1.689], loss: 1.695327, mean_absolute_error: 4.007731, mean_q: 7.575498\n",
      "Episode 55 reward: 10.0\n",
      "  1345/100000: episode: 55, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.113 [-0.963, 1.640], loss: 1.414285, mean_absolute_error: 4.039093, mean_q: 7.651923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 56 reward: 12.0\n",
      "  1357/100000: episode: 56, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.090 [-1.011, 1.621], loss: 2.073461, mean_absolute_error: 4.172506, mean_q: 7.859346\n",
      "Episode 57 reward: 11.0\n",
      "  1368/100000: episode: 57, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-1.000, 1.734], loss: 1.386971, mean_absolute_error: 4.053177, mean_q: 7.793995\n",
      "Episode 58 reward: 11.0\n",
      "  1379/100000: episode: 58, duration: 0.073s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-0.956, 1.704], loss: 1.299281, mean_absolute_error: 4.117334, mean_q: 7.889252\n",
      "Episode 59 reward: 9.0\n",
      "  1388/100000: episode: 59, duration: 0.055s, episode steps: 9, steps per second: 162, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.125 [-1.023, 1.756], loss: 0.924447, mean_absolute_error: 4.097021, mean_q: 8.011242\n",
      "Episode 60 reward: 15.0\n",
      "  1403/100000: episode: 60, duration: 0.086s, episode steps: 15, steps per second: 174, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.088 [-1.028, 1.736], loss: 1.531819, mean_absolute_error: 4.264658, mean_q: 8.171691\n",
      "Episode 61 reward: 10.0\n",
      "  1413/100000: episode: 61, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.100 [-0.994, 1.596], loss: 2.121275, mean_absolute_error: 4.355541, mean_q: 8.185969\n",
      "Episode 62 reward: 13.0\n",
      "  1426/100000: episode: 62, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-1.173, 1.949], loss: 2.350205, mean_absolute_error: 4.432541, mean_q: 8.320570\n",
      "Episode 63 reward: 14.0\n",
      "  1440/100000: episode: 63, duration: 0.090s, episode steps: 14, steps per second: 156, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-0.941, 1.573], loss: 2.261594, mean_absolute_error: 4.516861, mean_q: 8.384892\n",
      "Episode 64 reward: 12.0\n",
      "  1452/100000: episode: 64, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.940, 1.630], loss: 2.717377, mean_absolute_error: 4.513979, mean_q: 8.336509\n",
      "Episode 65 reward: 13.0\n",
      "  1465/100000: episode: 65, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-0.999, 1.699], loss: 2.122046, mean_absolute_error: 4.482491, mean_q: 8.424798\n",
      "Episode 66 reward: 12.0\n",
      "  1477/100000: episode: 66, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-1.155, 1.796], loss: 2.145446, mean_absolute_error: 4.512969, mean_q: 8.510081\n",
      "Episode 67 reward: 13.0\n",
      "  1490/100000: episode: 67, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-0.989, 1.701], loss: 1.963839, mean_absolute_error: 4.572457, mean_q: 8.641608\n",
      "Episode 68 reward: 10.0\n",
      "  1500/100000: episode: 68, duration: 0.062s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.133 [-0.962, 1.676], loss: 1.847092, mean_absolute_error: 4.592325, mean_q: 8.737643\n",
      "Episode 69 reward: 11.0\n",
      "  1511/100000: episode: 69, duration: 0.067s, episode steps: 11, steps per second: 163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.104 [-1.160, 1.798], loss: 2.631041, mean_absolute_error: 4.729171, mean_q: 8.837165\n",
      "Episode 70 reward: 12.0\n",
      "  1523/100000: episode: 70, duration: 0.070s, episode steps: 12, steps per second: 171, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.126 [-0.968, 1.659], loss: 1.555063, mean_absolute_error: 4.668921, mean_q: 8.884070\n",
      "Episode 71 reward: 10.0\n",
      "  1533/100000: episode: 71, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.124 [-1.029, 1.695], loss: 1.657945, mean_absolute_error: 4.777484, mean_q: 9.052954\n",
      "Episode 72 reward: 9.0\n",
      "  1542/100000: episode: 72, duration: 0.054s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-0.955, 1.756], loss: 2.392484, mean_absolute_error: 4.831968, mean_q: 9.121515\n",
      "Episode 73 reward: 11.0\n",
      "  1553/100000: episode: 73, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.094 [-1.029, 1.701], loss: 2.538006, mean_absolute_error: 4.839483, mean_q: 9.146717\n",
      "Episode 74 reward: 9.0\n",
      "  1562/100000: episode: 74, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.149 [-0.950, 1.735], loss: 1.032995, mean_absolute_error: 4.717856, mean_q: 9.217475\n",
      "Episode 75 reward: 10.0\n",
      "  1572/100000: episode: 75, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.122 [-0.961, 1.627], loss: 2.018270, mean_absolute_error: 4.806091, mean_q: 9.272251\n",
      "Episode 76 reward: 11.0\n",
      "  1583/100000: episode: 76, duration: 0.083s, episode steps: 11, steps per second: 133, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.149 [-0.943, 1.749], loss: 1.541434, mean_absolute_error: 4.911952, mean_q: 9.431992\n",
      "Episode 77 reward: 13.0\n",
      "  1596/100000: episode: 77, duration: 0.087s, episode steps: 13, steps per second: 149, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.098 [-0.960, 1.651], loss: 2.138624, mean_absolute_error: 5.023127, mean_q: 9.579592\n",
      "Episode 78 reward: 11.0\n",
      "  1607/100000: episode: 78, duration: 0.061s, episode steps: 11, steps per second: 180, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.136, 1.953], loss: 2.262796, mean_absolute_error: 5.056812, mean_q: 9.622674\n",
      "Episode 79 reward: 11.0\n",
      "  1618/100000: episode: 79, duration: 0.069s, episode steps: 11, steps per second: 160, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-0.951, 1.728], loss: 2.368849, mean_absolute_error: 5.081478, mean_q: 9.616058\n",
      "Episode 80 reward: 12.0\n",
      "  1630/100000: episode: 80, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.121 [-0.942, 1.689], loss: 2.950932, mean_absolute_error: 5.204885, mean_q: 9.655078\n",
      "Episode 81 reward: 12.0\n",
      "  1642/100000: episode: 81, duration: 0.069s, episode steps: 12, steps per second: 174, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-0.972, 1.721], loss: 2.746459, mean_absolute_error: 5.181338, mean_q: 9.683318\n",
      "Episode 82 reward: 12.0\n",
      "  1654/100000: episode: 82, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.110 [-0.940, 1.628], loss: 2.795881, mean_absolute_error: 5.227440, mean_q: 9.703387\n",
      "Episode 83 reward: 13.0\n",
      "  1667/100000: episode: 83, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-1.002, 1.564], loss: 2.618569, mean_absolute_error: 5.289851, mean_q: 9.803687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 84 reward: 12.0\n",
      "  1679/100000: episode: 84, duration: 0.085s, episode steps: 12, steps per second: 142, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.748, 1.441], loss: 2.563508, mean_absolute_error: 5.267357, mean_q: 9.861629\n",
      "Episode 85 reward: 11.0\n",
      "  1690/100000: episode: 85, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.800, 1.389], loss: 2.727736, mean_absolute_error: 5.295441, mean_q: 9.917642\n",
      "Episode 86 reward: 11.0\n",
      "  1701/100000: episode: 86, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.107 [-0.966, 1.461], loss: 2.457676, mean_absolute_error: 5.329578, mean_q: 9.991495\n",
      "Episode 87 reward: 16.0\n",
      "  1717/100000: episode: 87, duration: 0.120s, episode steps: 16, steps per second: 133, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.099 [-0.938, 1.683], loss: 2.822702, mean_absolute_error: 5.380773, mean_q: 10.075325\n",
      "Episode 88 reward: 16.0\n",
      "  1733/100000: episode: 88, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.070 [-0.826, 1.429], loss: 3.450948, mean_absolute_error: 5.437539, mean_q: 10.050990\n",
      "Episode 89 reward: 13.0\n",
      "  1746/100000: episode: 89, duration: 0.083s, episode steps: 13, steps per second: 157, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.108 [-0.815, 1.421], loss: 4.282982, mean_absolute_error: 5.570270, mean_q: 10.070485\n",
      "Episode 90 reward: 16.0\n",
      "  1762/100000: episode: 90, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.083 [-0.752, 1.443], loss: 3.677858, mean_absolute_error: 5.555329, mean_q: 10.028099\n",
      "Episode 91 reward: 14.0\n",
      "  1776/100000: episode: 91, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.091 [-0.928, 1.470], loss: 4.223368, mean_absolute_error: 5.613404, mean_q: 10.056100\n",
      "Episode 92 reward: 14.0\n",
      "  1790/100000: episode: 92, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.098 [-0.815, 1.445], loss: 2.512457, mean_absolute_error: 5.459147, mean_q: 10.082144\n",
      "Episode 93 reward: 16.0\n",
      "  1806/100000: episode: 93, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.795, 1.451], loss: 2.458272, mean_absolute_error: 5.487459, mean_q: 10.259023\n",
      "Episode 94 reward: 13.0\n",
      "  1819/100000: episode: 94, duration: 0.080s, episode steps: 13, steps per second: 162, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-0.998, 1.481], loss: 3.363898, mean_absolute_error: 5.594447, mean_q: 10.349372\n",
      "Episode 95 reward: 10.0\n",
      "  1829/100000: episode: 95, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.145 [-0.757, 1.552], loss: 3.123234, mean_absolute_error: 5.620933, mean_q: 10.415875\n",
      "Episode 96 reward: 13.0\n",
      "  1842/100000: episode: 96, duration: 0.084s, episode steps: 13, steps per second: 155, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.118 [-0.745, 1.423], loss: 2.907848, mean_absolute_error: 5.582767, mean_q: 10.408122\n",
      "Episode 97 reward: 15.0\n",
      "  1857/100000: episode: 97, duration: 0.087s, episode steps: 15, steps per second: 173, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-0.804, 1.410], loss: 3.555556, mean_absolute_error: 5.684992, mean_q: 10.510613\n",
      "Episode 98 reward: 12.0\n",
      "  1869/100000: episode: 98, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.765, 1.534], loss: 3.712190, mean_absolute_error: 5.728415, mean_q: 10.505550\n",
      "Episode 99 reward: 12.0\n",
      "  1881/100000: episode: 99, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-0.802, 1.477], loss: 3.991927, mean_absolute_error: 5.738081, mean_q: 10.476478\n",
      "Episode 100 reward: 14.0\n",
      "  1895/100000: episode: 100, duration: 0.083s, episode steps: 14, steps per second: 169, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.068 [-0.833, 1.418], loss: 3.803463, mean_absolute_error: 5.804444, mean_q: 10.464974\n",
      "Episode 101 reward: 14.0\n",
      "  1909/100000: episode: 101, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.081 [-0.785, 1.429], loss: 2.874913, mean_absolute_error: 5.668701, mean_q: 10.497721\n",
      "Episode 102 reward: 15.0\n",
      "  1924/100000: episode: 102, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.796, 1.263], loss: 3.737946, mean_absolute_error: 5.801291, mean_q: 10.596434\n",
      "Episode 103 reward: 16.0\n",
      "  1940/100000: episode: 103, duration: 0.099s, episode steps: 16, steps per second: 162, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.089 [-0.773, 1.185], loss: 2.850656, mean_absolute_error: 5.787761, mean_q: 10.628527\n",
      "Episode 104 reward: 12.0\n",
      "  1952/100000: episode: 104, duration: 0.077s, episode steps: 12, steps per second: 157, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.104 [-0.597, 1.192], loss: 3.106996, mean_absolute_error: 5.806484, mean_q: 10.737145\n",
      "Episode 105 reward: 18.0\n",
      "  1970/100000: episode: 105, duration: 0.116s, episode steps: 18, steps per second: 156, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-0.616, 1.181], loss: 2.970740, mean_absolute_error: 5.802519, mean_q: 10.778113\n",
      "Episode 106 reward: 18.0\n",
      "  1988/100000: episode: 106, duration: 0.104s, episode steps: 18, steps per second: 173, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-0.606, 1.185], loss: 3.387106, mean_absolute_error: 5.884642, mean_q: 10.786449\n",
      "Episode 107 reward: 16.0\n",
      "  2004/100000: episode: 107, duration: 0.095s, episode steps: 16, steps per second: 168, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.583, 1.190], loss: 3.277803, mean_absolute_error: 5.906805, mean_q: 10.853665\n",
      "Episode 108 reward: 16.0\n",
      "  2020/100000: episode: 108, duration: 0.104s, episode steps: 16, steps per second: 154, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.112 [-0.555, 1.196], loss: 3.146471, mean_absolute_error: 5.889142, mean_q: 10.873797\n",
      "Episode 109 reward: 11.0\n",
      "  2031/100000: episode: 109, duration: 0.067s, episode steps: 11, steps per second: 164, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.123 [-0.566, 1.277], loss: 2.581343, mean_absolute_error: 5.836013, mean_q: 10.924934\n",
      "Episode 110 reward: 14.0\n",
      "  2045/100000: episode: 110, duration: 0.077s, episode steps: 14, steps per second: 181, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.086 [-0.596, 1.180], loss: 3.101325, mean_absolute_error: 5.914767, mean_q: 11.011175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 111 reward: 14.0\n",
      "  2059/100000: episode: 111, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.088 [-0.793, 1.210], loss: 2.325088, mean_absolute_error: 5.860105, mean_q: 11.043200\n",
      "Episode 112 reward: 18.0\n",
      "  2077/100000: episode: 112, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.080 [-0.578, 1.157], loss: 3.917350, mean_absolute_error: 6.043012, mean_q: 11.127014\n",
      "Episode 113 reward: 14.0\n",
      "  2091/100000: episode: 113, duration: 0.080s, episode steps: 14, steps per second: 174, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.113 [-0.572, 1.179], loss: 3.166391, mean_absolute_error: 5.999855, mean_q: 11.106357\n",
      "Episode 114 reward: 12.0\n",
      "  2103/100000: episode: 114, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.633, 1.210], loss: 3.439651, mean_absolute_error: 5.996755, mean_q: 11.120112\n",
      "Episode 115 reward: 12.0\n",
      "  2115/100000: episode: 115, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.822, 1.309], loss: 2.974118, mean_absolute_error: 6.027582, mean_q: 11.133819\n",
      "Episode 116 reward: 12.0\n",
      "  2127/100000: episode: 116, duration: 0.076s, episode steps: 12, steps per second: 159, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.113 [-0.627, 1.200], loss: 2.675881, mean_absolute_error: 6.026154, mean_q: 11.163524\n",
      "Episode 117 reward: 18.0\n",
      "  2145/100000: episode: 117, duration: 0.112s, episode steps: 18, steps per second: 160, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.067 [-0.779, 1.318], loss: 3.382601, mean_absolute_error: 6.102146, mean_q: 11.258423\n",
      "Episode 118 reward: 18.0\n",
      "  2163/100000: episode: 118, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.068 [-0.623, 1.164], loss: 2.501600, mean_absolute_error: 6.017955, mean_q: 11.308644\n",
      "Episode 119 reward: 19.0\n",
      "  2182/100000: episode: 119, duration: 0.114s, episode steps: 19, steps per second: 166, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.058 [-0.590, 1.189], loss: 2.980889, mean_absolute_error: 6.127044, mean_q: 11.399839\n",
      "Episode 120 reward: 24.0\n",
      "  2206/100000: episode: 120, duration: 0.136s, episode steps: 24, steps per second: 177, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.053 [-0.554, 1.154], loss: 3.134156, mean_absolute_error: 6.156559, mean_q: 11.448293\n",
      "Episode 121 reward: 17.0\n",
      "  2223/100000: episode: 121, duration: 0.100s, episode steps: 17, steps per second: 170, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.060 [-0.627, 0.974], loss: 2.619704, mean_absolute_error: 6.155406, mean_q: 11.469575\n",
      "Episode 122 reward: 13.0\n",
      "  2236/100000: episode: 122, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.113 [-0.795, 1.213], loss: 2.873314, mean_absolute_error: 6.207336, mean_q: 11.575342\n",
      "Episode 123 reward: 20.0\n",
      "  2256/100000: episode: 123, duration: 0.122s, episode steps: 20, steps per second: 164, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.052 [-0.626, 1.024], loss: 3.458784, mean_absolute_error: 6.257757, mean_q: 11.598331\n",
      "Episode 124 reward: 13.0\n",
      "  2269/100000: episode: 124, duration: 0.081s, episode steps: 13, steps per second: 161, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.108 [-0.588, 1.008], loss: 2.642379, mean_absolute_error: 6.206689, mean_q: 11.618361\n",
      "Episode 125 reward: 17.0\n",
      "  2286/100000: episode: 125, duration: 0.104s, episode steps: 17, steps per second: 164, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.093 [-0.592, 1.000], loss: 3.686893, mean_absolute_error: 6.357936, mean_q: 11.648787\n",
      "Episode 126 reward: 20.0\n",
      "  2306/100000: episode: 126, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.062 [-0.758, 1.278], loss: 2.678511, mean_absolute_error: 6.235858, mean_q: 11.718153\n",
      "Episode 127 reward: 19.0\n",
      "  2325/100000: episode: 127, duration: 0.115s, episode steps: 19, steps per second: 165, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.080 [-0.567, 1.224], loss: 3.164310, mean_absolute_error: 6.328342, mean_q: 11.811389\n",
      "Episode 128 reward: 14.0\n",
      "  2339/100000: episode: 128, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.108 [-0.553, 1.217], loss: 2.733939, mean_absolute_error: 6.331264, mean_q: 11.878672\n",
      "Episode 129 reward: 13.0\n",
      "  2352/100000: episode: 129, duration: 0.076s, episode steps: 13, steps per second: 171, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.100 [-0.609, 1.011], loss: 3.113442, mean_absolute_error: 6.365169, mean_q: 11.892145\n",
      "Episode 130 reward: 15.0\n",
      "  2367/100000: episode: 130, duration: 0.091s, episode steps: 15, steps per second: 165, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.106 [-0.608, 1.011], loss: 3.342181, mean_absolute_error: 6.396698, mean_q: 11.923991\n",
      "Episode 131 reward: 18.0\n",
      "  2385/100000: episode: 131, duration: 0.113s, episode steps: 18, steps per second: 159, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.077 [-0.606, 1.081], loss: 3.348246, mean_absolute_error: 6.438157, mean_q: 11.918210\n",
      "Episode 132 reward: 17.0\n",
      "  2402/100000: episode: 132, duration: 0.101s, episode steps: 17, steps per second: 168, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.114 [-0.395, 1.037], loss: 3.886708, mean_absolute_error: 6.483189, mean_q: 11.953350\n",
      "Episode 133 reward: 18.0\n",
      "  2420/100000: episode: 133, duration: 0.112s, episode steps: 18, steps per second: 161, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.102 [-0.557, 1.104], loss: 3.227845, mean_absolute_error: 6.489921, mean_q: 11.950778\n",
      "Episode 134 reward: 17.0\n",
      "  2437/100000: episode: 134, duration: 0.108s, episode steps: 17, steps per second: 157, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.758, 1.173], loss: 3.812012, mean_absolute_error: 6.544283, mean_q: 11.992734\n",
      "Episode 135 reward: 22.0\n",
      "  2459/100000: episode: 135, duration: 0.131s, episode steps: 22, steps per second: 167, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.069 [-0.599, 1.061], loss: 3.960298, mean_absolute_error: 6.547539, mean_q: 12.008783\n",
      "Episode 136 reward: 18.0\n",
      "  2477/100000: episode: 136, duration: 0.106s, episode steps: 18, steps per second: 169, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.449, 0.990], loss: 3.155999, mean_absolute_error: 6.494579, mean_q: 12.018888\n",
      "Episode 137 reward: 24.0\n",
      "  2501/100000: episode: 137, duration: 0.152s, episode steps: 24, steps per second: 157, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.066 [-0.596, 1.187], loss: 3.548548, mean_absolute_error: 6.559493, mean_q: 12.121074\n",
      "Episode 138 reward: 22.0\n",
      "  2523/100000: episode: 138, duration: 0.128s, episode steps: 22, steps per second: 172, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.082 [-0.390, 1.104], loss: 3.615274, mean_absolute_error: 6.573100, mean_q: 12.135345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 139 reward: 17.0\n",
      "  2540/100000: episode: 139, duration: 0.105s, episode steps: 17, steps per second: 162, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.086 [-0.645, 1.238], loss: 3.769333, mean_absolute_error: 6.599899, mean_q: 12.147138\n",
      "Episode 140 reward: 30.0\n",
      "  2570/100000: episode: 140, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.398, 0.843], loss: 3.686262, mean_absolute_error: 6.626273, mean_q: 12.114931\n",
      "Episode 141 reward: 20.0\n",
      "  2590/100000: episode: 141, duration: 0.121s, episode steps: 20, steps per second: 165, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.419, 0.841], loss: 3.241905, mean_absolute_error: 6.614342, mean_q: 12.217180\n",
      "Episode 142 reward: 28.0\n",
      "  2618/100000: episode: 142, duration: 0.157s, episode steps: 28, steps per second: 178, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.595, 0.839], loss: 3.584218, mean_absolute_error: 6.659071, mean_q: 12.285074\n",
      "Episode 143 reward: 14.0\n",
      "  2632/100000: episode: 143, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.133 [-0.553, 1.208], loss: 3.329911, mean_absolute_error: 6.645288, mean_q: 12.369858\n",
      "Episode 144 reward: 21.0\n",
      "  2653/100000: episode: 144, duration: 0.128s, episode steps: 21, steps per second: 165, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.094 [-0.609, 0.988], loss: 3.857432, mean_absolute_error: 6.721692, mean_q: 12.322263\n",
      "Episode 145 reward: 18.0\n",
      "  2671/100000: episode: 145, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.376, 0.865], loss: 3.039330, mean_absolute_error: 6.649401, mean_q: 12.376482\n",
      "Episode 146 reward: 35.0\n",
      "  2706/100000: episode: 146, duration: 0.215s, episode steps: 35, steps per second: 163, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.041 [-0.413, 0.838], loss: 3.314224, mean_absolute_error: 6.728687, mean_q: 12.537139\n",
      "Episode 147 reward: 43.0\n",
      "  2749/100000: episode: 147, duration: 0.268s, episode steps: 43, steps per second: 160, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.040 [-0.425, 0.782], loss: 3.704897, mean_absolute_error: 6.804550, mean_q: 12.551310\n",
      "Episode 148 reward: 29.0\n",
      "  2778/100000: episode: 148, duration: 0.181s, episode steps: 29, steps per second: 160, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.106 [-0.211, 0.781], loss: 3.242366, mean_absolute_error: 6.790566, mean_q: 12.624213\n",
      "Episode 149 reward: 69.0\n",
      "  2847/100000: episode: 149, duration: 0.385s, episode steps: 69, steps per second: 179, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.000 [-0.389, 0.827], loss: 3.380412, mean_absolute_error: 6.866024, mean_q: 12.783696\n",
      "Episode 150 reward: 40.0\n",
      "  2887/100000: episode: 150, duration: 0.235s, episode steps: 40, steps per second: 170, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.400, 0.870], loss: 3.669032, mean_absolute_error: 6.933873, mean_q: 12.817721\n",
      "Episode 151 reward: 35.0\n",
      "  2922/100000: episode: 151, duration: 0.209s, episode steps: 35, steps per second: 168, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.071 [-0.322, 0.889], loss: 3.121917, mean_absolute_error: 6.922868, mean_q: 12.950214\n",
      "Episode 152 reward: 29.0\n",
      "  2951/100000: episode: 152, duration: 0.177s, episode steps: 29, steps per second: 164, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.073 [-0.440, 0.821], loss: 3.738078, mean_absolute_error: 7.022694, mean_q: 13.035754\n",
      "Episode 153 reward: 31.0\n",
      "  2982/100000: episode: 153, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.077 [-0.409, 1.195], loss: 3.104025, mean_absolute_error: 6.997909, mean_q: 13.066130\n",
      "Episode 154 reward: 36.0\n",
      "  3018/100000: episode: 154, duration: 0.207s, episode steps: 36, steps per second: 174, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.431, 0.944], loss: 3.227590, mean_absolute_error: 7.042256, mean_q: 13.170187\n",
      "Episode 155 reward: 37.0\n",
      "  3055/100000: episode: 155, duration: 0.219s, episode steps: 37, steps per second: 169, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.087 [-0.273, 0.780], loss: 3.964226, mean_absolute_error: 7.145889, mean_q: 13.203195\n",
      "Episode 156 reward: 65.0\n",
      "  3120/100000: episode: 156, duration: 0.376s, episode steps: 65, steps per second: 173, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.026 [-0.260, 1.035], loss: 2.973105, mean_absolute_error: 7.146292, mean_q: 13.386578\n",
      "Episode 157 reward: 36.0\n",
      "  3156/100000: episode: 157, duration: 0.205s, episode steps: 36, steps per second: 175, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.096 [-0.233, 0.808], loss: 3.131057, mean_absolute_error: 7.217124, mean_q: 13.591453\n",
      "Episode 158 reward: 53.0\n",
      "  3209/100000: episode: 158, duration: 0.309s, episode steps: 53, steps per second: 171, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.069 [-0.474, 0.891], loss: 3.479893, mean_absolute_error: 7.296047, mean_q: 13.622912\n",
      "Episode 159 reward: 27.0\n",
      "  3236/100000: episode: 159, duration: 0.152s, episode steps: 27, steps per second: 177, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.116 [-0.175, 0.850], loss: 3.119223, mean_absolute_error: 7.291273, mean_q: 13.720533\n",
      "Episode 160 reward: 77.0\n",
      "  3313/100000: episode: 160, duration: 0.448s, episode steps: 77, steps per second: 172, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.061 [-0.337, 0.820], loss: 2.760389, mean_absolute_error: 7.395764, mean_q: 14.012566\n",
      "Episode 161 reward: 55.0\n",
      "  3368/100000: episode: 161, duration: 0.314s, episode steps: 55, steps per second: 175, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.140 [-0.420, 0.747], loss: 3.244042, mean_absolute_error: 7.514141, mean_q: 14.198060\n",
      "Episode 162 reward: 106.0\n",
      "  3474/100000: episode: 162, duration: 0.598s, episode steps: 106, steps per second: 177, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.867, 0.720], loss: 3.630976, mean_absolute_error: 7.676330, mean_q: 14.455966\n",
      "Episode 163 reward: 30.0\n",
      "  3504/100000: episode: 163, duration: 0.173s, episode steps: 30, steps per second: 174, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-0.803, 0.175], loss: 3.865246, mean_absolute_error: 7.796122, mean_q: 14.553333\n",
      "Episode 164 reward: 79.0\n",
      "  3583/100000: episode: 164, duration: 0.447s, episode steps: 79, steps per second: 177, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.015 [-0.824, 0.412], loss: 3.661196, mean_absolute_error: 7.846888, mean_q: 14.708710\n",
      "Episode 165 reward: 22.0\n",
      "  3605/100000: episode: 165, duration: 0.123s, episode steps: 22, steps per second: 179, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.861, 0.372], loss: 4.054660, mean_absolute_error: 7.928110, mean_q: 14.909481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 166 reward: 27.0\n",
      "  3632/100000: episode: 166, duration: 0.165s, episode steps: 27, steps per second: 164, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.071 [-1.054, 0.362], loss: 2.851150, mean_absolute_error: 7.950523, mean_q: 15.113782\n",
      "Episode 167 reward: 25.0\n",
      "  3657/100000: episode: 167, duration: 0.143s, episode steps: 25, steps per second: 175, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.080 [-0.932, 0.414], loss: 4.225405, mean_absolute_error: 8.086159, mean_q: 15.216100\n",
      "Episode 168 reward: 20.0\n",
      "  3677/100000: episode: 168, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-0.856, 0.444], loss: 3.714986, mean_absolute_error: 8.035290, mean_q: 15.158844\n",
      "Episode 169 reward: 16.0\n",
      "  3693/100000: episode: 169, duration: 0.094s, episode steps: 16, steps per second: 170, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.115 [-1.134, 0.581], loss: 3.640295, mean_absolute_error: 8.130962, mean_q: 15.394829\n",
      "Episode 170 reward: 19.0\n",
      "  3712/100000: episode: 170, duration: 0.121s, episode steps: 19, steps per second: 157, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.074 [-1.011, 0.429], loss: 3.515338, mean_absolute_error: 8.105507, mean_q: 15.397530\n",
      "Episode 171 reward: 20.0\n",
      "  3732/100000: episode: 171, duration: 0.120s, episode steps: 20, steps per second: 167, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.094 [-1.112, 0.376], loss: 3.761430, mean_absolute_error: 8.144342, mean_q: 15.437167\n",
      "Episode 172 reward: 22.0\n",
      "  3754/100000: episode: 172, duration: 0.126s, episode steps: 22, steps per second: 175, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.055 [-1.094, 0.626], loss: 3.872093, mean_absolute_error: 8.208078, mean_q: 15.543137\n",
      "Episode 173 reward: 14.0\n",
      "  3768/100000: episode: 173, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.102 [-1.124, 0.457], loss: 3.047110, mean_absolute_error: 8.192877, mean_q: 15.666336\n",
      "Episode 174 reward: 18.0\n",
      "  3786/100000: episode: 174, duration: 0.109s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.101 [-1.198, 0.729], loss: 4.809261, mean_absolute_error: 8.419256, mean_q: 15.837276\n",
      "Episode 175 reward: 18.0\n",
      "  3804/100000: episode: 175, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.070 [-1.136, 0.803], loss: 4.618979, mean_absolute_error: 8.343949, mean_q: 15.647725\n",
      "Episode 176 reward: 12.0\n",
      "  3816/100000: episode: 176, duration: 0.076s, episode steps: 12, steps per second: 157, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.133 [-1.368, 0.766], loss: 4.901977, mean_absolute_error: 8.386020, mean_q: 15.695079\n",
      "Episode 177 reward: 14.0\n",
      "  3830/100000: episode: 177, duration: 0.089s, episode steps: 14, steps per second: 157, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.091 [-1.246, 0.618], loss: 4.144757, mean_absolute_error: 8.413379, mean_q: 15.841982\n",
      "Episode 178 reward: 18.0\n",
      "  3848/100000: episode: 178, duration: 0.135s, episode steps: 18, steps per second: 133, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.079 [-1.186, 0.766], loss: 3.859311, mean_absolute_error: 8.376743, mean_q: 15.946738\n",
      "Episode 179 reward: 16.0\n",
      "  3864/100000: episode: 179, duration: 0.116s, episode steps: 16, steps per second: 138, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.088 [-1.701, 0.982], loss: 5.797871, mean_absolute_error: 8.580676, mean_q: 16.117683\n",
      "Episode 180 reward: 11.0\n",
      "  3875/100000: episode: 180, duration: 0.080s, episode steps: 11, steps per second: 137, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.116 [-1.306, 0.617], loss: 4.287721, mean_absolute_error: 8.517334, mean_q: 16.099400\n",
      "Episode 181 reward: 17.0\n",
      "  3892/100000: episode: 181, duration: 0.122s, episode steps: 17, steps per second: 139, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.075 [-1.443, 0.818], loss: 4.860308, mean_absolute_error: 8.576788, mean_q: 16.166405\n",
      "Episode 182 reward: 12.0\n",
      "  3904/100000: episode: 182, duration: 0.085s, episode steps: 12, steps per second: 141, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.129 [-1.501, 0.758], loss: 4.411435, mean_absolute_error: 8.582659, mean_q: 16.250860\n",
      "Episode 183 reward: 12.0\n",
      "  3916/100000: episode: 183, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.103 [-1.496, 0.777], loss: 3.384329, mean_absolute_error: 8.546835, mean_q: 16.293583\n",
      "Episode 184 reward: 10.0\n",
      "  3926/100000: episode: 184, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.140 [-1.495, 0.743], loss: 3.234030, mean_absolute_error: 8.547972, mean_q: 16.398167\n",
      "Episode 185 reward: 13.0\n",
      "  3939/100000: episode: 185, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.079 [-1.670, 1.000], loss: 4.521091, mean_absolute_error: 8.673404, mean_q: 16.481718\n",
      "Episode 186 reward: 14.0\n",
      "  3953/100000: episode: 186, duration: 0.103s, episode steps: 14, steps per second: 136, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-1.916, 1.174], loss: 6.283825, mean_absolute_error: 8.726390, mean_q: 16.339037\n",
      "Episode 187 reward: 9.0\n",
      "  3962/100000: episode: 187, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.338, 1.383], loss: 4.648911, mean_absolute_error: 8.734097, mean_q: 16.560663\n",
      "Episode 188 reward: 11.0\n",
      "  3973/100000: episode: 188, duration: 0.074s, episode steps: 11, steps per second: 149, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.130 [-1.924, 1.137], loss: 4.223912, mean_absolute_error: 8.777848, mean_q: 16.540449\n",
      "Episode 189 reward: 12.0\n",
      "  3985/100000: episode: 189, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.098 [-1.907, 1.158], loss: 6.081720, mean_absolute_error: 8.812973, mean_q: 16.576208\n",
      "Episode 190 reward: 13.0\n",
      "  3998/100000: episode: 190, duration: 0.091s, episode steps: 13, steps per second: 143, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.693, 1.007], loss: 4.534271, mean_absolute_error: 8.779543, mean_q: 16.588627\n",
      "Episode 191 reward: 11.0\n",
      "  4009/100000: episode: 191, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.113 [-2.107, 1.369], loss: 4.099966, mean_absolute_error: 8.770985, mean_q: 16.606760\n",
      "Episode 192 reward: 9.0\n",
      "  4018/100000: episode: 192, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.124 [-1.912, 1.187], loss: 3.676088, mean_absolute_error: 8.783382, mean_q: 16.730560\n",
      "Episode 193 reward: 10.0\n",
      "  4028/100000: episode: 193, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.063, 1.165], loss: 3.920977, mean_absolute_error: 8.910231, mean_q: 16.903564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 194 reward: 10.0\n",
      "  4038/100000: episode: 194, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.143 [-1.875, 1.151], loss: 4.378322, mean_absolute_error: 8.923762, mean_q: 17.015604\n",
      "Episode 195 reward: 9.0\n",
      "  4047/100000: episode: 195, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.133 [-1.981, 1.226], loss: 3.744203, mean_absolute_error: 8.894413, mean_q: 16.949753\n",
      "Episode 196 reward: 12.0\n",
      "  4059/100000: episode: 196, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.096 [-1.918, 1.223], loss: 5.704932, mean_absolute_error: 8.967443, mean_q: 16.855680\n",
      "Episode 197 reward: 10.0\n",
      "  4069/100000: episode: 197, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.206, 1.348], loss: 5.399154, mean_absolute_error: 9.026675, mean_q: 17.023022\n",
      "Episode 198 reward: 10.0\n",
      "  4079/100000: episode: 198, duration: 0.059s, episode steps: 10, steps per second: 170, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.139, 1.341], loss: 7.192988, mean_absolute_error: 9.099424, mean_q: 16.972544\n",
      "Episode 199 reward: 10.0\n",
      "  4089/100000: episode: 199, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.112 [-2.654, 1.782], loss: 5.797388, mean_absolute_error: 8.995279, mean_q: 16.967405\n",
      "Episode 200 reward: 13.0\n",
      "  4102/100000: episode: 200, duration: 0.081s, episode steps: 13, steps per second: 160, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.102 [-2.825, 1.732], loss: 4.243831, mean_absolute_error: 9.076799, mean_q: 17.326933\n",
      "Episode 201 reward: 10.0\n",
      "  4112/100000: episode: 201, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.034, 1.983], loss: 5.775044, mean_absolute_error: 9.106188, mean_q: 17.276577\n",
      "Episode 202 reward: 10.0\n",
      "  4122/100000: episode: 202, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.113, 1.908], loss: 3.695948, mean_absolute_error: 8.976844, mean_q: 17.289536\n",
      "Episode 203 reward: 10.0\n",
      "  4132/100000: episode: 203, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.062, 1.949], loss: 4.953351, mean_absolute_error: 9.149221, mean_q: 17.490227\n",
      "Episode 204 reward: 12.0\n",
      "  4144/100000: episode: 204, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.103 [-3.120, 2.003], loss: 4.585377, mean_absolute_error: 9.128940, mean_q: 17.439505\n",
      "Episode 205 reward: 10.0\n",
      "  4154/100000: episode: 205, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.998, 1.930], loss: 4.701437, mean_absolute_error: 9.199162, mean_q: 17.595814\n",
      "Episode 206 reward: 10.0\n",
      "  4164/100000: episode: 206, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-3.070, 2.002], loss: 6.253493, mean_absolute_error: 9.291574, mean_q: 17.609707\n",
      "Episode 207 reward: 10.0\n",
      "  4174/100000: episode: 207, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.081, 1.934], loss: 4.063609, mean_absolute_error: 9.167135, mean_q: 17.565586\n",
      "Episode 208 reward: 11.0\n",
      "  4185/100000: episode: 208, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.869, 1.758], loss: 5.631836, mean_absolute_error: 9.323853, mean_q: 17.683453\n",
      "Episode 209 reward: 9.0\n",
      "  4194/100000: episode: 209, duration: 0.057s, episode steps: 9, steps per second: 157, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.126 [-2.503, 1.610], loss: 5.548801, mean_absolute_error: 9.323442, mean_q: 17.852808\n",
      "Episode 210 reward: 10.0\n",
      "  4204/100000: episode: 210, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.017, 1.906], loss: 7.568040, mean_absolute_error: 9.414118, mean_q: 17.675110\n",
      "Episode 211 reward: 9.0\n",
      "  4213/100000: episode: 211, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.821, 1.725], loss: 4.359785, mean_absolute_error: 9.247715, mean_q: 17.633425\n",
      "Episode 212 reward: 10.0\n",
      "  4223/100000: episode: 212, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.007, 1.912], loss: 7.948104, mean_absolute_error: 9.411856, mean_q: 17.760513\n",
      "Episode 213 reward: 9.0\n",
      "  4232/100000: episode: 213, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.814, 1.794], loss: 4.472047, mean_absolute_error: 9.273271, mean_q: 17.649981\n",
      "Episode 214 reward: 10.0\n",
      "  4242/100000: episode: 214, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.052, 1.968], loss: 4.009387, mean_absolute_error: 9.381933, mean_q: 17.955862\n",
      "Episode 215 reward: 12.0\n",
      "  4254/100000: episode: 215, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.114 [-2.609, 1.600], loss: 5.929577, mean_absolute_error: 9.462811, mean_q: 17.917093\n",
      "Episode 216 reward: 10.0\n",
      "  4264/100000: episode: 216, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.672, 1.757], loss: 7.698892, mean_absolute_error: 9.590189, mean_q: 17.877920\n",
      "Episode 217 reward: 9.0\n",
      "  4273/100000: episode: 217, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.787, 1.740], loss: 5.377651, mean_absolute_error: 9.536186, mean_q: 18.009935\n",
      "Episode 218 reward: 9.0\n",
      "  4282/100000: episode: 218, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.156 [-2.522, 1.522], loss: 8.011447, mean_absolute_error: 9.570209, mean_q: 17.962963\n",
      "Episode 219 reward: 9.0\n",
      "  4291/100000: episode: 219, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.895, 1.779], loss: 10.955700, mean_absolute_error: 9.726059, mean_q: 17.986980\n",
      "Episode 220 reward: 10.0\n",
      "  4301/100000: episode: 220, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.584, 1.575], loss: 6.252419, mean_absolute_error: 9.591636, mean_q: 18.001007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 221 reward: 10.0\n",
      "  4311/100000: episode: 221, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.706, 1.720], loss: 6.609975, mean_absolute_error: 9.593665, mean_q: 17.907705\n",
      "Episode 222 reward: 10.0\n",
      "  4321/100000: episode: 222, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.107, 1.942], loss: 5.901322, mean_absolute_error: 9.558410, mean_q: 17.933226\n",
      "Episode 223 reward: 9.0\n",
      "  4330/100000: episode: 223, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-2.785, 1.803], loss: 8.417294, mean_absolute_error: 9.719343, mean_q: 18.046442\n",
      "Episode 224 reward: 10.0\n",
      "  4340/100000: episode: 224, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-3.119, 1.934], loss: 5.736335, mean_absolute_error: 9.587946, mean_q: 18.097950\n",
      "Episode 225 reward: 8.0\n",
      "  4348/100000: episode: 225, duration: 0.066s, episode steps: 8, steps per second: 121, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.509, 1.615], loss: 10.134245, mean_absolute_error: 9.831151, mean_q: 18.096258\n",
      "Episode 226 reward: 11.0\n",
      "  4359/100000: episode: 226, duration: 0.091s, episode steps: 11, steps per second: 121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.311, 2.147], loss: 5.939439, mean_absolute_error: 9.602296, mean_q: 18.089479\n",
      "Episode 227 reward: 8.0\n",
      "  4367/100000: episode: 227, duration: 0.055s, episode steps: 8, steps per second: 147, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.587, 1.597], loss: 5.709366, mean_absolute_error: 9.687263, mean_q: 18.183838\n",
      "Episode 228 reward: 11.0\n",
      "  4378/100000: episode: 228, duration: 0.069s, episode steps: 11, steps per second: 159, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.872, 1.749], loss: 7.438013, mean_absolute_error: 9.707434, mean_q: 18.098343\n",
      "Episode 229 reward: 9.0\n",
      "  4387/100000: episode: 229, duration: 0.058s, episode steps: 9, steps per second: 156, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.129 [-2.260, 1.393], loss: 6.961801, mean_absolute_error: 9.687673, mean_q: 18.217514\n",
      "Episode 230 reward: 10.0\n",
      "  4397/100000: episode: 230, duration: 0.087s, episode steps: 10, steps per second: 114, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.571, 1.530], loss: 4.938693, mean_absolute_error: 9.595767, mean_q: 18.231068\n",
      "Episode 231 reward: 9.0\n",
      "  4406/100000: episode: 231, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.322, 1.325], loss: 7.950595, mean_absolute_error: 9.697752, mean_q: 18.191946\n",
      "Episode 232 reward: 12.0\n",
      "  4418/100000: episode: 232, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.080 [-2.966, 1.982], loss: 6.265848, mean_absolute_error: 9.716086, mean_q: 18.337984\n",
      "Episode 233 reward: 8.0\n",
      "  4426/100000: episode: 233, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-2.587, 1.527], loss: 8.405105, mean_absolute_error: 9.803297, mean_q: 18.189995\n",
      "Episode 234 reward: 11.0\n",
      "  4437/100000: episode: 234, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.126 [-2.767, 1.752], loss: 8.274768, mean_absolute_error: 9.790703, mean_q: 18.204771\n",
      "Episode 235 reward: 9.0\n",
      "  4446/100000: episode: 235, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.836, 1.780], loss: 5.760633, mean_absolute_error: 9.654991, mean_q: 18.189283\n",
      "Episode 236 reward: 10.0\n",
      "  4456/100000: episode: 236, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.053, 1.905], loss: 5.809030, mean_absolute_error: 9.735890, mean_q: 18.376949\n",
      "Episode 237 reward: 10.0\n",
      "  4466/100000: episode: 237, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.126 [-3.035, 1.981], loss: 7.632594, mean_absolute_error: 9.793442, mean_q: 18.291494\n",
      "Episode 238 reward: 12.0\n",
      "  4478/100000: episode: 238, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.121 [-2.981, 1.908], loss: 7.446513, mean_absolute_error: 9.860805, mean_q: 18.428518\n",
      "Episode 239 reward: 10.0\n",
      "  4488/100000: episode: 239, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.132, 1.924], loss: 6.420327, mean_absolute_error: 9.827622, mean_q: 18.458199\n",
      "Episode 240 reward: 8.0\n",
      "  4496/100000: episode: 240, duration: 0.051s, episode steps: 8, steps per second: 158, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.569, 1.571], loss: 9.176857, mean_absolute_error: 9.975485, mean_q: 18.392960\n",
      "Episode 241 reward: 9.0\n",
      "  4505/100000: episode: 241, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.832, 1.756], loss: 4.855652, mean_absolute_error: 9.817444, mean_q: 18.571978\n",
      "Episode 242 reward: 9.0\n",
      "  4514/100000: episode: 242, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.165 [-2.492, 1.518], loss: 5.822113, mean_absolute_error: 9.846826, mean_q: 18.497169\n",
      "Episode 243 reward: 9.0\n",
      "  4523/100000: episode: 243, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.807, 1.803], loss: 8.497564, mean_absolute_error: 9.984488, mean_q: 18.615669\n",
      "Episode 244 reward: 10.0\n",
      "  4533/100000: episode: 244, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.151 [-2.516, 1.529], loss: 12.896210, mean_absolute_error: 10.211494, mean_q: 18.512421\n",
      "Episode 245 reward: 10.0\n",
      "  4543/100000: episode: 245, duration: 0.061s, episode steps: 10, steps per second: 165, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.094, 1.317], loss: 11.418892, mean_absolute_error: 10.024038, mean_q: 18.221737\n",
      "Episode 246 reward: 13.0\n",
      "  4556/100000: episode: 246, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.104 [-2.220, 1.332], loss: 5.777394, mean_absolute_error: 9.792995, mean_q: 18.250881\n",
      "Episode 247 reward: 10.0\n",
      "  4566/100000: episode: 247, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.140 [-2.019, 1.202], loss: 9.398602, mean_absolute_error: 10.006541, mean_q: 18.287312\n",
      "Episode 248 reward: 12.0\n",
      "  4578/100000: episode: 248, duration: 0.072s, episode steps: 12, steps per second: 168, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.113 [-2.449, 1.523], loss: 8.455362, mean_absolute_error: 10.040251, mean_q: 18.422043\n",
      "Episode 249 reward: 8.0\n",
      "  4586/100000: episode: 249, duration: 0.052s, episode steps: 8, steps per second: 155, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.161 [-2.209, 1.333], loss: 7.934110, mean_absolute_error: 9.965893, mean_q: 18.299641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250 reward: 9.0\n",
      "  4595/100000: episode: 250, duration: 0.064s, episode steps: 9, steps per second: 142, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.137 [-2.303, 1.395], loss: 5.313713, mean_absolute_error: 9.769633, mean_q: 18.289791\n",
      "Episode 251 reward: 12.0\n",
      "  4607/100000: episode: 251, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.099 [-2.137, 1.380], loss: 8.986015, mean_absolute_error: 9.919201, mean_q: 18.226797\n",
      "Episode 252 reward: 9.0\n",
      "  4616/100000: episode: 252, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.285, 1.368], loss: 8.378172, mean_absolute_error: 9.960607, mean_q: 18.351091\n",
      "Episode 253 reward: 12.0\n",
      "  4628/100000: episode: 253, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.087 [-1.955, 1.226], loss: 10.030358, mean_absolute_error: 10.038075, mean_q: 18.356239\n",
      "Episode 254 reward: 14.0\n",
      "  4642/100000: episode: 254, duration: 0.084s, episode steps: 14, steps per second: 168, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.084 [-1.919, 1.171], loss: 8.991358, mean_absolute_error: 9.953380, mean_q: 18.176947\n",
      "Episode 255 reward: 11.0\n",
      "  4653/100000: episode: 255, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.249, 1.407], loss: 8.507944, mean_absolute_error: 9.924084, mean_q: 18.089088\n",
      "Episode 256 reward: 11.0\n",
      "  4664/100000: episode: 256, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.110 [-1.385, 0.821], loss: 9.032798, mean_absolute_error: 9.940502, mean_q: 18.100361\n",
      "Episode 257 reward: 18.0\n",
      "  4682/100000: episode: 257, duration: 0.107s, episode steps: 18, steps per second: 169, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.061 [-1.437, 0.829], loss: 8.236152, mean_absolute_error: 9.982910, mean_q: 18.259281\n",
      "Episode 258 reward: 14.0\n",
      "  4696/100000: episode: 258, duration: 0.085s, episode steps: 14, steps per second: 165, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.090 [-1.581, 0.999], loss: 7.580261, mean_absolute_error: 9.848585, mean_q: 18.113979\n",
      "Episode 259 reward: 13.0\n",
      "  4709/100000: episode: 259, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.096 [-1.406, 0.781], loss: 6.359921, mean_absolute_error: 9.825034, mean_q: 18.182213\n",
      "Episode 260 reward: 13.0\n",
      "  4722/100000: episode: 260, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.239, 0.584], loss: 7.105319, mean_absolute_error: 9.806198, mean_q: 18.105085\n",
      "Episode 261 reward: 26.0\n",
      "  4748/100000: episode: 261, duration: 0.162s, episode steps: 26, steps per second: 161, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.048 [-1.209, 0.585], loss: 10.096174, mean_absolute_error: 9.985262, mean_q: 18.092543\n",
      "Episode 262 reward: 26.0\n",
      "  4774/100000: episode: 262, duration: 0.153s, episode steps: 26, steps per second: 170, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-0.907, 0.558], loss: 5.843936, mean_absolute_error: 9.749281, mean_q: 18.022610\n",
      "Episode 263 reward: 55.0\n",
      "  4829/100000: episode: 263, duration: 0.351s, episode steps: 55, steps per second: 157, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.095 [-0.264, 0.789], loss: 6.154450, mean_absolute_error: 9.750164, mean_q: 18.153521\n",
      "Episode 264 reward: 15.0\n",
      "  4844/100000: episode: 264, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.108 [-1.388, 0.586], loss: 6.010285, mean_absolute_error: 9.722246, mean_q: 18.117338\n",
      "Episode 265 reward: 22.0\n",
      "  4866/100000: episode: 265, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.068 [-1.207, 0.550], loss: 6.555631, mean_absolute_error: 9.755136, mean_q: 18.106874\n",
      "Episode 266 reward: 22.0\n",
      "  4888/100000: episode: 266, duration: 0.141s, episode steps: 22, steps per second: 156, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.073 [-0.996, 0.411], loss: 7.689671, mean_absolute_error: 9.837849, mean_q: 18.140368\n",
      "Episode 267 reward: 16.0\n",
      "  4904/100000: episode: 267, duration: 0.117s, episode steps: 16, steps per second: 136, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.126 [-1.139, 0.397], loss: 6.933033, mean_absolute_error: 9.784826, mean_q: 18.059492\n",
      "Episode 268 reward: 26.0\n",
      "  4930/100000: episode: 268, duration: 0.159s, episode steps: 26, steps per second: 164, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.073 [-1.101, 0.405], loss: 7.566043, mean_absolute_error: 9.790526, mean_q: 17.963453\n",
      "Episode 269 reward: 26.0\n",
      "  4956/100000: episode: 269, duration: 0.150s, episode steps: 26, steps per second: 174, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.032, 0.388], loss: 6.940940, mean_absolute_error: 9.783614, mean_q: 17.972416\n",
      "Episode 270 reward: 80.0\n",
      "  5036/100000: episode: 270, duration: 0.445s, episode steps: 80, steps per second: 180, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.078 [-0.479, 0.728], loss: 6.987689, mean_absolute_error: 9.723566, mean_q: 17.911301\n",
      "Episode 271 reward: 58.0\n",
      "  5094/100000: episode: 271, duration: 0.349s, episode steps: 58, steps per second: 166, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.076 [-0.265, 0.776], loss: 6.245594, mean_absolute_error: 9.627702, mean_q: 17.810772\n",
      "Episode 272 reward: 35.0\n",
      "  5129/100000: episode: 272, duration: 0.198s, episode steps: 35, steps per second: 177, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.105 [-0.177, 0.927], loss: 5.009419, mean_absolute_error: 9.563441, mean_q: 17.934195\n",
      "Episode 273 reward: 29.0\n",
      "  5158/100000: episode: 273, duration: 0.173s, episode steps: 29, steps per second: 167, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.092 [-0.243, 0.884], loss: 5.706364, mean_absolute_error: 9.579062, mean_q: 17.890924\n",
      "Episode 274 reward: 48.0\n",
      "  5206/100000: episode: 274, duration: 0.272s, episode steps: 48, steps per second: 176, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.050 [-0.239, 0.820], loss: 5.076085, mean_absolute_error: 9.571412, mean_q: 17.919968\n",
      "Episode 275 reward: 23.0\n",
      "  5229/100000: episode: 275, duration: 0.135s, episode steps: 23, steps per second: 171, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.130 [-0.372, 0.822], loss: 6.033405, mean_absolute_error: 9.637459, mean_q: 17.908192\n",
      "Episode 276 reward: 35.0\n",
      "  5264/100000: episode: 276, duration: 0.218s, episode steps: 35, steps per second: 160, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.103 [-0.239, 0.818], loss: 4.094271, mean_absolute_error: 9.473011, mean_q: 17.873863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 277 reward: 49.0\n",
      "  5313/100000: episode: 277, duration: 0.284s, episode steps: 49, steps per second: 173, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.113 [-0.237, 0.920], loss: 3.913048, mean_absolute_error: 9.512217, mean_q: 18.072205\n",
      "Episode 278 reward: 28.0\n",
      "  5341/100000: episode: 278, duration: 0.167s, episode steps: 28, steps per second: 168, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.161, 0.916], loss: 4.699475, mean_absolute_error: 9.624839, mean_q: 18.196720\n",
      "Episode 279 reward: 26.0\n",
      "  5367/100000: episode: 279, duration: 0.154s, episode steps: 26, steps per second: 169, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.274, 1.185], loss: 6.310135, mean_absolute_error: 9.699812, mean_q: 18.140228\n",
      "Episode 280 reward: 27.0\n",
      "  5394/100000: episode: 280, duration: 0.173s, episode steps: 27, steps per second: 156, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.091 [-0.382, 0.864], loss: 5.935631, mean_absolute_error: 9.607058, mean_q: 17.868349\n",
      "Episode 281 reward: 30.0\n",
      "  5424/100000: episode: 281, duration: 0.178s, episode steps: 30, steps per second: 168, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.349, 0.869], loss: 6.920169, mean_absolute_error: 9.679628, mean_q: 17.895996\n",
      "Episode 282 reward: 24.0\n",
      "  5448/100000: episode: 282, duration: 0.133s, episode steps: 24, steps per second: 181, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.412, 0.967], loss: 4.799809, mean_absolute_error: 9.534493, mean_q: 17.798332\n",
      "Episode 283 reward: 23.0\n",
      "  5471/100000: episode: 283, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.088 [-0.442, 0.853], loss: 5.133971, mean_absolute_error: 9.585300, mean_q: 17.943483\n",
      "Episode 284 reward: 38.0\n",
      "  5509/100000: episode: 284, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.075 [-0.403, 0.792], loss: 5.049645, mean_absolute_error: 9.577285, mean_q: 17.977060\n",
      "Episode 285 reward: 82.0\n",
      "  5591/100000: episode: 285, duration: 0.447s, episode steps: 82, steps per second: 183, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.020 [-0.382, 0.747], loss: 5.334374, mean_absolute_error: 9.551314, mean_q: 17.879879\n",
      "Episode 286 reward: 112.0\n",
      "  5703/100000: episode: 286, duration: 0.614s, episode steps: 112, steps per second: 182, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.033 [-0.512, 0.750], loss: 4.783890, mean_absolute_error: 9.551043, mean_q: 17.983852\n",
      "Episode 287 reward: 27.0\n",
      "  5730/100000: episode: 287, duration: 0.152s, episode steps: 27, steps per second: 178, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.086 [-0.428, 0.819], loss: 5.735332, mean_absolute_error: 9.549009, mean_q: 17.804619\n",
      "Episode 288 reward: 33.0\n",
      "  5763/100000: episode: 288, duration: 0.207s, episode steps: 33, steps per second: 159, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.081 [-0.215, 0.836], loss: 5.481446, mean_absolute_error: 9.590730, mean_q: 17.875450\n",
      "Episode 289 reward: 30.0\n",
      "  5793/100000: episode: 289, duration: 0.211s, episode steps: 30, steps per second: 142, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.364, 0.851], loss: 5.703556, mean_absolute_error: 9.576003, mean_q: 17.828409\n",
      "Episode 290 reward: 47.0\n",
      "  5840/100000: episode: 290, duration: 0.321s, episode steps: 47, steps per second: 146, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.064 [-0.332, 0.851], loss: 5.051739, mean_absolute_error: 9.529992, mean_q: 17.823151\n",
      "Episode 291 reward: 31.0\n",
      "  5871/100000: episode: 291, duration: 0.195s, episode steps: 31, steps per second: 159, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.119 [-0.165, 0.750], loss: 4.477096, mean_absolute_error: 9.539365, mean_q: 17.998362\n",
      "Episode 292 reward: 131.0\n",
      "  6002/100000: episode: 292, duration: 0.726s, episode steps: 131, steps per second: 180, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.029 [-0.432, 0.858], loss: 5.092095, mean_absolute_error: 9.605543, mean_q: 18.046028\n",
      "Episode 293 reward: 48.0\n",
      "  6050/100000: episode: 293, duration: 0.265s, episode steps: 48, steps per second: 181, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.105 [-0.182, 0.747], loss: 4.211084, mean_absolute_error: 9.564637, mean_q: 18.127991\n",
      "Episode 294 reward: 47.0\n",
      "  6097/100000: episode: 294, duration: 0.265s, episode steps: 47, steps per second: 177, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.112 [-0.239, 0.674], loss: 4.937161, mean_absolute_error: 9.613485, mean_q: 18.158112\n",
      "Episode 295 reward: 40.0\n",
      "  6137/100000: episode: 295, duration: 0.233s, episode steps: 40, steps per second: 172, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.122 [-0.886, 0.370], loss: 4.331646, mean_absolute_error: 9.580864, mean_q: 18.143551\n",
      "Episode 296 reward: 43.0\n",
      "  6180/100000: episode: 296, duration: 0.241s, episode steps: 43, steps per second: 179, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.097 [-0.961, 0.382], loss: 6.329565, mean_absolute_error: 9.730042, mean_q: 18.203072\n",
      "Episode 297 reward: 46.0\n",
      "  6226/100000: episode: 297, duration: 0.266s, episode steps: 46, steps per second: 173, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.125 [-0.234, 0.954], loss: 5.024691, mean_absolute_error: 9.645894, mean_q: 18.104424\n",
      "Episode 298 reward: 72.0\n",
      "  6298/100000: episode: 298, duration: 0.398s, episode steps: 72, steps per second: 181, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.089 [-0.552, 0.784], loss: 4.664915, mean_absolute_error: 9.665629, mean_q: 18.211592\n",
      "Episode 299 reward: 61.0\n",
      "  6359/100000: episode: 299, duration: 0.328s, episode steps: 61, steps per second: 186, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.131 [-0.470, 0.819], loss: 3.990545, mean_absolute_error: 9.598625, mean_q: 18.211489\n",
      "Episode 300 reward: 67.0\n",
      "  6426/100000: episode: 300, duration: 0.368s, episode steps: 67, steps per second: 182, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.055 [-0.848, 0.427], loss: 4.542029, mean_absolute_error: 9.714518, mean_q: 18.406773\n",
      "Episode 301 reward: 50.0\n",
      "  6476/100000: episode: 301, duration: 0.277s, episode steps: 50, steps per second: 180, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.069 [-0.735, 0.248], loss: 4.579332, mean_absolute_error: 9.790217, mean_q: 18.589346\n",
      "Episode 302 reward: 40.0\n",
      "  6516/100000: episode: 302, duration: 0.230s, episode steps: 40, steps per second: 174, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.096 [-0.736, 0.190], loss: 4.007314, mean_absolute_error: 9.757225, mean_q: 18.555441\n",
      "Episode 303 reward: 43.0\n",
      "  6559/100000: episode: 303, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.142 [-0.746, 0.271], loss: 4.299910, mean_absolute_error: 9.840443, mean_q: 18.688803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 304 reward: 54.0\n",
      "  6613/100000: episode: 304, duration: 0.305s, episode steps: 54, steps per second: 177, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.119 [-0.412, 0.667], loss: 5.820991, mean_absolute_error: 9.906590, mean_q: 18.634323\n",
      "Episode 305 reward: 118.0\n",
      "  6731/100000: episode: 305, duration: 0.658s, episode steps: 118, steps per second: 179, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.032 [-0.404, 0.849], loss: 4.763710, mean_absolute_error: 9.905890, mean_q: 18.745239\n",
      "Episode 306 reward: 62.0\n",
      "  6793/100000: episode: 306, duration: 0.349s, episode steps: 62, steps per second: 178, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.035 [-0.820, 0.619], loss: 4.658542, mean_absolute_error: 9.947299, mean_q: 18.837564\n",
      "Episode 307 reward: 86.0\n",
      "  6879/100000: episode: 307, duration: 0.481s, episode steps: 86, steps per second: 179, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.041 [-0.833, 0.755], loss: 4.090215, mean_absolute_error: 9.996934, mean_q: 19.027788\n",
      "Episode 308 reward: 38.0\n",
      "  6917/100000: episode: 308, duration: 0.221s, episode steps: 38, steps per second: 172, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.076, 0.182], loss: 3.834767, mean_absolute_error: 10.079820, mean_q: 19.257380\n",
      "Episode 309 reward: 52.0\n",
      "  6969/100000: episode: 309, duration: 0.276s, episode steps: 52, steps per second: 188, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.152 [-0.235, 0.748], loss: 4.690901, mean_absolute_error: 10.147215, mean_q: 19.293934\n",
      "Episode 310 reward: 31.0\n",
      "  7000/100000: episode: 310, duration: 0.154s, episode steps: 31, steps per second: 201, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.123 [-0.720, 0.380], loss: 4.894519, mean_absolute_error: 10.131103, mean_q: 19.209129\n",
      "Episode 311 reward: 94.0\n",
      "  7094/100000: episode: 311, duration: 0.495s, episode steps: 94, steps per second: 190, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.274, 0.383], loss: 4.732736, mean_absolute_error: 10.192715, mean_q: 19.325893\n",
      "Episode 312 reward: 56.0\n",
      "  7150/100000: episode: 312, duration: 0.281s, episode steps: 56, steps per second: 200, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.099 [-0.704, 0.265], loss: 4.200608, mean_absolute_error: 10.210928, mean_q: 19.412289\n",
      "Episode 313 reward: 74.0\n",
      "  7224/100000: episode: 313, duration: 0.372s, episode steps: 74, steps per second: 199, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.066 [-0.861, 0.582], loss: 4.691255, mean_absolute_error: 10.309920, mean_q: 19.596581\n",
      "Episode 314 reward: 68.0\n",
      "  7292/100000: episode: 314, duration: 0.352s, episode steps: 68, steps per second: 193, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.059 [-0.863, 0.576], loss: 4.303649, mean_absolute_error: 10.325090, mean_q: 19.690245\n",
      "Episode 315 reward: 156.0\n",
      "  7448/100000: episode: 315, duration: 0.780s, episode steps: 156, steps per second: 200, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.046 [-0.868, 0.395], loss: 4.650943, mean_absolute_error: 10.467143, mean_q: 19.943228\n",
      "Episode 316 reward: 46.0\n",
      "  7494/100000: episode: 316, duration: 0.299s, episode steps: 46, steps per second: 154, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.148 [-0.350, 0.761], loss: 3.495301, mean_absolute_error: 10.516746, mean_q: 20.170080\n",
      "Episode 317 reward: 41.0\n",
      "  7535/100000: episode: 317, duration: 0.243s, episode steps: 41, steps per second: 169, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.132 [-1.040, 0.324], loss: 4.564785, mean_absolute_error: 10.595422, mean_q: 20.208651\n",
      "Episode 318 reward: 43.0\n",
      "  7578/100000: episode: 318, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.127 [-0.201, 0.676], loss: 4.328864, mean_absolute_error: 10.609745, mean_q: 20.261702\n",
      "Episode 319 reward: 43.0\n",
      "  7621/100000: episode: 319, duration: 0.247s, episode steps: 43, steps per second: 174, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.122 [-0.414, 0.702], loss: 4.757153, mean_absolute_error: 10.650272, mean_q: 20.330275\n",
      "Episode 320 reward: 131.0\n",
      "  7752/100000: episode: 320, duration: 0.740s, episode steps: 131, steps per second: 177, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.144 [-0.825, 0.766], loss: 4.619213, mean_absolute_error: 10.757557, mean_q: 20.499454\n",
      "Episode 321 reward: 47.0\n",
      "  7799/100000: episode: 321, duration: 0.268s, episode steps: 47, steps per second: 176, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.127 [-0.180, 0.644], loss: 6.287406, mean_absolute_error: 10.932295, mean_q: 20.703318\n",
      "Episode 322 reward: 48.0\n",
      "  7847/100000: episode: 322, duration: 0.270s, episode steps: 48, steps per second: 178, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.116 [-0.354, 0.708], loss: 5.727959, mean_absolute_error: 10.869319, mean_q: 20.596178\n",
      "Episode 323 reward: 71.0\n",
      "  7918/100000: episode: 323, duration: 0.404s, episode steps: 71, steps per second: 176, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.043 [-0.798, 0.346], loss: 5.262898, mean_absolute_error: 10.920226, mean_q: 20.845510\n",
      "Episode 324 reward: 42.0\n",
      "  7960/100000: episode: 324, duration: 0.241s, episode steps: 42, steps per second: 174, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.073 [-0.718, 0.375], loss: 3.597385, mean_absolute_error: 10.911387, mean_q: 20.970804\n",
      "Episode 325 reward: 67.0\n",
      "  8027/100000: episode: 325, duration: 0.367s, episode steps: 67, steps per second: 183, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.140 [-0.277, 0.759], loss: 5.134759, mean_absolute_error: 11.060420, mean_q: 21.131256\n",
      "Episode 326 reward: 74.0\n",
      "  8101/100000: episode: 326, duration: 0.411s, episode steps: 74, steps per second: 180, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.097 [-0.342, 0.766], loss: 4.811924, mean_absolute_error: 11.106425, mean_q: 21.237043\n",
      "Episode 327 reward: 32.0\n",
      "  8133/100000: episode: 327, duration: 0.185s, episode steps: 32, steps per second: 173, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.125 [-0.213, 0.867], loss: 4.165912, mean_absolute_error: 10.978630, mean_q: 21.048674\n",
      "Episode 328 reward: 101.0\n",
      "  8234/100000: episode: 328, duration: 0.558s, episode steps: 101, steps per second: 181, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.055 [-0.784, 0.489], loss: 4.900990, mean_absolute_error: 11.222403, mean_q: 21.502655\n",
      "Episode 329 reward: 56.0\n",
      "  8290/100000: episode: 329, duration: 0.319s, episode steps: 56, steps per second: 175, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.075 [-0.771, 0.400], loss: 4.485378, mean_absolute_error: 11.255239, mean_q: 21.637405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 330 reward: 69.0\n",
      "  8359/100000: episode: 330, duration: 0.388s, episode steps: 69, steps per second: 178, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.065 [-0.741, 0.341], loss: 5.059013, mean_absolute_error: 11.373915, mean_q: 21.763760\n",
      "Episode 331 reward: 68.0\n",
      "  8427/100000: episode: 331, duration: 0.374s, episode steps: 68, steps per second: 182, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.050 [-0.739, 0.345], loss: 5.407334, mean_absolute_error: 11.452188, mean_q: 21.929550\n",
      "Episode 332 reward: 37.0\n",
      "  8464/100000: episode: 332, duration: 0.224s, episode steps: 37, steps per second: 165, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.109 [-0.696, 0.264], loss: 6.020202, mean_absolute_error: 11.475592, mean_q: 21.820463\n",
      "Episode 333 reward: 160.0\n",
      "  8624/100000: episode: 333, duration: 0.898s, episode steps: 160, steps per second: 178, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.045 [-0.481, 0.996], loss: 5.194696, mean_absolute_error: 11.565045, mean_q: 22.163416\n",
      "Episode 334 reward: 55.0\n",
      "  8679/100000: episode: 334, duration: 0.308s, episode steps: 55, steps per second: 178, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.114 [-0.787, 0.285], loss: 6.734532, mean_absolute_error: 11.676190, mean_q: 22.220709\n",
      "Episode 335 reward: 60.0\n",
      "  8739/100000: episode: 335, duration: 0.332s, episode steps: 60, steps per second: 181, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.098 [-0.707, 0.289], loss: 4.862185, mean_absolute_error: 11.643098, mean_q: 22.365118\n",
      "Episode 336 reward: 84.0\n",
      "  8823/100000: episode: 336, duration: 0.469s, episode steps: 84, steps per second: 179, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.060 [-0.789, 0.341], loss: 5.026922, mean_absolute_error: 11.783916, mean_q: 22.650728\n",
      "Episode 337 reward: 61.0\n",
      "  8884/100000: episode: 337, duration: 0.347s, episode steps: 61, steps per second: 176, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.158 [-0.279, 0.925], loss: 6.220128, mean_absolute_error: 11.909544, mean_q: 22.771980\n",
      "Episode 338 reward: 54.0\n",
      "  8938/100000: episode: 338, duration: 0.299s, episode steps: 54, steps per second: 181, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.157 [-0.253, 0.742], loss: 6.979939, mean_absolute_error: 11.938407, mean_q: 22.777519\n",
      "Episode 339 reward: 44.0\n",
      "  8982/100000: episode: 339, duration: 0.255s, episode steps: 44, steps per second: 173, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.129, 0.396], loss: 5.566618, mean_absolute_error: 11.886550, mean_q: 22.755037\n",
      "Episode 340 reward: 66.0\n",
      "  9048/100000: episode: 340, duration: 0.359s, episode steps: 66, steps per second: 184, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.157 [-0.266, 0.764], loss: 5.838868, mean_absolute_error: 11.961437, mean_q: 22.941156\n",
      "Episode 341 reward: 93.0\n",
      "  9141/100000: episode: 341, duration: 0.525s, episode steps: 93, steps per second: 177, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.119 [-0.553, 0.913], loss: 5.552598, mean_absolute_error: 12.055304, mean_q: 23.158875\n",
      "Episode 342 reward: 61.0\n",
      "  9202/100000: episode: 342, duration: 0.353s, episode steps: 61, steps per second: 173, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.132 [-0.268, 0.887], loss: 5.819622, mean_absolute_error: 12.134415, mean_q: 23.218946\n",
      "Episode 343 reward: 73.0\n",
      "  9275/100000: episode: 343, duration: 0.399s, episode steps: 73, steps per second: 183, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.106 [-0.417, 0.874], loss: 5.093015, mean_absolute_error: 12.082010, mean_q: 23.210402\n",
      "Episode 344 reward: 72.0\n",
      "  9347/100000: episode: 344, duration: 0.399s, episode steps: 72, steps per second: 180, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.078 [-0.864, 0.350], loss: 5.430393, mean_absolute_error: 12.151340, mean_q: 23.329046\n",
      "Episode 345 reward: 71.0\n",
      "  9418/100000: episode: 345, duration: 0.395s, episode steps: 71, steps per second: 180, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.058 [-0.861, 0.402], loss: 4.689939, mean_absolute_error: 12.266811, mean_q: 23.669472\n",
      "Episode 346 reward: 69.0\n",
      "  9487/100000: episode: 346, duration: 0.396s, episode steps: 69, steps per second: 174, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.085 [-0.737, 0.432], loss: 7.627387, mean_absolute_error: 12.435400, mean_q: 23.778872\n",
      "Episode 347 reward: 49.0\n",
      "  9536/100000: episode: 347, duration: 0.277s, episode steps: 49, steps per second: 177, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.167 [-0.440, 0.745], loss: 5.811533, mean_absolute_error: 12.346750, mean_q: 23.724583\n",
      "Episode 348 reward: 67.0\n",
      "  9603/100000: episode: 348, duration: 0.377s, episode steps: 67, steps per second: 178, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.154 [-0.381, 0.954], loss: 4.926654, mean_absolute_error: 12.449345, mean_q: 24.003569\n",
      "Episode 349 reward: 74.0\n",
      "  9677/100000: episode: 349, duration: 0.401s, episode steps: 74, steps per second: 185, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.160 [-0.486, 1.148], loss: 5.812237, mean_absolute_error: 12.542834, mean_q: 24.115494\n",
      "Episode 350 reward: 200.0\n",
      "  9877/100000: episode: 350, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.563, 0.565], loss: 5.635347, mean_absolute_error: 12.600612, mean_q: 24.270048\n",
      "Episode 351 reward: 57.0\n",
      "  9934/100000: episode: 351, duration: 0.329s, episode steps: 57, steps per second: 173, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.155 [-0.218, 0.899], loss: 7.160566, mean_absolute_error: 12.812338, mean_q: 24.585909\n",
      "Episode 352 reward: 32.0\n",
      "  9966/100000: episode: 352, duration: 0.186s, episode steps: 32, steps per second: 172, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.110 [-0.851, 0.218], loss: 6.961992, mean_absolute_error: 12.787378, mean_q: 24.538166\n",
      "Episode 353 reward: 64.0\n",
      " 10030/100000: episode: 353, duration: 0.363s, episode steps: 64, steps per second: 176, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.137 [-0.444, 1.263], loss: 6.564391, mean_absolute_error: 12.801425, mean_q: 24.615173\n",
      "Episode 354 reward: 78.0\n",
      " 10108/100000: episode: 354, duration: 0.438s, episode steps: 78, steps per second: 178, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.161 [-0.441, 1.141], loss: 5.870219, mean_absolute_error: 12.883153, mean_q: 24.815205\n",
      "Episode 355 reward: 34.0\n",
      " 10142/100000: episode: 355, duration: 0.197s, episode steps: 34, steps per second: 172, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.122 [-0.754, 0.177], loss: 4.848994, mean_absolute_error: 12.819715, mean_q: 24.871012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 356 reward: 43.0\n",
      " 10185/100000: episode: 356, duration: 0.246s, episode steps: 43, steps per second: 175, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.141 [-0.735, 0.216], loss: 5.845871, mean_absolute_error: 12.898246, mean_q: 24.918406\n",
      "Episode 357 reward: 118.0\n",
      " 10303/100000: episode: 357, duration: 0.672s, episode steps: 118, steps per second: 176, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.106 [-0.495, 0.770], loss: 5.207743, mean_absolute_error: 13.010008, mean_q: 25.130171\n",
      "Episode 358 reward: 87.0\n",
      " 10390/100000: episode: 358, duration: 0.500s, episode steps: 87, steps per second: 174, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.103 [-0.829, 0.271], loss: 5.797424, mean_absolute_error: 13.128880, mean_q: 25.339199\n",
      "Episode 359 reward: 71.0\n",
      " 10461/100000: episode: 359, duration: 0.390s, episode steps: 71, steps per second: 182, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.127 [-0.411, 0.930], loss: 6.600369, mean_absolute_error: 13.178889, mean_q: 25.339897\n",
      "Episode 360 reward: 71.0\n",
      " 10532/100000: episode: 360, duration: 0.387s, episode steps: 71, steps per second: 183, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.122 [-0.812, 0.385], loss: 4.941605, mean_absolute_error: 13.183520, mean_q: 25.527637\n",
      "Episode 361 reward: 149.0\n",
      " 10681/100000: episode: 361, duration: 0.819s, episode steps: 149, steps per second: 182, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.034 [-0.801, 0.362], loss: 5.947352, mean_absolute_error: 13.369720, mean_q: 25.818123\n",
      "Episode 362 reward: 43.0\n",
      " 10724/100000: episode: 362, duration: 0.246s, episode steps: 43, steps per second: 175, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.121 [-0.652, 0.353], loss: 8.334225, mean_absolute_error: 13.551985, mean_q: 25.981060\n",
      "Episode 363 reward: 60.0\n",
      " 10784/100000: episode: 363, duration: 0.327s, episode steps: 60, steps per second: 183, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.171 [-0.239, 1.076], loss: 6.736751, mean_absolute_error: 13.419206, mean_q: 25.772409\n",
      "Episode 364 reward: 65.0\n",
      " 10849/100000: episode: 364, duration: 0.361s, episode steps: 65, steps per second: 180, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.155 [-0.280, 0.948], loss: 5.566972, mean_absolute_error: 13.514459, mean_q: 26.082766\n",
      "Episode 365 reward: 76.0\n",
      " 10925/100000: episode: 365, duration: 0.426s, episode steps: 76, steps per second: 178, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.163 [-0.347, 1.327], loss: 6.048786, mean_absolute_error: 13.652432, mean_q: 26.314177\n",
      "Episode 366 reward: 78.0\n",
      " 11003/100000: episode: 366, duration: 0.439s, episode steps: 78, steps per second: 178, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.149 [-0.206, 1.104], loss: 5.092378, mean_absolute_error: 13.599747, mean_q: 26.295414\n",
      "Episode 367 reward: 67.0\n",
      " 11070/100000: episode: 367, duration: 0.382s, episode steps: 67, steps per second: 176, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.218 [-0.374, 1.132], loss: 7.945565, mean_absolute_error: 13.707954, mean_q: 26.376322\n",
      "Episode 368 reward: 106.0\n",
      " 11176/100000: episode: 368, duration: 0.590s, episode steps: 106, steps per second: 180, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.075 [-0.390, 1.088], loss: 5.885067, mean_absolute_error: 13.686647, mean_q: 26.430923\n",
      "Episode 369 reward: 64.0\n",
      " 11240/100000: episode: 369, duration: 0.368s, episode steps: 64, steps per second: 174, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.202 [-0.370, 1.127], loss: 6.069489, mean_absolute_error: 13.705567, mean_q: 26.414047\n",
      "Episode 370 reward: 99.0\n",
      " 11339/100000: episode: 370, duration: 0.560s, episode steps: 99, steps per second: 177, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.065 [-0.959, 0.351], loss: 5.691278, mean_absolute_error: 13.856567, mean_q: 26.771566\n",
      "Episode 371 reward: 102.0\n",
      " 11441/100000: episode: 371, duration: 0.567s, episode steps: 102, steps per second: 180, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.153 [-0.263, 1.101], loss: 6.208808, mean_absolute_error: 13.933658, mean_q: 26.941465\n",
      "Episode 372 reward: 91.0\n",
      " 11532/100000: episode: 372, duration: 0.511s, episode steps: 91, steps per second: 178, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.164 [-0.404, 1.257], loss: 6.028662, mean_absolute_error: 13.949593, mean_q: 26.908295\n",
      "Episode 373 reward: 70.0\n",
      " 11602/100000: episode: 373, duration: 0.396s, episode steps: 70, steps per second: 177, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.214 [-0.374, 1.289], loss: 6.052225, mean_absolute_error: 14.022429, mean_q: 27.140898\n",
      "Episode 374 reward: 91.0\n",
      " 11693/100000: episode: 374, duration: 0.508s, episode steps: 91, steps per second: 179, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.174 [-0.235, 1.285], loss: 7.569806, mean_absolute_error: 14.157772, mean_q: 27.251829\n",
      "Episode 375 reward: 74.0\n",
      " 11767/100000: episode: 375, duration: 0.422s, episode steps: 74, steps per second: 175, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.207 [-0.356, 1.120], loss: 6.764149, mean_absolute_error: 14.115110, mean_q: 27.224981\n",
      "Episode 376 reward: 66.0\n",
      " 11833/100000: episode: 376, duration: 0.385s, episode steps: 66, steps per second: 171, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.087 [-0.698, 0.180], loss: 6.485838, mean_absolute_error: 14.230005, mean_q: 27.448065\n",
      "Episode 377 reward: 73.0\n",
      " 11906/100000: episode: 377, duration: 0.422s, episode steps: 73, steps per second: 173, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.205 [-0.242, 1.242], loss: 7.730327, mean_absolute_error: 14.128591, mean_q: 27.143276\n",
      "Episode 378 reward: 69.0\n",
      " 11975/100000: episode: 378, duration: 0.384s, episode steps: 69, steps per second: 180, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.226 [-0.409, 1.263], loss: 7.262050, mean_absolute_error: 14.209909, mean_q: 27.400719\n",
      "Episode 379 reward: 44.0\n",
      " 12019/100000: episode: 379, duration: 0.254s, episode steps: 44, steps per second: 173, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.112 [-0.859, 0.217], loss: 6.093953, mean_absolute_error: 14.210330, mean_q: 27.496307\n",
      "Episode 380 reward: 93.0\n",
      " 12112/100000: episode: 380, duration: 0.520s, episode steps: 93, steps per second: 179, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.085 [-0.936, 0.383], loss: 7.379183, mean_absolute_error: 14.362799, mean_q: 27.625700\n",
      "Episode 381 reward: 79.0\n",
      " 12191/100000: episode: 381, duration: 0.442s, episode steps: 79, steps per second: 179, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.205 [-0.357, 1.226], loss: 6.198262, mean_absolute_error: 14.352721, mean_q: 27.769821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 382 reward: 83.0\n",
      " 12274/100000: episode: 382, duration: 0.472s, episode steps: 83, steps per second: 176, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.100 [-1.219, 0.257], loss: 6.599673, mean_absolute_error: 14.339184, mean_q: 27.653618\n",
      "Episode 383 reward: 45.0\n",
      " 12319/100000: episode: 383, duration: 0.260s, episode steps: 45, steps per second: 173, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.135 [-0.710, 0.229], loss: 5.498701, mean_absolute_error: 14.439000, mean_q: 27.921032\n",
      "Episode 384 reward: 200.0\n",
      " 12519/100000: episode: 384, duration: 1.098s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.056 [-0.497, 1.224], loss: 6.597198, mean_absolute_error: 14.520765, mean_q: 28.020063\n",
      "Episode 385 reward: 98.0\n",
      " 12617/100000: episode: 385, duration: 0.559s, episode steps: 98, steps per second: 175, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.161 [-0.392, 1.458], loss: 6.420678, mean_absolute_error: 14.573109, mean_q: 28.078526\n",
      "Episode 386 reward: 90.0\n",
      " 12707/100000: episode: 386, duration: 0.517s, episode steps: 90, steps per second: 174, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.210 [-0.307, 1.491], loss: 6.407371, mean_absolute_error: 14.616804, mean_q: 28.196743\n",
      "Episode 387 reward: 107.0\n",
      " 12814/100000: episode: 387, duration: 0.605s, episode steps: 107, steps per second: 177, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.171 [-0.715, 1.259], loss: 7.840414, mean_absolute_error: 14.667477, mean_q: 28.201008\n",
      "Episode 388 reward: 68.0\n",
      " 12882/100000: episode: 388, duration: 0.380s, episode steps: 68, steps per second: 179, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.235 [-0.278, 1.407], loss: 5.977145, mean_absolute_error: 14.572205, mean_q: 28.200634\n",
      "Episode 389 reward: 136.0\n",
      " 13018/100000: episode: 389, duration: 0.732s, episode steps: 136, steps per second: 186, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.123 [-0.751, 1.427], loss: 6.407213, mean_absolute_error: 14.726611, mean_q: 28.399130\n",
      "Episode 390 reward: 55.0\n",
      " 13073/100000: episode: 390, duration: 0.324s, episode steps: 55, steps per second: 170, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.160 [-0.796, 0.290], loss: 6.561025, mean_absolute_error: 14.763997, mean_q: 28.458782\n",
      "Episode 391 reward: 90.0\n",
      " 13163/100000: episode: 391, duration: 0.487s, episode steps: 90, steps per second: 185, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.223 [-0.464, 1.481], loss: 8.691765, mean_absolute_error: 14.817239, mean_q: 28.464472\n",
      "Episode 392 reward: 74.0\n",
      " 13237/100000: episode: 392, duration: 0.409s, episode steps: 74, steps per second: 181, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.241 [-0.225, 1.483], loss: 7.696715, mean_absolute_error: 14.851492, mean_q: 28.518427\n",
      "Episode 393 reward: 75.0\n",
      " 13312/100000: episode: 393, duration: 0.417s, episode steps: 75, steps per second: 180, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.258 [-0.235, 1.477], loss: 5.842829, mean_absolute_error: 14.805501, mean_q: 28.610277\n",
      "Episode 394 reward: 80.0\n",
      " 13392/100000: episode: 394, duration: 0.443s, episode steps: 80, steps per second: 181, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.233 [-0.235, 1.419], loss: 6.298131, mean_absolute_error: 14.879626, mean_q: 28.701046\n",
      "Episode 395 reward: 64.0\n",
      " 13456/100000: episode: 395, duration: 0.364s, episode steps: 64, steps per second: 176, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.134 [-0.732, 0.303], loss: 6.277734, mean_absolute_error: 14.928932, mean_q: 28.834145\n",
      "Episode 396 reward: 86.0\n",
      " 13542/100000: episode: 396, duration: 0.472s, episode steps: 86, steps per second: 182, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.281 [-0.407, 1.436], loss: 5.590159, mean_absolute_error: 14.956816, mean_q: 28.935600\n",
      "Episode 397 reward: 122.0\n",
      " 13664/100000: episode: 397, duration: 0.671s, episode steps: 122, steps per second: 182, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.195 [-0.463, 1.489], loss: 6.571887, mean_absolute_error: 14.993160, mean_q: 28.983643\n",
      "Episode 398 reward: 97.0\n",
      " 13761/100000: episode: 398, duration: 0.532s, episode steps: 97, steps per second: 182, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.228 [-0.446, 1.468], loss: 5.101341, mean_absolute_error: 14.979954, mean_q: 29.014528\n",
      "Episode 399 reward: 81.0\n",
      " 13842/100000: episode: 399, duration: 0.494s, episode steps: 81, steps per second: 164, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.288 [-0.427, 1.598], loss: 5.010233, mean_absolute_error: 15.067562, mean_q: 29.226452\n",
      "Episode 400 reward: 101.0\n",
      " 13943/100000: episode: 400, duration: 0.575s, episode steps: 101, steps per second: 176, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.241 [-0.386, 1.654], loss: 5.841162, mean_absolute_error: 15.150879, mean_q: 29.287844\n",
      "Episode 401 reward: 92.0\n",
      " 14035/100000: episode: 401, duration: 0.520s, episode steps: 92, steps per second: 177, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.265 [-0.437, 1.588], loss: 6.583259, mean_absolute_error: 15.309266, mean_q: 29.595053\n",
      "Episode 402 reward: 77.0\n",
      " 14112/100000: episode: 402, duration: 0.425s, episode steps: 77, steps per second: 181, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.080 [-0.750, 0.347], loss: 8.030457, mean_absolute_error: 15.279689, mean_q: 29.477535\n",
      "Episode 403 reward: 200.0\n",
      " 14312/100000: episode: 403, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.080 [-0.433, 1.288], loss: 6.796681, mean_absolute_error: 15.275843, mean_q: 29.536875\n",
      "Episode 404 reward: 111.0\n",
      " 14423/100000: episode: 404, duration: 0.646s, episode steps: 111, steps per second: 172, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.228 [-0.527, 1.432], loss: 6.693336, mean_absolute_error: 15.373580, mean_q: 29.751276\n",
      "Episode 405 reward: 124.0\n",
      " 14547/100000: episode: 405, duration: 0.681s, episode steps: 124, steps per second: 182, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.213 [-0.446, 1.655], loss: 5.831405, mean_absolute_error: 15.433625, mean_q: 29.972980\n",
      "Episode 406 reward: 71.0\n",
      " 14618/100000: episode: 406, duration: 0.395s, episode steps: 71, steps per second: 180, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.132 [-0.722, 0.364], loss: 7.004361, mean_absolute_error: 15.591002, mean_q: 30.184374\n",
      "Episode 407 reward: 52.0\n",
      " 14670/100000: episode: 407, duration: 0.294s, episode steps: 52, steps per second: 177, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.149 [-0.754, 0.276], loss: 5.568608, mean_absolute_error: 15.652374, mean_q: 30.381554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 408 reward: 72.0\n",
      " 14742/100000: episode: 408, duration: 0.392s, episode steps: 72, steps per second: 184, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.100 [-0.935, 0.418], loss: 6.155410, mean_absolute_error: 15.760193, mean_q: 30.585104\n",
      "Episode 409 reward: 108.0\n",
      " 14850/100000: episode: 409, duration: 0.613s, episode steps: 108, steps per second: 176, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.268 [-0.377, 1.793], loss: 7.499080, mean_absolute_error: 15.837807, mean_q: 30.676731\n",
      "Episode 410 reward: 181.0\n",
      " 15031/100000: episode: 410, duration: 0.998s, episode steps: 181, steps per second: 181, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.133 [-0.858, 0.338], loss: 7.164948, mean_absolute_error: 15.854271, mean_q: 30.723703\n",
      "Episode 411 reward: 118.0\n",
      " 15149/100000: episode: 411, duration: 0.647s, episode steps: 118, steps per second: 182, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.245 [-0.415, 1.679], loss: 7.239357, mean_absolute_error: 16.059309, mean_q: 31.114346\n",
      "Episode 412 reward: 107.0\n",
      " 15256/100000: episode: 412, duration: 0.591s, episode steps: 107, steps per second: 181, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.269 [-0.688, 1.669], loss: 5.624620, mean_absolute_error: 16.025469, mean_q: 31.202694\n",
      "Episode 413 reward: 95.0\n",
      " 15351/100000: episode: 413, duration: 0.514s, episode steps: 95, steps per second: 185, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.316 [-0.353, 1.661], loss: 6.867991, mean_absolute_error: 16.125788, mean_q: 31.284658\n",
      "Episode 414 reward: 80.0\n",
      " 15431/100000: episode: 414, duration: 0.451s, episode steps: 80, steps per second: 177, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.331 [-0.352, 1.864], loss: 6.428247, mean_absolute_error: 16.080652, mean_q: 31.255138\n",
      "Episode 415 reward: 111.0\n",
      " 15542/100000: episode: 415, duration: 0.614s, episode steps: 111, steps per second: 181, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.274 [-0.476, 1.622], loss: 7.824359, mean_absolute_error: 16.211399, mean_q: 31.438606\n",
      "Episode 416 reward: 118.0\n",
      " 15660/100000: episode: 416, duration: 0.655s, episode steps: 118, steps per second: 180, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.246 [-0.565, 1.501], loss: 5.785329, mean_absolute_error: 16.295736, mean_q: 31.746254\n",
      "Episode 417 reward: 94.0\n",
      " 15754/100000: episode: 417, duration: 0.511s, episode steps: 94, steps per second: 184, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.360 [-0.352, 2.030], loss: 5.640221, mean_absolute_error: 16.402113, mean_q: 31.984461\n",
      "Episode 418 reward: 97.0\n",
      " 15851/100000: episode: 418, duration: 0.533s, episode steps: 97, steps per second: 182, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.331 [-0.436, 1.663], loss: 5.762786, mean_absolute_error: 16.421457, mean_q: 32.010242\n",
      "Episode 419 reward: 100.0\n",
      " 15951/100000: episode: 419, duration: 0.551s, episode steps: 100, steps per second: 181, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.328 [-0.418, 1.684], loss: 7.524294, mean_absolute_error: 16.489214, mean_q: 31.995546\n",
      "Episode 420 reward: 158.0\n",
      " 16109/100000: episode: 420, duration: 0.872s, episode steps: 158, steps per second: 181, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.216 [-0.478, 1.790], loss: 6.653144, mean_absolute_error: 16.567713, mean_q: 32.199818\n",
      "Episode 421 reward: 97.0\n",
      " 16206/100000: episode: 421, duration: 0.545s, episode steps: 97, steps per second: 178, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.320 [-0.346, 1.663], loss: 6.375152, mean_absolute_error: 16.672029, mean_q: 32.450043\n",
      "Episode 422 reward: 97.0\n",
      " 16303/100000: episode: 422, duration: 0.566s, episode steps: 97, steps per second: 171, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.329 [-0.527, 1.692], loss: 6.957778, mean_absolute_error: 16.795099, mean_q: 32.696125\n",
      "Episode 423 reward: 179.0\n",
      " 16482/100000: episode: 423, duration: 0.994s, episode steps: 179, steps per second: 180, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.223 [-0.541, 1.998], loss: 7.180067, mean_absolute_error: 16.782713, mean_q: 32.632423\n",
      "Episode 424 reward: 200.0\n",
      " 16682/100000: episode: 424, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.128 [-0.593, 1.458], loss: 6.926422, mean_absolute_error: 16.959288, mean_q: 33.038528\n",
      "Episode 425 reward: 200.0\n",
      " 16882/100000: episode: 425, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-0.572, 0.521], loss: 6.056545, mean_absolute_error: 17.027937, mean_q: 33.216381\n",
      "Episode 426 reward: 167.0\n",
      " 17049/100000: episode: 426, duration: 0.996s, episode steps: 167, steps per second: 168, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.207 [-0.631, 1.618], loss: 7.050998, mean_absolute_error: 17.295559, mean_q: 33.691109\n",
      "Episode 427 reward: 120.0\n",
      " 17169/100000: episode: 427, duration: 0.697s, episode steps: 120, steps per second: 172, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.314 [-0.335, 1.842], loss: 6.688066, mean_absolute_error: 17.357780, mean_q: 33.842258\n",
      "Episode 428 reward: 103.0\n",
      " 17272/100000: episode: 428, duration: 0.655s, episode steps: 103, steps per second: 157, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.313 [-0.457, 1.645], loss: 6.827547, mean_absolute_error: 17.379698, mean_q: 33.905582\n",
      "Episode 429 reward: 118.0\n",
      " 17390/100000: episode: 429, duration: 0.680s, episode steps: 118, steps per second: 174, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.340 [-0.373, 1.838], loss: 7.018872, mean_absolute_error: 17.442841, mean_q: 34.031662\n",
      "Episode 430 reward: 49.0\n",
      " 17439/100000: episode: 430, duration: 0.278s, episode steps: 49, steps per second: 177, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.146 [-0.783, 0.225], loss: 5.978235, mean_absolute_error: 17.672123, mean_q: 34.502575\n",
      "Episode 431 reward: 188.0\n",
      " 17627/100000: episode: 431, duration: 1.065s, episode steps: 188, steps per second: 177, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.203 [-0.531, 1.809], loss: 7.388754, mean_absolute_error: 17.614510, mean_q: 34.328728\n",
      "Episode 432 reward: 87.0\n",
      " 17714/100000: episode: 432, duration: 0.491s, episode steps: 87, steps per second: 177, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.064 [-0.867, 0.427], loss: 5.227926, mean_absolute_error: 17.650545, mean_q: 34.585205\n",
      "Episode 433 reward: 54.0\n",
      " 17768/100000: episode: 433, duration: 0.306s, episode steps: 54, steps per second: 176, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.126 [-0.756, 0.356], loss: 8.707457, mean_absolute_error: 17.787540, mean_q: 34.735764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 434 reward: 57.0\n",
      " 17825/100000: episode: 434, duration: 0.325s, episode steps: 57, steps per second: 175, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.155 [-0.949, 0.183], loss: 7.545706, mean_absolute_error: 17.928652, mean_q: 34.923744\n",
      "Episode 435 reward: 137.0\n",
      " 17962/100000: episode: 435, duration: 0.808s, episode steps: 137, steps per second: 169, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.309 [-0.500, 1.859], loss: 7.263951, mean_absolute_error: 17.912479, mean_q: 34.992485\n",
      "Episode 436 reward: 41.0\n",
      " 18003/100000: episode: 436, duration: 0.232s, episode steps: 41, steps per second: 177, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.158 [-0.722, 0.267], loss: 5.100492, mean_absolute_error: 17.891886, mean_q: 35.180859\n",
      "Episode 437 reward: 45.0\n",
      " 18048/100000: episode: 437, duration: 0.256s, episode steps: 45, steps per second: 176, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.140 [-0.903, 0.203], loss: 6.944775, mean_absolute_error: 18.140047, mean_q: 35.493359\n",
      "Episode 438 reward: 67.0\n",
      " 18115/100000: episode: 438, duration: 0.378s, episode steps: 67, steps per second: 177, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.107 [-0.870, 0.422], loss: 7.591149, mean_absolute_error: 18.113117, mean_q: 35.346283\n",
      "Episode 439 reward: 74.0\n",
      " 18189/100000: episode: 439, duration: 0.418s, episode steps: 74, steps per second: 177, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.115 [-0.692, 0.467], loss: 10.172748, mean_absolute_error: 18.258553, mean_q: 35.510105\n",
      "Episode 440 reward: 65.0\n",
      " 18254/100000: episode: 440, duration: 0.403s, episode steps: 65, steps per second: 161, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.165 [-0.932, 0.532], loss: 8.806772, mean_absolute_error: 18.275909, mean_q: 35.561436\n",
      "Episode 441 reward: 122.0\n",
      " 18376/100000: episode: 441, duration: 0.764s, episode steps: 122, steps per second: 160, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.318 [-0.280, 1.859], loss: 7.771799, mean_absolute_error: 18.312273, mean_q: 35.703178\n",
      "Episode 442 reward: 66.0\n",
      " 18442/100000: episode: 442, duration: 0.382s, episode steps: 66, steps per second: 173, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.095 [-0.838, 0.353], loss: 6.945744, mean_absolute_error: 18.260246, mean_q: 35.713566\n",
      "Episode 443 reward: 51.0\n",
      " 18493/100000: episode: 443, duration: 0.301s, episode steps: 51, steps per second: 170, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.156 [-0.911, 0.437], loss: 8.406873, mean_absolute_error: 18.419197, mean_q: 35.941090\n",
      "Episode 444 reward: 163.0\n",
      " 18656/100000: episode: 444, duration: 0.894s, episode steps: 163, steps per second: 182, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.276 [-0.594, 2.035], loss: 8.086048, mean_absolute_error: 18.476408, mean_q: 36.077351\n",
      "Episode 445 reward: 111.0\n",
      " 18767/100000: episode: 445, duration: 0.605s, episode steps: 111, steps per second: 184, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.063 [-1.131, 0.404], loss: 6.344067, mean_absolute_error: 18.647522, mean_q: 36.518673\n",
      "Episode 446 reward: 59.0\n",
      " 18826/100000: episode: 446, duration: 0.309s, episode steps: 59, steps per second: 191, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.135 [-0.868, 0.345], loss: 6.830643, mean_absolute_error: 18.638685, mean_q: 36.516304\n",
      "Episode 447 reward: 44.0\n",
      " 18870/100000: episode: 447, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.136 [-0.766, 0.221], loss: 8.338823, mean_absolute_error: 18.802944, mean_q: 36.687088\n",
      "Episode 448 reward: 68.0\n",
      " 18938/100000: episode: 448, duration: 0.352s, episode steps: 68, steps per second: 193, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.097 [-0.752, 0.374], loss: 8.034673, mean_absolute_error: 18.860460, mean_q: 36.827496\n",
      "Episode 449 reward: 74.0\n",
      " 19012/100000: episode: 449, duration: 0.406s, episode steps: 74, steps per second: 182, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.106 [-0.873, 0.243], loss: 7.131944, mean_absolute_error: 18.948084, mean_q: 37.078854\n",
      "Episode 450 reward: 45.0\n",
      " 19057/100000: episode: 450, duration: 0.236s, episode steps: 45, steps per second: 190, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.136 [-0.876, 0.421], loss: 10.739991, mean_absolute_error: 19.166136, mean_q: 37.223743\n",
      "Episode 451 reward: 55.0\n",
      " 19112/100000: episode: 451, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.155 [-0.895, 0.318], loss: 9.100984, mean_absolute_error: 19.024115, mean_q: 37.066185\n",
      "Episode 452 reward: 155.0\n",
      " 19267/100000: episode: 452, duration: 0.780s, episode steps: 155, steps per second: 199, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.290 [-0.362, 1.984], loss: 9.562103, mean_absolute_error: 19.104691, mean_q: 37.190399\n",
      "Episode 453 reward: 74.0\n",
      " 19341/100000: episode: 453, duration: 0.396s, episode steps: 74, steps per second: 187, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.115 [-0.888, 0.527], loss: 10.033910, mean_absolute_error: 19.067963, mean_q: 37.089439\n",
      "Episode 454 reward: 131.0\n",
      " 19472/100000: episode: 454, duration: 0.676s, episode steps: 131, steps per second: 194, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.343 [-0.299, 1.959], loss: 8.701291, mean_absolute_error: 19.115152, mean_q: 37.295383\n",
      "Episode 455 reward: 187.0\n",
      " 19659/100000: episode: 455, duration: 1.046s, episode steps: 187, steps per second: 179, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.241 [-0.515, 2.001], loss: 8.762168, mean_absolute_error: 19.194221, mean_q: 37.437744\n",
      "Episode 456 reward: 75.0\n",
      " 19734/100000: episode: 456, duration: 0.422s, episode steps: 75, steps per second: 178, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.123 [-0.920, 0.235], loss: 10.680480, mean_absolute_error: 19.232714, mean_q: 37.398670\n",
      "Episode 457 reward: 200.0\n",
      " 19934/100000: episode: 457, duration: 1.093s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.159 [-0.561, 1.244], loss: 8.381888, mean_absolute_error: 19.299637, mean_q: 37.684933\n",
      "Episode 458 reward: 67.0\n",
      " 20001/100000: episode: 458, duration: 0.400s, episode steps: 67, steps per second: 167, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.118 [-0.939, 0.352], loss: 7.790703, mean_absolute_error: 19.462849, mean_q: 38.108006\n",
      "Episode 459 reward: 51.0\n",
      " 20052/100000: episode: 459, duration: 0.281s, episode steps: 51, steps per second: 182, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.175 [-0.933, 0.196], loss: 9.136375, mean_absolute_error: 19.532986, mean_q: 38.151466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 460 reward: 57.0\n",
      " 20109/100000: episode: 460, duration: 0.329s, episode steps: 57, steps per second: 174, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.172 [-0.938, 0.397], loss: 10.213633, mean_absolute_error: 19.552258, mean_q: 38.115520\n",
      "Episode 461 reward: 38.0\n",
      " 20147/100000: episode: 461, duration: 0.215s, episode steps: 38, steps per second: 177, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.173 [-0.744, 0.178], loss: 12.880100, mean_absolute_error: 19.689806, mean_q: 38.165436\n",
      "Episode 462 reward: 93.0\n",
      " 20240/100000: episode: 462, duration: 0.522s, episode steps: 93, steps per second: 178, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.068 [-0.886, 0.404], loss: 8.568763, mean_absolute_error: 19.645985, mean_q: 38.412525\n",
      "Episode 463 reward: 51.0\n",
      " 20291/100000: episode: 463, duration: 0.295s, episode steps: 51, steps per second: 173, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.145 [-0.936, 0.221], loss: 8.455376, mean_absolute_error: 19.623171, mean_q: 38.404472\n",
      "Episode 464 reward: 92.0\n",
      " 20383/100000: episode: 464, duration: 0.521s, episode steps: 92, steps per second: 177, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.093 [-1.041, 0.274], loss: 9.810204, mean_absolute_error: 19.819883, mean_q: 38.655930\n",
      "Episode 465 reward: 77.0\n",
      " 20460/100000: episode: 465, duration: 0.440s, episode steps: 77, steps per second: 175, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.115 [-0.885, 0.234], loss: 9.886881, mean_absolute_error: 19.705244, mean_q: 38.444305\n",
      "Episode 466 reward: 70.0\n",
      " 20530/100000: episode: 466, duration: 0.400s, episode steps: 70, steps per second: 175, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.124 [-0.887, 0.325], loss: 9.482407, mean_absolute_error: 19.871660, mean_q: 38.698242\n",
      "Episode 467 reward: 200.0\n",
      " 20730/100000: episode: 467, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.235 [-0.590, 1.860], loss: 7.692839, mean_absolute_error: 19.928198, mean_q: 38.966557\n",
      "Episode 468 reward: 55.0\n",
      " 20785/100000: episode: 468, duration: 0.305s, episode steps: 55, steps per second: 180, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.145 [-0.886, 0.329], loss: 9.052979, mean_absolute_error: 20.116129, mean_q: 39.214645\n",
      "Episode 469 reward: 88.0\n",
      " 20873/100000: episode: 469, duration: 0.487s, episode steps: 88, steps per second: 181, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.115 [-1.064, 0.412], loss: 8.580688, mean_absolute_error: 20.066301, mean_q: 39.131947\n",
      "Episode 470 reward: 67.0\n",
      " 20940/100000: episode: 470, duration: 0.369s, episode steps: 67, steps per second: 182, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.186 [-1.128, 0.358], loss: 10.684599, mean_absolute_error: 20.243608, mean_q: 39.305984\n",
      "Episode 471 reward: 146.0\n",
      " 21086/100000: episode: 471, duration: 0.800s, episode steps: 146, steps per second: 183, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.363 [-0.474, 2.184], loss: 9.179700, mean_absolute_error: 20.105810, mean_q: 39.264317\n",
      "Episode 472 reward: 56.0\n",
      " 21142/100000: episode: 472, duration: 0.319s, episode steps: 56, steps per second: 176, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.136 [-0.866, 0.332], loss: 10.890699, mean_absolute_error: 20.244360, mean_q: 39.480610\n",
      "Episode 473 reward: 64.0\n",
      " 21206/100000: episode: 473, duration: 0.355s, episode steps: 64, steps per second: 180, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.142 [-0.907, 0.389], loss: 8.566870, mean_absolute_error: 20.341917, mean_q: 39.714645\n",
      "Episode 474 reward: 77.0\n",
      " 21283/100000: episode: 474, duration: 0.426s, episode steps: 77, steps per second: 181, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.105 [-0.889, 0.213], loss: 9.510468, mean_absolute_error: 20.402161, mean_q: 39.864834\n",
      "Episode 475 reward: 48.0\n",
      " 21331/100000: episode: 475, duration: 0.266s, episode steps: 48, steps per second: 181, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.132 [-0.830, 0.183], loss: 7.284676, mean_absolute_error: 20.296844, mean_q: 39.804489\n",
      "Episode 476 reward: 58.0\n",
      " 21389/100000: episode: 476, duration: 0.332s, episode steps: 58, steps per second: 175, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.119 [-0.919, 0.377], loss: 10.292571, mean_absolute_error: 20.433701, mean_q: 39.941219\n",
      "Episode 477 reward: 90.0\n",
      " 21479/100000: episode: 477, duration: 0.499s, episode steps: 90, steps per second: 181, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.099 [-0.891, 0.408], loss: 9.145993, mean_absolute_error: 20.528465, mean_q: 40.065163\n",
      "Episode 478 reward: 58.0\n",
      " 21537/100000: episode: 478, duration: 0.322s, episode steps: 58, steps per second: 180, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.169 [-0.914, 0.202], loss: 10.495179, mean_absolute_error: 20.517107, mean_q: 40.028652\n",
      "Episode 479 reward: 134.0\n",
      " 21671/100000: episode: 479, duration: 0.723s, episode steps: 134, steps per second: 185, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.065 [-1.060, 0.415], loss: 9.560963, mean_absolute_error: 20.654238, mean_q: 40.306984\n",
      "Episode 480 reward: 168.0\n",
      " 21839/100000: episode: 480, duration: 0.933s, episode steps: 168, steps per second: 180, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.341 [-0.467, 2.231], loss: 9.797605, mean_absolute_error: 20.792837, mean_q: 40.558247\n",
      "Episode 481 reward: 78.0\n",
      " 21917/100000: episode: 481, duration: 0.433s, episode steps: 78, steps per second: 180, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.127 [-1.099, 0.205], loss: 8.003566, mean_absolute_error: 20.830494, mean_q: 40.815842\n",
      "Episode 482 reward: 136.0\n",
      " 22053/100000: episode: 482, duration: 0.752s, episode steps: 136, steps per second: 181, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.080 [-0.941, 0.542], loss: 10.237340, mean_absolute_error: 20.837950, mean_q: 40.662884\n",
      "Episode 483 reward: 57.0\n",
      " 22110/100000: episode: 483, duration: 0.306s, episode steps: 57, steps per second: 186, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.175 [-0.954, 0.425], loss: 8.518962, mean_absolute_error: 20.930466, mean_q: 40.960728\n",
      "Episode 484 reward: 193.0\n",
      " 22303/100000: episode: 484, duration: 1.065s, episode steps: 193, steps per second: 181, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.318 [-0.477, 2.416], loss: 10.798087, mean_absolute_error: 21.032171, mean_q: 41.090584\n",
      "Episode 485 reward: 113.0\n",
      " 22416/100000: episode: 485, duration: 0.626s, episode steps: 113, steps per second: 181, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.073 [-0.918, 0.410], loss: 12.016047, mean_absolute_error: 21.146025, mean_q: 41.180542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 486 reward: 55.0\n",
      " 22471/100000: episode: 486, duration: 0.310s, episode steps: 55, steps per second: 178, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.163 [-0.915, 0.396], loss: 9.101160, mean_absolute_error: 21.161612, mean_q: 41.325748\n",
      "Episode 487 reward: 186.0\n",
      " 22657/100000: episode: 487, duration: 1.030s, episode steps: 186, steps per second: 181, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.304 [-0.777, 2.201], loss: 9.044780, mean_absolute_error: 21.281351, mean_q: 41.622658\n",
      "Episode 488 reward: 200.0\n",
      " 22857/100000: episode: 488, duration: 1.078s, episode steps: 200, steps per second: 186, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.284 [-0.769, 2.021], loss: 8.460954, mean_absolute_error: 21.381245, mean_q: 41.912022\n",
      "Episode 489 reward: 67.0\n",
      " 22924/100000: episode: 489, duration: 0.369s, episode steps: 67, steps per second: 181, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.145 [-1.043, 0.254], loss: 10.672243, mean_absolute_error: 21.630070, mean_q: 42.241283\n",
      "Episode 490 reward: 56.0\n",
      " 22980/100000: episode: 490, duration: 0.314s, episode steps: 56, steps per second: 178, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.158 [-1.109, 0.377], loss: 9.083924, mean_absolute_error: 21.601603, mean_q: 42.374168\n",
      "Episode 491 reward: 72.0\n",
      " 23052/100000: episode: 491, duration: 0.409s, episode steps: 72, steps per second: 176, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.159 [-0.952, 0.465], loss: 10.226868, mean_absolute_error: 21.762163, mean_q: 42.520187\n",
      "Episode 492 reward: 69.0\n",
      " 23121/100000: episode: 492, duration: 0.382s, episode steps: 69, steps per second: 181, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.141 [-0.924, 0.402], loss: 7.686712, mean_absolute_error: 21.766518, mean_q: 42.702892\n",
      "Episode 493 reward: 60.0\n",
      " 23181/100000: episode: 493, duration: 0.329s, episode steps: 60, steps per second: 183, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.147 [-1.039, 0.250], loss: 9.811997, mean_absolute_error: 21.887676, mean_q: 42.831902\n",
      "Episode 494 reward: 110.0\n",
      " 23291/100000: episode: 494, duration: 0.614s, episode steps: 110, steps per second: 179, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.084 [-1.056, 0.424], loss: 11.877862, mean_absolute_error: 21.921297, mean_q: 42.738625\n",
      "Episode 495 reward: 70.0\n",
      " 23361/100000: episode: 495, duration: 0.391s, episode steps: 70, steps per second: 179, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.133 [-0.958, 0.213], loss: 12.100968, mean_absolute_error: 21.944130, mean_q: 42.714684\n",
      "Episode 496 reward: 149.0\n",
      " 23510/100000: episode: 496, duration: 0.818s, episode steps: 149, steps per second: 182, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.072 [-1.046, 0.444], loss: 9.513085, mean_absolute_error: 21.923056, mean_q: 42.971130\n",
      "Episode 497 reward: 94.0\n",
      " 23604/100000: episode: 497, duration: 0.519s, episode steps: 94, steps per second: 181, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.098 [-0.806, 0.352], loss: 8.744142, mean_absolute_error: 22.123020, mean_q: 43.387547\n",
      "Episode 498 reward: 80.0\n",
      " 23684/100000: episode: 498, duration: 0.440s, episode steps: 80, steps per second: 182, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.165 [-1.326, 0.488], loss: 11.724167, mean_absolute_error: 22.261826, mean_q: 43.532207\n",
      "Episode 499 reward: 89.0\n",
      " 23773/100000: episode: 499, duration: 0.502s, episode steps: 89, steps per second: 177, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.146 [-1.093, 0.226], loss: 10.252462, mean_absolute_error: 22.229303, mean_q: 43.469410\n",
      "Episode 500 reward: 83.0\n",
      " 23856/100000: episode: 500, duration: 0.458s, episode steps: 83, steps per second: 181, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.103 [-0.917, 0.381], loss: 13.515712, mean_absolute_error: 22.302591, mean_q: 43.497356\n",
      "Episode 501 reward: 99.0\n",
      " 23955/100000: episode: 501, duration: 0.551s, episode steps: 99, steps per second: 180, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.132 [-1.061, 0.433], loss: 10.613614, mean_absolute_error: 22.219828, mean_q: 43.460903\n",
      "Episode 502 reward: 200.0\n",
      " 24155/100000: episode: 502, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.158 [-0.570, 1.502], loss: 10.383178, mean_absolute_error: 22.402353, mean_q: 43.835480\n",
      "Episode 503 reward: 82.0\n",
      " 24237/100000: episode: 503, duration: 0.457s, episode steps: 82, steps per second: 179, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.161 [-1.066, 0.256], loss: 11.969563, mean_absolute_error: 22.516279, mean_q: 43.955585\n",
      "Episode 504 reward: 62.0\n",
      " 24299/100000: episode: 504, duration: 0.344s, episode steps: 62, steps per second: 180, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.200 [-1.126, 0.408], loss: 9.061959, mean_absolute_error: 22.495399, mean_q: 44.250057\n",
      "Episode 505 reward: 166.0\n",
      " 24465/100000: episode: 505, duration: 0.919s, episode steps: 166, steps per second: 181, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.088 [-1.243, 0.332], loss: 9.720056, mean_absolute_error: 22.659168, mean_q: 44.405079\n",
      "Episode 506 reward: 152.0\n",
      " 24617/100000: episode: 506, duration: 0.832s, episode steps: 152, steps per second: 183, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.102 [-1.061, 0.575], loss: 10.761886, mean_absolute_error: 22.748121, mean_q: 44.587257\n",
      "Episode 507 reward: 141.0\n",
      " 24758/100000: episode: 507, duration: 0.766s, episode steps: 141, steps per second: 184, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.409 [-0.561, 2.374], loss: 11.751210, mean_absolute_error: 22.840874, mean_q: 44.664207\n",
      "Episode 508 reward: 118.0\n",
      " 24876/100000: episode: 508, duration: 0.659s, episode steps: 118, steps per second: 179, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.125 [-1.128, 0.514], loss: 10.660070, mean_absolute_error: 22.921118, mean_q: 44.900963\n",
      "Episode 509 reward: 85.0\n",
      " 24961/100000: episode: 509, duration: 0.468s, episode steps: 85, steps per second: 182, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.170 [-1.238, 0.233], loss: 9.633308, mean_absolute_error: 22.981127, mean_q: 45.165546\n",
      "Episode 510 reward: 75.0\n",
      " 25036/100000: episode: 510, duration: 0.418s, episode steps: 75, steps per second: 180, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.187 [-1.122, 0.384], loss: 12.736290, mean_absolute_error: 23.054689, mean_q: 45.054653\n",
      "Episode 511 reward: 79.0\n",
      " 25115/100000: episode: 511, duration: 0.438s, episode steps: 79, steps per second: 180, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.175 [-1.106, 0.487], loss: 9.375390, mean_absolute_error: 23.050423, mean_q: 45.270889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 512 reward: 141.0\n",
      " 25256/100000: episode: 512, duration: 0.756s, episode steps: 141, steps per second: 186, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.103 [-1.279, 0.572], loss: 9.206371, mean_absolute_error: 23.121870, mean_q: 45.440636\n",
      "Episode 513 reward: 198.0\n",
      " 25454/100000: episode: 513, duration: 1.087s, episode steps: 198, steps per second: 182, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.333 [-0.524, 2.423], loss: 11.324561, mean_absolute_error: 23.374090, mean_q: 45.834366\n",
      "Episode 514 reward: 200.0\n",
      " 25654/100000: episode: 514, duration: 1.089s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.199 [-0.474, 1.430], loss: 10.689075, mean_absolute_error: 23.477943, mean_q: 46.081913\n",
      "Episode 515 reward: 64.0\n",
      " 25718/100000: episode: 515, duration: 0.353s, episode steps: 64, steps per second: 181, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.134 [-0.896, 0.263], loss: 9.100462, mean_absolute_error: 23.546474, mean_q: 46.322872\n",
      "Episode 516 reward: 69.0\n",
      " 25787/100000: episode: 516, duration: 0.382s, episode steps: 69, steps per second: 181, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.207 [-1.277, 0.363], loss: 15.859776, mean_absolute_error: 23.595558, mean_q: 46.062675\n",
      "Episode 517 reward: 60.0\n",
      " 25847/100000: episode: 517, duration: 0.327s, episode steps: 60, steps per second: 183, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.185 [-1.141, 0.247], loss: 11.149464, mean_absolute_error: 23.740370, mean_q: 46.607151\n",
      "Episode 518 reward: 58.0\n",
      " 25905/100000: episode: 518, duration: 0.323s, episode steps: 58, steps per second: 180, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.205 [-1.112, 0.372], loss: 13.551934, mean_absolute_error: 23.511974, mean_q: 46.048832\n",
      "Episode 519 reward: 66.0\n",
      " 25971/100000: episode: 519, duration: 0.364s, episode steps: 66, steps per second: 181, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.199 [-1.142, 0.229], loss: 12.414275, mean_absolute_error: 23.625402, mean_q: 46.385880\n",
      "Episode 520 reward: 48.0\n",
      " 26019/100000: episode: 520, duration: 0.279s, episode steps: 48, steps per second: 172, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.196 [-1.136, 0.346], loss: 13.402270, mean_absolute_error: 23.806320, mean_q: 46.589340\n",
      "Episode 521 reward: 73.0\n",
      " 26092/100000: episode: 521, duration: 0.413s, episode steps: 73, steps per second: 177, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.231 [-1.670, 0.648], loss: 11.267272, mean_absolute_error: 23.800835, mean_q: 46.642315\n",
      "Episode 522 reward: 61.0\n",
      " 26153/100000: episode: 522, duration: 0.401s, episode steps: 61, steps per second: 152, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.230 [-1.277, 0.386], loss: 11.516687, mean_absolute_error: 23.921885, mean_q: 46.850418\n",
      "Episode 523 reward: 74.0\n",
      " 26227/100000: episode: 523, duration: 0.408s, episode steps: 74, steps per second: 181, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.197 [-1.240, 0.428], loss: 9.782580, mean_absolute_error: 23.897743, mean_q: 46.964443\n",
      "Episode 524 reward: 168.0\n",
      " 26395/100000: episode: 524, duration: 0.934s, episode steps: 168, steps per second: 180, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.074 [-1.096, 0.851], loss: 10.934521, mean_absolute_error: 24.003872, mean_q: 47.090565\n",
      "Episode 525 reward: 76.0\n",
      " 26471/100000: episode: 525, duration: 0.426s, episode steps: 76, steps per second: 178, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.178 [-1.116, 0.366], loss: 11.189069, mean_absolute_error: 24.107088, mean_q: 47.312664\n",
      "Episode 526 reward: 94.0\n",
      " 26565/100000: episode: 526, duration: 0.560s, episode steps: 94, steps per second: 168, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.135 [-1.237, 0.355], loss: 11.013102, mean_absolute_error: 24.227121, mean_q: 47.583984\n",
      "Episode 527 reward: 200.0\n",
      " 26765/100000: episode: 527, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.083 [-0.485, 0.742], loss: 11.598523, mean_absolute_error: 24.322809, mean_q: 47.763340\n",
      "Episode 528 reward: 104.0\n",
      " 26869/100000: episode: 528, duration: 0.566s, episode steps: 104, steps per second: 184, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.157 [-1.138, 0.351], loss: 11.906447, mean_absolute_error: 24.432608, mean_q: 48.001381\n",
      "Episode 529 reward: 69.0\n",
      " 26938/100000: episode: 529, duration: 0.385s, episode steps: 69, steps per second: 179, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.206 [-1.251, 0.218], loss: 11.466993, mean_absolute_error: 24.547108, mean_q: 48.245071\n",
      "Episode 530 reward: 107.0\n",
      " 27045/100000: episode: 530, duration: 0.566s, episode steps: 107, steps per second: 189, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.160 [-1.090, 0.679], loss: 13.344812, mean_absolute_error: 24.586447, mean_q: 48.199093\n",
      "Episode 531 reward: 68.0\n",
      " 27113/100000: episode: 531, duration: 0.373s, episode steps: 68, steps per second: 182, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.215 [-1.258, 0.390], loss: 14.044753, mean_absolute_error: 24.643541, mean_q: 48.285763\n",
      "Episode 532 reward: 70.0\n",
      " 27183/100000: episode: 532, duration: 0.380s, episode steps: 70, steps per second: 184, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.208 [-1.138, 0.378], loss: 12.378844, mean_absolute_error: 24.684437, mean_q: 48.418907\n",
      "Episode 533 reward: 85.0\n",
      " 27268/100000: episode: 533, duration: 0.461s, episode steps: 85, steps per second: 185, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.175 [-1.314, 0.199], loss: 17.039915, mean_absolute_error: 24.738167, mean_q: 48.324776\n",
      "Episode 534 reward: 81.0\n",
      " 27349/100000: episode: 534, duration: 0.461s, episode steps: 81, steps per second: 176, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.189 [-1.314, 0.220], loss: 12.305246, mean_absolute_error: 24.643322, mean_q: 48.305096\n",
      "Episode 535 reward: 79.0\n",
      " 27428/100000: episode: 535, duration: 0.435s, episode steps: 79, steps per second: 182, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.191 [-1.244, 0.246], loss: 13.775929, mean_absolute_error: 24.781246, mean_q: 48.592602\n",
      "Episode 536 reward: 69.0\n",
      " 27497/100000: episode: 536, duration: 0.371s, episode steps: 69, steps per second: 186, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.214 [-1.141, 0.380], loss: 10.972348, mean_absolute_error: 24.793222, mean_q: 48.684410\n",
      "Episode 537 reward: 79.0\n",
      " 27576/100000: episode: 537, duration: 0.424s, episode steps: 79, steps per second: 186, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.188 [-1.279, 0.247], loss: 11.700686, mean_absolute_error: 24.974100, mean_q: 49.029366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 538 reward: 68.0\n",
      " 27644/100000: episode: 538, duration: 0.370s, episode steps: 68, steps per second: 184, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.217 [-1.266, 0.397], loss: 12.585102, mean_absolute_error: 24.852835, mean_q: 48.787533\n",
      "Episode 539 reward: 103.0\n",
      " 27747/100000: episode: 539, duration: 0.567s, episode steps: 103, steps per second: 182, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.153 [-1.297, 0.376], loss: 11.432095, mean_absolute_error: 25.014179, mean_q: 49.241276\n",
      "Episode 540 reward: 95.0\n",
      " 27842/100000: episode: 540, duration: 0.515s, episode steps: 95, steps per second: 184, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.183 [-1.319, 0.340], loss: 13.695927, mean_absolute_error: 25.075594, mean_q: 49.204288\n",
      "Episode 541 reward: 102.0\n",
      " 27944/100000: episode: 541, duration: 0.570s, episode steps: 102, steps per second: 179, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.129 [-1.058, 0.374], loss: 14.125788, mean_absolute_error: 25.120941, mean_q: 49.230164\n",
      "Episode 542 reward: 63.0\n",
      " 28007/100000: episode: 542, duration: 0.352s, episode steps: 63, steps per second: 179, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.220 [-1.104, 0.213], loss: 12.281576, mean_absolute_error: 25.108143, mean_q: 49.414577\n",
      "Episode 543 reward: 71.0\n",
      " 28078/100000: episode: 543, duration: 0.396s, episode steps: 71, steps per second: 179, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.206 [-1.306, 0.202], loss: 15.046583, mean_absolute_error: 25.258728, mean_q: 49.450363\n",
      "Episode 544 reward: 69.0\n",
      " 28147/100000: episode: 544, duration: 0.397s, episode steps: 69, steps per second: 174, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.198 [-1.247, 0.290], loss: 11.953448, mean_absolute_error: 25.218246, mean_q: 49.549488\n",
      "Episode 545 reward: 93.0\n",
      " 28240/100000: episode: 545, duration: 0.526s, episode steps: 93, steps per second: 177, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.181 [-1.298, 0.367], loss: 14.768244, mean_absolute_error: 25.377581, mean_q: 49.750896\n",
      "Episode 546 reward: 117.0\n",
      " 28357/100000: episode: 546, duration: 0.646s, episode steps: 117, steps per second: 181, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.163 [-1.279, 0.442], loss: 14.044394, mean_absolute_error: 25.257872, mean_q: 49.494404\n",
      "Episode 547 reward: 200.0\n",
      " 28557/100000: episode: 547, duration: 1.101s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.085 [-1.224, 0.726], loss: 12.382480, mean_absolute_error: 25.410976, mean_q: 49.920021\n",
      "Episode 548 reward: 163.0\n",
      " 28720/100000: episode: 548, duration: 0.901s, episode steps: 163, steps per second: 181, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.113 [-1.265, 0.580], loss: 14.406754, mean_absolute_error: 25.612158, mean_q: 50.218506\n",
      "Episode 549 reward: 87.0\n",
      " 28807/100000: episode: 549, duration: 0.482s, episode steps: 87, steps per second: 181, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.212 [-1.317, 0.283], loss: 13.441117, mean_absolute_error: 25.539070, mean_q: 50.120247\n",
      "Episode 550 reward: 102.0\n",
      " 28909/100000: episode: 550, duration: 0.582s, episode steps: 102, steps per second: 175, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.184 [-1.275, 0.513], loss: 12.163743, mean_absolute_error: 25.708677, mean_q: 50.533367\n",
      "Episode 551 reward: 96.0\n",
      " 29005/100000: episode: 551, duration: 0.532s, episode steps: 96, steps per second: 180, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.196 [-1.435, 0.515], loss: 11.207626, mean_absolute_error: 25.788483, mean_q: 50.733166\n",
      "Episode 552 reward: 200.0\n",
      " 29205/100000: episode: 552, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.683, 0.561], loss: 14.409266, mean_absolute_error: 25.774843, mean_q: 50.613636\n",
      "Episode 553 reward: 125.0\n",
      " 29330/100000: episode: 553, duration: 0.692s, episode steps: 125, steps per second: 181, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.162 [-1.310, 0.451], loss: 11.100396, mean_absolute_error: 25.871214, mean_q: 50.934059\n",
      "Episode 554 reward: 122.0\n",
      " 29452/100000: episode: 554, duration: 0.668s, episode steps: 122, steps per second: 183, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.155 [-1.411, 0.545], loss: 14.816034, mean_absolute_error: 25.997541, mean_q: 51.040436\n",
      "Episode 555 reward: 106.0\n",
      " 29558/100000: episode: 555, duration: 0.583s, episode steps: 106, steps per second: 182, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.216 [-1.449, 0.480], loss: 12.901057, mean_absolute_error: 26.079134, mean_q: 51.272549\n",
      "Episode 556 reward: 116.0\n",
      " 29674/100000: episode: 556, duration: 0.663s, episode steps: 116, steps per second: 175, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.181 [-1.315, 0.447], loss: 13.180042, mean_absolute_error: 26.054090, mean_q: 51.293045\n",
      "Episode 557 reward: 200.0\n",
      " 29874/100000: episode: 557, duration: 1.145s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.432, 0.552], loss: 13.338090, mean_absolute_error: 26.243803, mean_q: 51.582451\n",
      "Episode 558 reward: 111.0\n",
      " 29985/100000: episode: 558, duration: 0.638s, episode steps: 111, steps per second: 174, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.194 [-1.305, 0.509], loss: 13.763036, mean_absolute_error: 26.343718, mean_q: 51.793114\n",
      "Episode 559 reward: 193.0\n",
      " 30178/100000: episode: 559, duration: 1.079s, episode steps: 193, steps per second: 179, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.124 [-1.447, 0.777], loss: 12.950628, mean_absolute_error: 26.413309, mean_q: 52.000626\n",
      "Episode 560 reward: 200.0\n",
      " 30378/100000: episode: 560, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.279 [-0.453, 2.078], loss: 14.973802, mean_absolute_error: 26.539467, mean_q: 52.153233\n",
      "Episode 561 reward: 200.0\n",
      " 30578/100000: episode: 561, duration: 1.114s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.597, 0.553], loss: 13.633049, mean_absolute_error: 26.661976, mean_q: 52.495926\n",
      "Episode 562 reward: 95.0\n",
      " 30673/100000: episode: 562, duration: 0.539s, episode steps: 95, steps per second: 176, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.247 [-1.621, 0.403], loss: 11.328603, mean_absolute_error: 26.801401, mean_q: 52.892075\n",
      "Episode 563 reward: 200.0\n",
      " 30873/100000: episode: 563, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.060 [-0.625, 0.602], loss: 14.173498, mean_absolute_error: 26.875759, mean_q: 52.929951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 564 reward: 116.0\n",
      " 30989/100000: episode: 564, duration: 0.638s, episode steps: 116, steps per second: 182, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.222 [-1.499, 0.616], loss: 11.565417, mean_absolute_error: 26.956203, mean_q: 53.219822\n",
      "Episode 565 reward: 134.0\n",
      " 31123/100000: episode: 565, duration: 0.752s, episode steps: 134, steps per second: 178, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.177 [-1.414, 0.348], loss: 13.993960, mean_absolute_error: 27.140373, mean_q: 53.458309\n",
      "Episode 566 reward: 136.0\n",
      " 31259/100000: episode: 566, duration: 0.749s, episode steps: 136, steps per second: 182, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.153 [-1.498, 0.600], loss: 14.524711, mean_absolute_error: 27.153740, mean_q: 53.423553\n",
      "Episode 567 reward: 193.0\n",
      " 31452/100000: episode: 567, duration: 1.074s, episode steps: 193, steps per second: 180, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.333 [-0.742, 2.409], loss: 15.625476, mean_absolute_error: 27.327154, mean_q: 53.760597\n",
      "Episode 568 reward: 171.0\n",
      " 31623/100000: episode: 568, duration: 0.957s, episode steps: 171, steps per second: 179, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.126 [-1.420, 0.483], loss: 14.379806, mean_absolute_error: 27.412886, mean_q: 53.955444\n",
      "Episode 569 reward: 110.0\n",
      " 31733/100000: episode: 569, duration: 0.616s, episode steps: 110, steps per second: 179, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.237 [-1.616, 0.576], loss: 14.156704, mean_absolute_error: 27.480362, mean_q: 54.054588\n",
      "Episode 570 reward: 200.0\n",
      " 31933/100000: episode: 570, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.072 [-0.465, 0.703], loss: 15.688599, mean_absolute_error: 27.530231, mean_q: 54.233208\n",
      "Episode 571 reward: 197.0\n",
      " 32130/100000: episode: 571, duration: 1.112s, episode steps: 197, steps per second: 177, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.131 [-1.472, 0.443], loss: 12.514607, mean_absolute_error: 27.696362, mean_q: 54.673447\n",
      "Episode 572 reward: 95.0\n",
      " 32225/100000: episode: 572, duration: 0.531s, episode steps: 95, steps per second: 179, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.238 [-1.657, 0.521], loss: 14.327246, mean_absolute_error: 27.906496, mean_q: 54.997932\n",
      "Episode 573 reward: 121.0\n",
      " 32346/100000: episode: 573, duration: 0.680s, episode steps: 121, steps per second: 178, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.208 [-1.623, 0.320], loss: 13.264945, mean_absolute_error: 28.052446, mean_q: 55.399906\n",
      "Episode 574 reward: 151.0\n",
      " 32497/100000: episode: 574, duration: 0.853s, episode steps: 151, steps per second: 177, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.172 [-1.587, 0.554], loss: 16.764404, mean_absolute_error: 28.093601, mean_q: 55.298065\n",
      "Episode 575 reward: 101.0\n",
      " 32598/100000: episode: 575, duration: 0.570s, episode steps: 101, steps per second: 177, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.240 [-1.418, 0.499], loss: 14.589581, mean_absolute_error: 28.103262, mean_q: 55.370247\n",
      "Episode 576 reward: 200.0\n",
      " 32798/100000: episode: 576, duration: 1.184s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.375, 0.586], loss: 14.447585, mean_absolute_error: 28.121576, mean_q: 55.522060\n",
      "Episode 577 reward: 101.0\n",
      " 32899/100000: episode: 577, duration: 0.578s, episode steps: 101, steps per second: 175, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.246 [-1.492, 0.424], loss: 17.177122, mean_absolute_error: 28.438133, mean_q: 55.989155\n",
      "Episode 578 reward: 112.0\n",
      " 33011/100000: episode: 578, duration: 0.644s, episode steps: 112, steps per second: 174, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.246 [-1.498, 0.613], loss: 13.918235, mean_absolute_error: 28.370031, mean_q: 55.959866\n",
      "Episode 579 reward: 137.0\n",
      " 33148/100000: episode: 579, duration: 0.785s, episode steps: 137, steps per second: 175, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.192 [-1.622, 0.644], loss: 17.243538, mean_absolute_error: 28.409689, mean_q: 55.864513\n",
      "Episode 580 reward: 150.0\n",
      " 33298/100000: episode: 580, duration: 0.850s, episode steps: 150, steps per second: 176, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.179 [-1.445, 0.409], loss: 15.182205, mean_absolute_error: 28.477013, mean_q: 56.152344\n",
      "Episode 581 reward: 114.0\n",
      " 33412/100000: episode: 581, duration: 0.681s, episode steps: 114, steps per second: 167, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.226 [-1.452, 0.459], loss: 15.754560, mean_absolute_error: 28.566824, mean_q: 56.241226\n",
      "Episode 582 reward: 200.0\n",
      " 33612/100000: episode: 582, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.006 [-0.627, 0.524], loss: 14.491604, mean_absolute_error: 28.634878, mean_q: 56.519184\n",
      "Episode 583 reward: 120.0\n",
      " 33732/100000: episode: 583, duration: 0.703s, episode steps: 120, steps per second: 171, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.254 [-1.498, 0.362], loss: 16.156757, mean_absolute_error: 28.785061, mean_q: 56.747192\n",
      "Episode 584 reward: 159.0\n",
      " 33891/100000: episode: 584, duration: 0.925s, episode steps: 159, steps per second: 172, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.175 [-1.618, 0.429], loss: 11.929595, mean_absolute_error: 28.986193, mean_q: 57.383217\n",
      "Episode 585 reward: 139.0\n",
      " 34030/100000: episode: 585, duration: 0.784s, episode steps: 139, steps per second: 177, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.231 [-1.779, 0.498], loss: 13.267766, mean_absolute_error: 29.023687, mean_q: 57.329010\n",
      "Episode 586 reward: 200.0\n",
      " 34230/100000: episode: 586, duration: 1.142s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.118 [-1.433, 0.506], loss: 14.514129, mean_absolute_error: 29.227715, mean_q: 57.768341\n",
      "Episode 587 reward: 132.0\n",
      " 34362/100000: episode: 587, duration: 0.742s, episode steps: 132, steps per second: 178, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.245 [-1.846, 0.359], loss: 13.281644, mean_absolute_error: 29.377047, mean_q: 58.139130\n",
      "Episode 588 reward: 138.0\n",
      " 34500/100000: episode: 588, duration: 0.789s, episode steps: 138, steps per second: 175, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.213 [-1.668, 0.436], loss: 14.654469, mean_absolute_error: 29.476940, mean_q: 58.217030\n",
      "Episode 589 reward: 146.0\n",
      " 34646/100000: episode: 589, duration: 0.816s, episode steps: 146, steps per second: 179, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.221 [-1.870, 0.490], loss: 17.183815, mean_absolute_error: 29.718634, mean_q: 58.582993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 590 reward: 143.0\n",
      " 34789/100000: episode: 590, duration: 0.800s, episode steps: 143, steps per second: 179, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.230 [-1.599, 0.473], loss: 18.616835, mean_absolute_error: 29.619434, mean_q: 58.355701\n",
      "Episode 591 reward: 164.0\n",
      " 34953/100000: episode: 591, duration: 0.917s, episode steps: 164, steps per second: 179, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.222 [-1.854, 0.640], loss: 14.326041, mean_absolute_error: 29.606976, mean_q: 58.547161\n",
      "Episode 592 reward: 151.0\n",
      " 35104/100000: episode: 592, duration: 0.901s, episode steps: 151, steps per second: 168, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.222 [-1.645, 0.514], loss: 16.581055, mean_absolute_error: 29.844944, mean_q: 58.967636\n",
      "Episode 593 reward: 166.0\n",
      " 35270/100000: episode: 593, duration: 0.957s, episode steps: 166, steps per second: 173, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.241 [-1.968, 0.559], loss: 15.393606, mean_absolute_error: 29.827360, mean_q: 58.920052\n",
      "Episode 594 reward: 200.0\n",
      " 35470/100000: episode: 594, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.541, 0.578], loss: 16.309364, mean_absolute_error: 30.055975, mean_q: 59.420269\n",
      "Episode 595 reward: 131.0\n",
      " 35601/100000: episode: 595, duration: 0.745s, episode steps: 131, steps per second: 176, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.288 [-1.683, 0.435], loss: 17.280624, mean_absolute_error: 30.043779, mean_q: 59.335014\n",
      "Episode 596 reward: 200.0\n",
      " 35801/100000: episode: 596, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.182 [-1.587, 0.492], loss: 15.731978, mean_absolute_error: 30.047487, mean_q: 59.451149\n",
      "Episode 597 reward: 200.0\n",
      " 36001/100000: episode: 597, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.091 [-0.781, 0.591], loss: 18.330551, mean_absolute_error: 30.200678, mean_q: 59.693531\n",
      "Episode 598 reward: 200.0\n",
      " 36201/100000: episode: 598, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.184 [-0.597, 1.512], loss: 16.901016, mean_absolute_error: 30.352280, mean_q: 60.026512\n",
      "Episode 599 reward: 200.0\n",
      " 36401/100000: episode: 599, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.127 [-1.130, 0.544], loss: 21.398550, mean_absolute_error: 30.601370, mean_q: 60.278038\n",
      "Episode 600 reward: 194.0\n",
      " 36595/100000: episode: 600, duration: 1.077s, episode steps: 194, steps per second: 180, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.344 [-0.799, 2.427], loss: 19.184673, mean_absolute_error: 30.425343, mean_q: 60.094826\n",
      "Episode 601 reward: 172.0\n",
      " 36767/100000: episode: 601, duration: 0.977s, episode steps: 172, steps per second: 176, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.246 [-1.672, 0.490], loss: 17.541777, mean_absolute_error: 30.680368, mean_q: 60.606071\n",
      "Episode 602 reward: 155.0\n",
      " 36922/100000: episode: 602, duration: 0.875s, episode steps: 155, steps per second: 177, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.259 [-1.818, 0.342], loss: 15.819933, mean_absolute_error: 30.702211, mean_q: 60.738003\n",
      "Episode 603 reward: 200.0\n",
      " 37122/100000: episode: 603, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.146 [-0.450, 1.122], loss: 18.084248, mean_absolute_error: 30.993895, mean_q: 61.252563\n",
      "Episode 604 reward: 170.0\n",
      " 37292/100000: episode: 604, duration: 0.952s, episode steps: 170, steps per second: 179, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.250 [-1.869, 0.502], loss: 19.750566, mean_absolute_error: 30.982359, mean_q: 61.244762\n",
      "Episode 605 reward: 173.0\n",
      " 37465/100000: episode: 605, duration: 0.971s, episode steps: 173, steps per second: 178, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.246 [-1.968, 0.373], loss: 15.333294, mean_absolute_error: 31.313442, mean_q: 62.070156\n",
      "Episode 606 reward: 187.0\n",
      " 37652/100000: episode: 606, duration: 1.044s, episode steps: 187, steps per second: 179, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.247 [-2.027, 0.476], loss: 17.516672, mean_absolute_error: 31.486893, mean_q: 62.274399\n",
      "Episode 607 reward: 200.0\n",
      " 37852/100000: episode: 607, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.057 [-0.627, 0.572], loss: 18.536837, mean_absolute_error: 31.518774, mean_q: 62.316319\n",
      "Episode 608 reward: 200.0\n",
      " 38052/100000: episode: 608, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.093 [-0.710, 0.743], loss: 19.118357, mean_absolute_error: 31.624887, mean_q: 62.647751\n",
      "Episode 609 reward: 200.0\n",
      " 38252/100000: episode: 609, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.441, 0.613], loss: 16.880688, mean_absolute_error: 31.747456, mean_q: 62.906395\n",
      "Episode 610 reward: 200.0\n",
      " 38452/100000: episode: 610, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.226 [-0.475, 1.648], loss: 16.929356, mean_absolute_error: 31.911537, mean_q: 63.091530\n",
      "Episode 611 reward: 200.0\n",
      " 38652/100000: episode: 611, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.316 [-0.489, 2.367], loss: 19.101463, mean_absolute_error: 32.231197, mean_q: 63.810654\n",
      "Episode 612 reward: 197.0\n",
      " 38849/100000: episode: 612, duration: 1.106s, episode steps: 197, steps per second: 178, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.266 [-1.973, 0.505], loss: 22.702770, mean_absolute_error: 32.317398, mean_q: 63.848148\n",
      "Episode 613 reward: 200.0\n",
      " 39049/100000: episode: 613, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-0.541, 0.629], loss: 17.184788, mean_absolute_error: 32.286518, mean_q: 64.042633\n",
      "Episode 614 reward: 200.0\n",
      " 39249/100000: episode: 614, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.268 [-0.520, 1.967], loss: 18.798840, mean_absolute_error: 32.543556, mean_q: 64.375496\n",
      "Episode 615 reward: 200.0\n",
      " 39449/100000: episode: 615, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.113 [-0.521, 1.147], loss: 18.010771, mean_absolute_error: 32.666416, mean_q: 64.699402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 616 reward: 200.0\n",
      " 39649/100000: episode: 616, duration: 1.088s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.198 [-0.487, 1.433], loss: 23.585434, mean_absolute_error: 32.714497, mean_q: 64.498055\n",
      "Episode 617 reward: 200.0\n",
      " 39849/100000: episode: 617, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.116 [-1.076, 0.537], loss: 14.833260, mean_absolute_error: 32.972355, mean_q: 65.273529\n",
      "Episode 618 reward: 171.0\n",
      " 40020/100000: episode: 618, duration: 0.958s, episode steps: 171, steps per second: 179, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.290 [-2.013, 0.515], loss: 21.247639, mean_absolute_error: 33.118446, mean_q: 65.371689\n",
      "Episode 619 reward: 200.0\n",
      " 40220/100000: episode: 619, duration: 1.101s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.190 [-0.554, 1.469], loss: 24.162561, mean_absolute_error: 33.062767, mean_q: 65.291992\n",
      "Episode 620 reward: 200.0\n",
      " 40420/100000: episode: 620, duration: 1.089s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.132 [-0.499, 1.116], loss: 22.846352, mean_absolute_error: 32.969059, mean_q: 65.160858\n",
      "Episode 621 reward: 200.0\n",
      " 40620/100000: episode: 621, duration: 1.102s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-0.547, 0.601], loss: 20.522530, mean_absolute_error: 33.114300, mean_q: 65.537773\n",
      "Episode 622 reward: 176.0\n",
      " 40796/100000: episode: 622, duration: 0.964s, episode steps: 176, steps per second: 183, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.377 [-0.519, 2.422], loss: 22.597120, mean_absolute_error: 33.136780, mean_q: 65.496925\n",
      "Episode 623 reward: 200.0\n",
      " 40996/100000: episode: 623, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.152 [-1.086, 0.512], loss: 18.276955, mean_absolute_error: 33.428463, mean_q: 66.228905\n",
      "Episode 624 reward: 200.0\n",
      " 41196/100000: episode: 624, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-1.116, 0.800], loss: 19.933706, mean_absolute_error: 33.641239, mean_q: 66.655602\n",
      "Episode 625 reward: 195.0\n",
      " 41391/100000: episode: 625, duration: 1.142s, episode steps: 195, steps per second: 171, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.335 [-0.621, 2.433], loss: 17.145929, mean_absolute_error: 33.664627, mean_q: 66.748314\n",
      "Episode 626 reward: 200.0\n",
      " 41591/100000: episode: 626, duration: 1.158s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.210 [-0.871, 1.637], loss: 21.480341, mean_absolute_error: 33.890518, mean_q: 67.165817\n",
      "Episode 627 reward: 175.0\n",
      " 41766/100000: episode: 627, duration: 0.957s, episode steps: 175, steps per second: 183, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.382 [-0.520, 2.419], loss: 22.979370, mean_absolute_error: 34.102684, mean_q: 67.430176\n",
      "Episode 628 reward: 191.0\n",
      " 41957/100000: episode: 628, duration: 1.052s, episode steps: 191, steps per second: 182, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.343 [-0.482, 2.420], loss: 22.257450, mean_absolute_error: 34.139381, mean_q: 67.637047\n",
      "Episode 629 reward: 200.0\n",
      " 42157/100000: episode: 629, duration: 1.101s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.284 [-0.762, 2.080], loss: 16.576561, mean_absolute_error: 34.349365, mean_q: 68.152885\n",
      "Episode 630 reward: 200.0\n",
      " 42357/100000: episode: 630, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.217 [-0.371, 1.667], loss: 24.228491, mean_absolute_error: 34.720921, mean_q: 68.631111\n",
      "Episode 631 reward: 184.0\n",
      " 42541/100000: episode: 631, duration: 1.015s, episode steps: 184, steps per second: 181, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.366 [-0.460, 2.427], loss: 18.088543, mean_absolute_error: 34.666870, mean_q: 68.665504\n",
      "Episode 632 reward: 180.0\n",
      " 42721/100000: episode: 632, duration: 1.001s, episode steps: 180, steps per second: 180, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.361 [-0.530, 2.419], loss: 23.353161, mean_absolute_error: 34.702839, mean_q: 68.756454\n",
      "Episode 633 reward: 174.0\n",
      " 42895/100000: episode: 633, duration: 0.961s, episode steps: 174, steps per second: 181, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.262 [-0.515, 1.641], loss: 24.141113, mean_absolute_error: 34.934834, mean_q: 69.180038\n",
      "Episode 634 reward: 192.0\n",
      " 43087/100000: episode: 634, duration: 1.031s, episode steps: 192, steps per second: 186, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.340 [-0.499, 2.419], loss: 20.210375, mean_absolute_error: 35.383911, mean_q: 70.163948\n",
      "Episode 635 reward: 182.0\n",
      " 43269/100000: episode: 635, duration: 0.996s, episode steps: 182, steps per second: 183, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.373 [-0.560, 2.419], loss: 21.319809, mean_absolute_error: 35.300362, mean_q: 69.887634\n",
      "Episode 636 reward: 180.0\n",
      " 43449/100000: episode: 636, duration: 0.993s, episode steps: 180, steps per second: 181, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.373 [-0.527, 2.430], loss: 18.542543, mean_absolute_error: 35.553368, mean_q: 70.672615\n",
      "Episode 637 reward: 194.0\n",
      " 43643/100000: episode: 637, duration: 1.043s, episode steps: 194, steps per second: 186, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.353 [-0.693, 2.411], loss: 23.758774, mean_absolute_error: 35.872330, mean_q: 71.056915\n",
      "Episode 638 reward: 198.0\n",
      " 43841/100000: episode: 638, duration: 1.057s, episode steps: 198, steps per second: 187, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.330 [-0.652, 2.415], loss: 23.002472, mean_absolute_error: 35.883801, mean_q: 71.214714\n",
      "Episode 639 reward: 200.0\n",
      " 44041/100000: episode: 639, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.264 [-0.823, 1.927], loss: 21.680923, mean_absolute_error: 35.998585, mean_q: 71.408295\n",
      "Episode 640 reward: 200.0\n",
      " 44241/100000: episode: 640, duration: 1.084s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.337 [-0.501, 2.373], loss: 22.842680, mean_absolute_error: 36.381805, mean_q: 72.173302\n",
      "Episode 641 reward: 200.0\n",
      " 44441/100000: episode: 641, duration: 1.102s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.237 [-0.632, 1.743], loss: 24.957355, mean_absolute_error: 36.369297, mean_q: 72.067863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 642 reward: 200.0\n",
      " 44641/100000: episode: 642, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.191 [-0.587, 1.459], loss: 23.340315, mean_absolute_error: 36.608589, mean_q: 72.569168\n",
      "Episode 643 reward: 200.0\n",
      " 44841/100000: episode: 643, duration: 1.087s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-0.548, 0.610], loss: 22.590271, mean_absolute_error: 36.734566, mean_q: 72.747665\n",
      "Episode 644 reward: 191.0\n",
      " 45032/100000: episode: 644, duration: 1.046s, episode steps: 191, steps per second: 183, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.358 [-0.594, 2.434], loss: 26.302023, mean_absolute_error: 36.777069, mean_q: 72.786575\n",
      "Episode 645 reward: 173.0\n",
      " 45205/100000: episode: 645, duration: 0.955s, episode steps: 173, steps per second: 181, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.386 [-0.581, 2.403], loss: 23.459768, mean_absolute_error: 37.080357, mean_q: 73.552696\n",
      "Episode 646 reward: 176.0\n",
      " 45381/100000: episode: 646, duration: 0.950s, episode steps: 176, steps per second: 185, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.385 [-0.679, 2.410], loss: 21.983879, mean_absolute_error: 37.201382, mean_q: 73.881432\n",
      "Episode 647 reward: 174.0\n",
      " 45555/100000: episode: 647, duration: 0.969s, episode steps: 174, steps per second: 180, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.388 [-0.400, 2.413], loss: 29.108545, mean_absolute_error: 37.399399, mean_q: 73.946503\n",
      "Episode 648 reward: 200.0\n",
      " 45755/100000: episode: 648, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.056 [-0.640, 0.516], loss: 23.754457, mean_absolute_error: 37.386566, mean_q: 74.063766\n",
      "Episode 649 reward: 200.0\n",
      " 45955/100000: episode: 649, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.252 [-0.463, 1.863], loss: 25.301201, mean_absolute_error: 37.600174, mean_q: 74.338165\n",
      "Episode 650 reward: 200.0\n",
      " 46155/100000: episode: 650, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.347 [-0.727, 2.414], loss: 23.575119, mean_absolute_error: 37.591488, mean_q: 74.483391\n",
      "Episode 651 reward: 182.0\n",
      " 46337/100000: episode: 651, duration: 0.983s, episode steps: 182, steps per second: 185, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.369 [-0.497, 2.420], loss: 21.983957, mean_absolute_error: 37.873802, mean_q: 75.052681\n",
      "Episode 652 reward: 196.0\n",
      " 46533/100000: episode: 652, duration: 1.068s, episode steps: 196, steps per second: 183, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.351 [-0.560, 2.401], loss: 26.029531, mean_absolute_error: 37.815105, mean_q: 74.836891\n",
      "Episode 653 reward: 200.0\n",
      " 46733/100000: episode: 653, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.260 [-0.575, 1.892], loss: 20.539406, mean_absolute_error: 38.092918, mean_q: 75.504059\n",
      "Episode 654 reward: 180.0\n",
      " 46913/100000: episode: 654, duration: 1.011s, episode steps: 180, steps per second: 178, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.387 [-0.644, 2.406], loss: 27.662310, mean_absolute_error: 38.062847, mean_q: 75.267685\n",
      "Episode 655 reward: 200.0\n",
      " 47113/100000: episode: 655, duration: 1.102s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.256 [-0.834, 1.821], loss: 23.041941, mean_absolute_error: 38.046818, mean_q: 75.297516\n",
      "Episode 656 reward: 200.0\n",
      " 47313/100000: episode: 656, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.048 [-0.522, 0.756], loss: 26.099815, mean_absolute_error: 38.291645, mean_q: 75.788132\n",
      "Episode 657 reward: 182.0\n",
      " 47495/100000: episode: 657, duration: 1.008s, episode steps: 182, steps per second: 181, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.383 [-0.612, 2.415], loss: 21.139168, mean_absolute_error: 38.283138, mean_q: 75.885971\n",
      "Episode 658 reward: 184.0\n",
      " 47679/100000: episode: 658, duration: 1.032s, episode steps: 184, steps per second: 178, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.389 [-0.717, 2.422], loss: 27.998816, mean_absolute_error: 38.664452, mean_q: 76.486656\n",
      "Episode 659 reward: 200.0\n",
      " 47879/100000: episode: 659, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.333 [-0.586, 2.290], loss: 29.835058, mean_absolute_error: 38.398319, mean_q: 75.986198\n",
      "Episode 660 reward: 200.0\n",
      " 48079/100000: episode: 660, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.313 [-0.794, 2.211], loss: 17.885138, mean_absolute_error: 38.500668, mean_q: 76.492332\n",
      "Episode 661 reward: 200.0\n",
      " 48279/100000: episode: 661, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.353 [-0.475, 2.381], loss: 28.572172, mean_absolute_error: 38.744976, mean_q: 76.712585\n",
      "Episode 662 reward: 200.0\n",
      " 48479/100000: episode: 662, duration: 1.120s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.252 [-0.551, 1.800], loss: 23.368765, mean_absolute_error: 38.885963, mean_q: 77.040497\n",
      "Episode 663 reward: 200.0\n",
      " 48679/100000: episode: 663, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.295 [-0.540, 2.072], loss: 23.434717, mean_absolute_error: 38.989525, mean_q: 77.342720\n",
      "Episode 664 reward: 189.0\n",
      " 48868/100000: episode: 664, duration: 1.037s, episode steps: 189, steps per second: 182, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.369 [-0.505, 2.414], loss: 24.652458, mean_absolute_error: 39.096737, mean_q: 77.566689\n",
      "Episode 665 reward: 189.0\n",
      " 49057/100000: episode: 665, duration: 1.050s, episode steps: 189, steps per second: 180, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.363 [-0.849, 2.402], loss: 21.858057, mean_absolute_error: 39.295559, mean_q: 78.000702\n",
      "Episode 666 reward: 193.0\n",
      " 49250/100000: episode: 666, duration: 1.078s, episode steps: 193, steps per second: 179, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.365 [-0.722, 2.427], loss: 27.267447, mean_absolute_error: 39.432568, mean_q: 78.002388\n",
      "Episode 667 reward: 200.0\n",
      " 49450/100000: episode: 667, duration: 1.080s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.130 [-0.490, 1.026], loss: 27.022974, mean_absolute_error: 39.489178, mean_q: 78.106667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 668 reward: 200.0\n",
      " 49650/100000: episode: 668, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.258 [-0.619, 1.876], loss: 24.388081, mean_absolute_error: 39.628292, mean_q: 78.439957\n",
      "Episode 669 reward: 200.0\n",
      " 49850/100000: episode: 669, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.328 [-0.559, 2.331], loss: 23.134954, mean_absolute_error: 39.692379, mean_q: 78.656921\n",
      "Episode 670 reward: 183.0\n",
      " 50033/100000: episode: 670, duration: 1.017s, episode steps: 183, steps per second: 180, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.376 [-0.483, 2.416], loss: 19.791746, mean_absolute_error: 39.694988, mean_q: 78.827774\n",
      "Episode 671 reward: 195.0\n",
      " 50228/100000: episode: 671, duration: 1.073s, episode steps: 195, steps per second: 182, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.370 [-0.726, 2.417], loss: 22.125435, mean_absolute_error: 40.003662, mean_q: 79.431961\n",
      "Episode 672 reward: 200.0\n",
      " 50428/100000: episode: 672, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.333 [-0.393, 2.318], loss: 22.467457, mean_absolute_error: 40.345856, mean_q: 80.031487\n",
      "Episode 673 reward: 184.0\n",
      " 50612/100000: episode: 673, duration: 1.026s, episode steps: 184, steps per second: 179, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.387 [-0.437, 2.423], loss: 22.164843, mean_absolute_error: 40.397858, mean_q: 80.092148\n",
      "Episode 674 reward: 195.0\n",
      " 50807/100000: episode: 674, duration: 1.180s, episode steps: 195, steps per second: 165, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.369 [-0.442, 2.412], loss: 24.101433, mean_absolute_error: 40.453972, mean_q: 80.101509\n",
      "Episode 675 reward: 200.0\n",
      " 51007/100000: episode: 675, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.307 [-0.580, 2.138], loss: 24.136816, mean_absolute_error: 40.591599, mean_q: 80.415352\n",
      "Episode 676 reward: 194.0\n",
      " 51201/100000: episode: 676, duration: 1.051s, episode steps: 194, steps per second: 185, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.371 [-0.459, 2.411], loss: 26.362228, mean_absolute_error: 40.731041, mean_q: 80.650803\n",
      "Episode 677 reward: 177.0\n",
      " 51378/100000: episode: 677, duration: 0.956s, episode steps: 177, steps per second: 185, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.396 [-0.535, 2.417], loss: 20.063564, mean_absolute_error: 40.810978, mean_q: 81.070145\n",
      "Episode 678 reward: 192.0\n",
      " 51570/100000: episode: 678, duration: 1.063s, episode steps: 192, steps per second: 181, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.371 [-0.579, 2.423], loss: 22.287729, mean_absolute_error: 40.873363, mean_q: 81.167603\n",
      "Episode 679 reward: 172.0\n",
      " 51742/100000: episode: 679, duration: 0.956s, episode steps: 172, steps per second: 180, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.407 [-0.408, 2.403], loss: 25.830589, mean_absolute_error: 41.236134, mean_q: 81.826591\n",
      "Episode 680 reward: 200.0\n",
      " 51942/100000: episode: 680, duration: 1.084s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.317 [-0.504, 2.154], loss: 27.902878, mean_absolute_error: 41.326874, mean_q: 81.875496\n",
      "Episode 681 reward: 168.0\n",
      " 52110/100000: episode: 681, duration: 0.928s, episode steps: 168, steps per second: 181, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.420 [-0.478, 2.400], loss: 24.450077, mean_absolute_error: 41.220097, mean_q: 81.811378\n",
      "Episode 682 reward: 170.0\n",
      " 52280/100000: episode: 682, duration: 0.939s, episode steps: 170, steps per second: 181, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.409 [-0.605, 2.425], loss: 29.245152, mean_absolute_error: 41.340622, mean_q: 81.895668\n",
      "Episode 683 reward: 200.0\n",
      " 52480/100000: episode: 683, duration: 1.096s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.305 [-0.509, 2.126], loss: 24.417564, mean_absolute_error: 41.178787, mean_q: 81.714928\n",
      "Episode 684 reward: 179.0\n",
      " 52659/100000: episode: 684, duration: 0.990s, episode steps: 179, steps per second: 181, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.398 [-0.431, 2.413], loss: 29.494593, mean_absolute_error: 41.666805, mean_q: 82.534088\n",
      "Episode 685 reward: 200.0\n",
      " 52859/100000: episode: 685, duration: 1.096s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.313 [-0.776, 2.132], loss: 28.043428, mean_absolute_error: 41.868374, mean_q: 83.002853\n",
      "Episode 686 reward: 200.0\n",
      " 53059/100000: episode: 686, duration: 1.088s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.272 [-0.530, 1.864], loss: 27.045191, mean_absolute_error: 41.647846, mean_q: 82.458183\n",
      "Episode 687 reward: 183.0\n",
      " 53242/100000: episode: 687, duration: 1.023s, episode steps: 183, steps per second: 179, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.398 [-0.779, 2.412], loss: 27.423737, mean_absolute_error: 41.664936, mean_q: 82.565842\n",
      "Episode 688 reward: 187.0\n",
      " 53429/100000: episode: 688, duration: 1.053s, episode steps: 187, steps per second: 178, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.386 [-0.558, 2.420], loss: 27.319504, mean_absolute_error: 42.057957, mean_q: 83.347839\n",
      "Episode 689 reward: 200.0\n",
      " 53629/100000: episode: 689, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.298 [-0.523, 2.086], loss: 22.410130, mean_absolute_error: 41.880314, mean_q: 83.117851\n",
      "Episode 690 reward: 188.0\n",
      " 53817/100000: episode: 690, duration: 1.056s, episode steps: 188, steps per second: 178, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.390 [-0.632, 2.423], loss: 25.086561, mean_absolute_error: 42.099270, mean_q: 83.459343\n",
      "Episode 691 reward: 200.0\n",
      " 54017/100000: episode: 691, duration: 1.133s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.365 [-0.491, 2.403], loss: 26.101948, mean_absolute_error: 42.120064, mean_q: 83.495941\n",
      "Episode 692 reward: 200.0\n",
      " 54217/100000: episode: 692, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.319 [-0.530, 2.137], loss: 29.726446, mean_absolute_error: 42.469826, mean_q: 84.197556\n",
      "Episode 693 reward: 183.0\n",
      " 54400/100000: episode: 693, duration: 1.017s, episode steps: 183, steps per second: 180, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.397 [-0.563, 2.422], loss: 26.646439, mean_absolute_error: 42.393753, mean_q: 84.073128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 694 reward: 179.0\n",
      " 54579/100000: episode: 694, duration: 0.989s, episode steps: 179, steps per second: 181, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.396 [-0.618, 2.419], loss: 21.357538, mean_absolute_error: 42.378220, mean_q: 84.291641\n",
      "Episode 695 reward: 177.0\n",
      " 54756/100000: episode: 695, duration: 0.989s, episode steps: 177, steps per second: 179, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.410 [-0.811, 2.411], loss: 29.026394, mean_absolute_error: 42.576508, mean_q: 84.419373\n",
      "Episode 696 reward: 200.0\n",
      " 54956/100000: episode: 696, duration: 1.133s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.304 [-0.744, 2.045], loss: 25.370377, mean_absolute_error: 42.586437, mean_q: 84.410583\n",
      "Episode 697 reward: 194.0\n",
      " 55150/100000: episode: 697, duration: 1.094s, episode steps: 194, steps per second: 177, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.377 [-0.606, 2.417], loss: 20.172686, mean_absolute_error: 42.945969, mean_q: 85.362717\n",
      "Episode 698 reward: 200.0\n",
      " 55350/100000: episode: 698, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.272 [-0.579, 1.870], loss: 24.694185, mean_absolute_error: 42.982147, mean_q: 85.350685\n",
      "Episode 699 reward: 200.0\n",
      " 55550/100000: episode: 699, duration: 1.107s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.309 [-0.607, 2.035], loss: 27.539200, mean_absolute_error: 43.139374, mean_q: 85.610222\n",
      "Episode 700 reward: 200.0\n",
      " 55750/100000: episode: 700, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.368 [-1.157, 2.379], loss: 32.005360, mean_absolute_error: 43.201836, mean_q: 85.605156\n",
      "Episode 701 reward: 190.0\n",
      " 55940/100000: episode: 701, duration: 1.075s, episode steps: 190, steps per second: 177, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.381 [-0.445, 2.410], loss: 32.337688, mean_absolute_error: 43.367153, mean_q: 85.996010\n",
      "Episode 702 reward: 189.0\n",
      " 56129/100000: episode: 702, duration: 1.065s, episode steps: 189, steps per second: 178, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.389 [-0.543, 2.410], loss: 28.745598, mean_absolute_error: 43.466125, mean_q: 86.091583\n",
      "Episode 703 reward: 200.0\n",
      " 56329/100000: episode: 703, duration: 1.107s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.200 [-0.477, 1.444], loss: 26.027809, mean_absolute_error: 43.224785, mean_q: 85.806709\n",
      "Episode 704 reward: 200.0\n",
      " 56529/100000: episode: 704, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.302 [-0.577, 2.009], loss: 25.376892, mean_absolute_error: 43.493359, mean_q: 86.431267\n",
      "Episode 705 reward: 196.0\n",
      " 56725/100000: episode: 705, duration: 1.073s, episode steps: 196, steps per second: 183, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.365 [-0.672, 2.424], loss: 26.959532, mean_absolute_error: 43.421909, mean_q: 86.207268\n",
      "Episode 706 reward: 189.0\n",
      " 56914/100000: episode: 706, duration: 1.067s, episode steps: 189, steps per second: 177, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.390 [-0.773, 2.417], loss: 27.182602, mean_absolute_error: 43.469040, mean_q: 86.411827\n",
      "Episode 707 reward: 200.0\n",
      " 57114/100000: episode: 707, duration: 1.179s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.307 [-0.665, 2.032], loss: 26.284002, mean_absolute_error: 43.826454, mean_q: 87.053360\n",
      "Episode 708 reward: 198.0\n",
      " 57312/100000: episode: 708, duration: 1.116s, episode steps: 198, steps per second: 177, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.371 [-0.496, 2.414], loss: 28.263813, mean_absolute_error: 43.898621, mean_q: 87.199730\n",
      "Episode 709 reward: 194.0\n",
      " 57506/100000: episode: 709, duration: 1.135s, episode steps: 194, steps per second: 171, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.393 [-0.763, 2.409], loss: 22.052191, mean_absolute_error: 44.046028, mean_q: 87.613380\n",
      "Episode 710 reward: 200.0\n",
      " 57706/100000: episode: 710, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.341 [-0.536, 2.228], loss: 30.535229, mean_absolute_error: 44.118526, mean_q: 87.611397\n",
      "Episode 711 reward: 200.0\n",
      " 57906/100000: episode: 711, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.253 [-0.464, 1.698], loss: 27.924528, mean_absolute_error: 44.241932, mean_q: 87.796173\n",
      "Episode 712 reward: 200.0\n",
      " 58106/100000: episode: 712, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.342 [-0.505, 2.196], loss: 28.455864, mean_absolute_error: 44.341900, mean_q: 88.133965\n",
      "Episode 713 reward: 200.0\n",
      " 58306/100000: episode: 713, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.327 [-0.684, 2.182], loss: 22.940874, mean_absolute_error: 44.358543, mean_q: 88.265800\n",
      "Episode 714 reward: 200.0\n",
      " 58506/100000: episode: 714, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.334 [-0.863, 2.114], loss: 26.358301, mean_absolute_error: 44.683826, mean_q: 88.875519\n",
      "Episode 715 reward: 181.0\n",
      " 58687/100000: episode: 715, duration: 1.001s, episode steps: 181, steps per second: 181, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.398 [-0.605, 2.401], loss: 24.996828, mean_absolute_error: 44.738960, mean_q: 88.942726\n",
      "Episode 716 reward: 200.0\n",
      " 58887/100000: episode: 716, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.341 [-0.837, 2.154], loss: 25.694317, mean_absolute_error: 45.037079, mean_q: 89.587166\n",
      "Episode 717 reward: 200.0\n",
      " 59087/100000: episode: 717, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.336 [-0.871, 2.141], loss: 30.529409, mean_absolute_error: 45.037052, mean_q: 89.426582\n",
      "Episode 718 reward: 200.0\n",
      " 59287/100000: episode: 718, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.279 [-0.448, 1.797], loss: 27.200615, mean_absolute_error: 44.864758, mean_q: 89.247986\n",
      "Episode 719 reward: 200.0\n",
      " 59487/100000: episode: 719, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.269 [-0.463, 1.783], loss: 26.758051, mean_absolute_error: 45.253727, mean_q: 89.945847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 720 reward: 200.0\n",
      " 59687/100000: episode: 720, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.383 [-0.883, 2.369], loss: 25.633379, mean_absolute_error: 45.192753, mean_q: 90.026810\n",
      "Episode 721 reward: 200.0\n",
      " 59887/100000: episode: 721, duration: 1.040s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.359 [-0.452, 2.268], loss: 39.792461, mean_absolute_error: 45.282482, mean_q: 89.628082\n",
      "Episode 722 reward: 200.0\n",
      " 60087/100000: episode: 722, duration: 1.022s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.186 [-0.523, 1.278], loss: 29.415045, mean_absolute_error: 45.172924, mean_q: 89.782928\n",
      "Episode 723 reward: 200.0\n",
      " 60287/100000: episode: 723, duration: 1.039s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.124 [-0.553, 0.999], loss: 31.962364, mean_absolute_error: 45.239880, mean_q: 89.891640\n",
      "Episode 724 reward: 200.0\n",
      " 60487/100000: episode: 724, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.277 [-0.428, 1.902], loss: 28.851519, mean_absolute_error: 45.302059, mean_q: 90.112907\n",
      "Episode 725 reward: 200.0\n",
      " 60687/100000: episode: 725, duration: 1.038s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.374 [-0.561, 2.380], loss: 33.541012, mean_absolute_error: 45.612759, mean_q: 90.559738\n",
      "Episode 726 reward: 200.0\n",
      " 60887/100000: episode: 726, duration: 1.085s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.337 [-0.784, 2.139], loss: 28.577543, mean_absolute_error: 45.450363, mean_q: 90.365837\n",
      "Episode 727 reward: 200.0\n",
      " 61087/100000: episode: 727, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.321 [-0.572, 2.079], loss: 27.340446, mean_absolute_error: 45.461071, mean_q: 90.514336\n",
      "Episode 728 reward: 194.0\n",
      " 61281/100000: episode: 728, duration: 1.057s, episode steps: 194, steps per second: 184, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.390 [-0.527, 2.420], loss: 24.255522, mean_absolute_error: 45.753433, mean_q: 91.098763\n",
      "Episode 729 reward: 200.0\n",
      " 61481/100000: episode: 729, duration: 1.096s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.356 [-0.468, 2.277], loss: 22.578590, mean_absolute_error: 45.849854, mean_q: 91.306229\n",
      "Episode 730 reward: 200.0\n",
      " 61681/100000: episode: 730, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.342 [-0.515, 2.170], loss: 29.908327, mean_absolute_error: 45.881107, mean_q: 91.290588\n",
      "Episode 731 reward: 200.0\n",
      " 61881/100000: episode: 731, duration: 1.108s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.323 [-0.496, 2.039], loss: 28.883190, mean_absolute_error: 46.229923, mean_q: 92.060936\n",
      "Episode 732 reward: 200.0\n",
      " 62081/100000: episode: 732, duration: 1.085s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.373 [-0.771, 2.366], loss: 21.735516, mean_absolute_error: 46.398643, mean_q: 92.401703\n",
      "Episode 733 reward: 200.0\n",
      " 62281/100000: episode: 733, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.250 [-0.790, 1.614], loss: 29.489799, mean_absolute_error: 46.312668, mean_q: 91.995819\n",
      "Episode 734 reward: 200.0\n",
      " 62481/100000: episode: 734, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.313 [-0.523, 2.019], loss: 20.094631, mean_absolute_error: 46.602764, mean_q: 92.921143\n",
      "Episode 735 reward: 184.0\n",
      " 62665/100000: episode: 735, duration: 1.008s, episode steps: 184, steps per second: 182, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.395 [-0.600, 2.410], loss: 20.262142, mean_absolute_error: 46.966259, mean_q: 93.765274\n",
      "Episode 736 reward: 200.0\n",
      " 62865/100000: episode: 736, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.371 [-0.668, 2.357], loss: 28.254454, mean_absolute_error: 46.991005, mean_q: 93.563644\n",
      "Episode 737 reward: 200.0\n",
      " 63065/100000: episode: 737, duration: 1.074s, episode steps: 200, steps per second: 186, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.347 [-0.574, 2.216], loss: 29.698009, mean_absolute_error: 46.925793, mean_q: 93.512070\n",
      "Episode 738 reward: 200.0\n",
      " 63265/100000: episode: 738, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.307 [-0.669, 1.999], loss: 27.802589, mean_absolute_error: 47.301128, mean_q: 94.213417\n",
      "Episode 739 reward: 197.0\n",
      " 63462/100000: episode: 739, duration: 1.090s, episode steps: 197, steps per second: 181, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.390 [-0.642, 2.407], loss: 28.397024, mean_absolute_error: 47.276997, mean_q: 94.225845\n",
      "Episode 740 reward: 200.0\n",
      " 63662/100000: episode: 740, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.264 [-0.618, 1.783], loss: 25.445496, mean_absolute_error: 47.259640, mean_q: 94.230156\n",
      "Episode 741 reward: 200.0\n",
      " 63862/100000: episode: 741, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.332 [-0.504, 2.110], loss: 34.551128, mean_absolute_error: 47.499023, mean_q: 94.415504\n",
      "Episode 742 reward: 200.0\n",
      " 64062/100000: episode: 742, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.309 [-0.477, 1.978], loss: 26.539619, mean_absolute_error: 47.403423, mean_q: 94.393127\n",
      "Episode 743 reward: 200.0\n",
      " 64262/100000: episode: 743, duration: 1.098s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.344 [-0.503, 2.234], loss: 27.897089, mean_absolute_error: 47.577145, mean_q: 94.874031\n",
      "Episode 744 reward: 200.0\n",
      " 64462/100000: episode: 744, duration: 1.101s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.267 [-0.503, 1.709], loss: 29.234602, mean_absolute_error: 47.774120, mean_q: 95.153984\n",
      "Episode 745 reward: 200.0\n",
      " 64662/100000: episode: 745, duration: 1.096s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.280 [-0.602, 1.778], loss: 29.323383, mean_absolute_error: 47.624111, mean_q: 94.796745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 746 reward: 200.0\n",
      " 64862/100000: episode: 746, duration: 1.089s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.318 [-0.548, 1.924], loss: 27.395039, mean_absolute_error: 47.628468, mean_q: 94.895271\n",
      "Episode 747 reward: 200.0\n",
      " 65062/100000: episode: 747, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.335 [-0.748, 2.090], loss: 29.407505, mean_absolute_error: 47.886425, mean_q: 95.359688\n",
      "Episode 748 reward: 200.0\n",
      " 65262/100000: episode: 748, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.242 [-0.522, 1.496], loss: 33.050728, mean_absolute_error: 47.846024, mean_q: 95.178391\n",
      "Episode 749 reward: 200.0\n",
      " 65462/100000: episode: 749, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.230 [-0.455, 1.453], loss: 32.133865, mean_absolute_error: 47.740089, mean_q: 95.140343\n",
      "Episode 750 reward: 200.0\n",
      " 65662/100000: episode: 750, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.289 [-0.648, 1.836], loss: 33.322063, mean_absolute_error: 47.941090, mean_q: 95.463768\n",
      "Episode 751 reward: 200.0\n",
      " 65862/100000: episode: 751, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.287 [-0.683, 1.832], loss: 34.523293, mean_absolute_error: 47.989925, mean_q: 95.486618\n",
      "Episode 752 reward: 200.0\n",
      " 66062/100000: episode: 752, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.263 [-0.605, 1.727], loss: 22.924961, mean_absolute_error: 48.103065, mean_q: 96.054382\n",
      "Episode 753 reward: 200.0\n",
      " 66262/100000: episode: 753, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.231 [-0.600, 1.584], loss: 24.741934, mean_absolute_error: 48.202267, mean_q: 96.196648\n",
      "Episode 754 reward: 200.0\n",
      " 66462/100000: episode: 754, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.289 [-0.582, 1.822], loss: 28.851500, mean_absolute_error: 48.200859, mean_q: 96.183502\n",
      "Episode 755 reward: 200.0\n",
      " 66662/100000: episode: 755, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.247 [-0.583, 1.555], loss: 30.647022, mean_absolute_error: 48.315796, mean_q: 96.365097\n",
      "Episode 756 reward: 200.0\n",
      " 66862/100000: episode: 756, duration: 1.128s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.222 [-0.736, 1.493], loss: 27.377415, mean_absolute_error: 48.404465, mean_q: 96.541718\n",
      "Episode 757 reward: 200.0\n",
      " 67062/100000: episode: 757, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.298 [-0.583, 1.877], loss: 23.410933, mean_absolute_error: 48.606014, mean_q: 97.163025\n",
      "Episode 758 reward: 200.0\n",
      " 67262/100000: episode: 758, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.256 [-0.479, 1.648], loss: 27.186134, mean_absolute_error: 48.935238, mean_q: 97.607796\n",
      "Episode 759 reward: 200.0\n",
      " 67462/100000: episode: 759, duration: 1.093s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.231 [-0.579, 1.494], loss: 38.079124, mean_absolute_error: 48.904453, mean_q: 97.296379\n",
      "Episode 760 reward: 200.0\n",
      " 67662/100000: episode: 760, duration: 1.081s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.246 [-0.545, 1.616], loss: 25.256786, mean_absolute_error: 48.621040, mean_q: 97.213516\n",
      "Episode 761 reward: 200.0\n",
      " 67862/100000: episode: 761, duration: 1.157s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.304 [-0.610, 1.943], loss: 28.205162, mean_absolute_error: 48.960236, mean_q: 97.714752\n",
      "Episode 762 reward: 200.0\n",
      " 68062/100000: episode: 762, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.241 [-0.519, 1.550], loss: 36.573589, mean_absolute_error: 48.927788, mean_q: 97.491241\n",
      "Episode 763 reward: 200.0\n",
      " 68262/100000: episode: 763, duration: 1.097s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.243 [-0.431, 1.565], loss: 26.355202, mean_absolute_error: 49.158356, mean_q: 98.127754\n",
      "Episode 764 reward: 200.0\n",
      " 68462/100000: episode: 764, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.293 [-0.511, 1.759], loss: 24.661615, mean_absolute_error: 49.139084, mean_q: 98.309784\n",
      "Episode 765 reward: 200.0\n",
      " 68662/100000: episode: 765, duration: 1.098s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.274 [-0.599, 1.747], loss: 30.208355, mean_absolute_error: 49.149002, mean_q: 98.054558\n",
      "Episode 766 reward: 200.0\n",
      " 68862/100000: episode: 766, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.187 [-0.827, 1.161], loss: 24.979551, mean_absolute_error: 49.391666, mean_q: 98.764427\n",
      "Episode 767 reward: 200.0\n",
      " 69062/100000: episode: 767, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.151 [-0.602, 1.021], loss: 29.751549, mean_absolute_error: 49.379471, mean_q: 98.693810\n",
      "Episode 768 reward: 200.0\n",
      " 69262/100000: episode: 768, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.165 [-0.507, 1.097], loss: 34.273907, mean_absolute_error: 49.564518, mean_q: 98.996582\n",
      "Episode 769 reward: 200.0\n",
      " 69462/100000: episode: 769, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.246 [-0.369, 1.513], loss: 28.352207, mean_absolute_error: 49.436180, mean_q: 98.726990\n",
      "Episode 770 reward: 200.0\n",
      " 69662/100000: episode: 770, duration: 1.093s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.231 [-0.742, 1.480], loss: 31.138596, mean_absolute_error: 49.662491, mean_q: 99.276123\n",
      "Episode 771 reward: 200.0\n",
      " 69862/100000: episode: 771, duration: 1.096s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.118 [-0.434, 0.813], loss: 29.466698, mean_absolute_error: 49.794666, mean_q: 99.460251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 772 reward: 200.0\n",
      " 70062/100000: episode: 772, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.210 [-0.571, 1.375], loss: 31.086555, mean_absolute_error: 49.961758, mean_q: 99.858986\n",
      "Episode 773 reward: 200.0\n",
      " 70262/100000: episode: 773, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.171 [-0.558, 1.125], loss: 31.008905, mean_absolute_error: 50.012520, mean_q: 99.937325\n",
      "Episode 774 reward: 200.0\n",
      " 70462/100000: episode: 774, duration: 1.085s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.269 [-0.606, 1.591], loss: 32.109512, mean_absolute_error: 50.133827, mean_q: 100.255508\n",
      "Episode 775 reward: 200.0\n",
      " 70662/100000: episode: 775, duration: 1.095s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.168 [-0.741, 1.124], loss: 32.763687, mean_absolute_error: 50.087807, mean_q: 100.106911\n",
      "Episode 776 reward: 200.0\n",
      " 70862/100000: episode: 776, duration: 1.108s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.194 [-0.634, 1.284], loss: 35.956924, mean_absolute_error: 50.299904, mean_q: 100.518982\n",
      "Episode 777 reward: 200.0\n",
      " 71062/100000: episode: 777, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.166 [-0.610, 1.079], loss: 29.978273, mean_absolute_error: 50.443844, mean_q: 100.815117\n",
      "Episode 778 reward: 200.0\n",
      " 71262/100000: episode: 778, duration: 1.102s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.074 [-0.554, 0.703], loss: 36.122242, mean_absolute_error: 50.373432, mean_q: 100.666077\n",
      "Episode 779 reward: 200.0\n",
      " 71462/100000: episode: 779, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.217 [-0.523, 1.319], loss: 28.470284, mean_absolute_error: 50.460049, mean_q: 100.924202\n",
      "Episode 780 reward: 200.0\n",
      " 71662/100000: episode: 780, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.169 [-0.548, 1.117], loss: 31.735136, mean_absolute_error: 50.689194, mean_q: 101.387390\n",
      "Episode 781 reward: 200.0\n",
      " 71862/100000: episode: 781, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.276 [-0.559, 1.694], loss: 33.129406, mean_absolute_error: 50.755199, mean_q: 101.456696\n",
      "Episode 782 reward: 200.0\n",
      " 72062/100000: episode: 782, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.237 [-0.566, 1.467], loss: 30.040615, mean_absolute_error: 50.717350, mean_q: 101.450127\n",
      "Episode 783 reward: 200.0\n",
      " 72262/100000: episode: 783, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.236 [-0.540, 1.515], loss: 27.397198, mean_absolute_error: 50.866249, mean_q: 101.797462\n",
      "Episode 784 reward: 200.0\n",
      " 72462/100000: episode: 784, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.168 [-0.835, 1.060], loss: 32.217010, mean_absolute_error: 50.778622, mean_q: 101.569221\n",
      "Episode 785 reward: 200.0\n",
      " 72662/100000: episode: 785, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.166 [-0.581, 1.099], loss: 28.049589, mean_absolute_error: 50.899574, mean_q: 101.868942\n",
      "Episode 786 reward: 200.0\n",
      " 72862/100000: episode: 786, duration: 1.131s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.182 [-0.587, 1.210], loss: 32.301151, mean_absolute_error: 50.854244, mean_q: 101.731934\n",
      "Episode 787 reward: 200.0\n",
      " 73062/100000: episode: 787, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.086 [-0.756, 0.732], loss: 36.961876, mean_absolute_error: 50.890087, mean_q: 101.669456\n",
      "Episode 788 reward: 200.0\n",
      " 73262/100000: episode: 788, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.146 [-0.546, 0.918], loss: 29.278942, mean_absolute_error: 51.182777, mean_q: 102.431953\n",
      "Episode 789 reward: 200.0\n",
      " 73462/100000: episode: 789, duration: 1.134s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.216 [-0.486, 1.337], loss: 36.166798, mean_absolute_error: 51.126663, mean_q: 102.146019\n",
      "Episode 790 reward: 200.0\n",
      " 73662/100000: episode: 790, duration: 1.112s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.114 [-0.545, 0.796], loss: 38.867359, mean_absolute_error: 50.911732, mean_q: 101.548119\n",
      "Episode 791 reward: 200.0\n",
      " 73862/100000: episode: 791, duration: 1.133s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.172 [-0.804, 1.133], loss: 31.205429, mean_absolute_error: 51.196327, mean_q: 102.501213\n",
      "Episode 792 reward: 200.0\n",
      " 74062/100000: episode: 792, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.278 [-0.544, 1.726], loss: 32.290855, mean_absolute_error: 50.968311, mean_q: 102.006004\n",
      "Episode 793 reward: 200.0\n",
      " 74262/100000: episode: 793, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.170 [-0.833, 1.103], loss: 28.755869, mean_absolute_error: 51.053436, mean_q: 102.306328\n",
      "Episode 794 reward: 200.0\n",
      " 74462/100000: episode: 794, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.215 [-1.093, 1.310], loss: 27.433250, mean_absolute_error: 51.084015, mean_q: 102.341873\n",
      "Episode 795 reward: 200.0\n",
      " 74662/100000: episode: 795, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.186 [-0.760, 1.123], loss: 34.591389, mean_absolute_error: 51.457798, mean_q: 102.846527\n",
      "Episode 796 reward: 200.0\n",
      " 74862/100000: episode: 796, duration: 1.134s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.245 [-0.555, 1.527], loss: 27.974373, mean_absolute_error: 51.278759, mean_q: 102.647560\n",
      "Episode 797 reward: 200.0\n",
      " 75062/100000: episode: 797, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.216 [-0.614, 1.389], loss: 32.360680, mean_absolute_error: 51.332077, mean_q: 102.740234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 798 reward: 200.0\n",
      " 75262/100000: episode: 798, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.129 [-0.599, 0.863], loss: 30.308907, mean_absolute_error: 51.379696, mean_q: 102.902145\n",
      "Episode 799 reward: 200.0\n",
      " 75462/100000: episode: 799, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.179 [-0.522, 1.085], loss: 26.413164, mean_absolute_error: 51.275955, mean_q: 102.808632\n",
      "Episode 800 reward: 200.0\n",
      " 75662/100000: episode: 800, duration: 1.108s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.231 [-0.612, 1.442], loss: 30.474638, mean_absolute_error: 51.412830, mean_q: 103.066093\n",
      "Episode 801 reward: 200.0\n",
      " 75862/100000: episode: 801, duration: 1.141s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.240 [-0.590, 1.531], loss: 29.568907, mean_absolute_error: 51.804760, mean_q: 103.843864\n",
      "Episode 802 reward: 200.0\n",
      " 76062/100000: episode: 802, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.126 [-0.540, 0.890], loss: 27.686264, mean_absolute_error: 51.675976, mean_q: 103.546112\n",
      "Episode 803 reward: 200.0\n",
      " 76262/100000: episode: 803, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.216 [-0.409, 1.313], loss: 30.472095, mean_absolute_error: 52.129383, mean_q: 104.455383\n",
      "Episode 804 reward: 200.0\n",
      " 76462/100000: episode: 804, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.165 [-0.520, 1.022], loss: 28.030180, mean_absolute_error: 51.685371, mean_q: 103.665245\n",
      "Episode 805 reward: 200.0\n",
      " 76662/100000: episode: 805, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.127 [-0.598, 0.839], loss: 27.694613, mean_absolute_error: 51.982704, mean_q: 104.210136\n",
      "Episode 806 reward: 200.0\n",
      " 76862/100000: episode: 806, duration: 1.114s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.186 [-0.493, 1.150], loss: 29.857162, mean_absolute_error: 52.042984, mean_q: 104.272705\n",
      "Episode 807 reward: 200.0\n",
      " 77062/100000: episode: 807, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-0.371, 0.948], loss: 27.804417, mean_absolute_error: 51.998600, mean_q: 104.250511\n",
      "Episode 808 reward: 200.0\n",
      " 77262/100000: episode: 808, duration: 1.128s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.126 [-0.608, 0.841], loss: 33.215343, mean_absolute_error: 52.283100, mean_q: 104.737328\n",
      "Episode 809 reward: 200.0\n",
      " 77462/100000: episode: 809, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.206 [-0.481, 1.237], loss: 28.682556, mean_absolute_error: 52.141064, mean_q: 104.507347\n",
      "Episode 810 reward: 200.0\n",
      " 77662/100000: episode: 810, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.143 [-0.561, 0.895], loss: 34.172970, mean_absolute_error: 52.149490, mean_q: 104.503021\n",
      "Episode 811 reward: 200.0\n",
      " 77862/100000: episode: 811, duration: 1.139s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.160 [-0.541, 1.014], loss: 26.869942, mean_absolute_error: 52.266567, mean_q: 104.897179\n",
      "Episode 812 reward: 200.0\n",
      " 78062/100000: episode: 812, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.268 [-0.490, 1.603], loss: 37.801178, mean_absolute_error: 52.476044, mean_q: 105.044113\n",
      "Episode 813 reward: 200.0\n",
      " 78262/100000: episode: 813, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.240 [-0.502, 1.471], loss: 35.687214, mean_absolute_error: 52.317841, mean_q: 104.662033\n",
      "Episode 814 reward: 200.0\n",
      " 78462/100000: episode: 814, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.200 [-0.895, 1.232], loss: 25.684824, mean_absolute_error: 52.443432, mean_q: 105.228622\n",
      "Episode 815 reward: 200.0\n",
      " 78662/100000: episode: 815, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.103 [-0.555, 0.865], loss: 24.905907, mean_absolute_error: 52.417274, mean_q: 105.251373\n",
      "Episode 816 reward: 200.0\n",
      " 78862/100000: episode: 816, duration: 1.138s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.792, 0.766], loss: 29.719639, mean_absolute_error: 52.455002, mean_q: 105.155037\n",
      "Episode 817 reward: 200.0\n",
      " 79062/100000: episode: 817, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.159 [-0.521, 1.048], loss: 23.973854, mean_absolute_error: 52.491913, mean_q: 105.274864\n",
      "Episode 818 reward: 200.0\n",
      " 79262/100000: episode: 818, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.160 [-0.584, 1.011], loss: 34.226452, mean_absolute_error: 52.405453, mean_q: 104.886055\n",
      "Episode 819 reward: 200.0\n",
      " 79462/100000: episode: 819, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.135 [-0.530, 0.935], loss: 33.241501, mean_absolute_error: 52.400742, mean_q: 104.855682\n",
      "Episode 820 reward: 200.0\n",
      " 79662/100000: episode: 820, duration: 1.114s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.120 [-0.551, 0.741], loss: 35.126732, mean_absolute_error: 52.144833, mean_q: 104.356873\n",
      "Episode 821 reward: 200.0\n",
      " 79862/100000: episode: 821, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.239 [-0.595, 1.438], loss: 31.410528, mean_absolute_error: 52.211014, mean_q: 104.479805\n",
      "Episode 822 reward: 200.0\n",
      " 80062/100000: episode: 822, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.174 [-0.506, 1.104], loss: 28.749557, mean_absolute_error: 52.214375, mean_q: 104.539627\n",
      "Episode 823 reward: 200.0\n",
      " 80262/100000: episode: 823, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.173 [-0.519, 1.080], loss: 31.366953, mean_absolute_error: 52.430119, mean_q: 104.984001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 824 reward: 200.0\n",
      " 80462/100000: episode: 824, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.162 [-0.474, 1.099], loss: 25.740572, mean_absolute_error: 52.048882, mean_q: 104.435959\n",
      "Episode 825 reward: 200.0\n",
      " 80662/100000: episode: 825, duration: 1.114s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.130 [-0.581, 0.905], loss: 31.576767, mean_absolute_error: 52.469845, mean_q: 105.150673\n",
      "Episode 826 reward: 200.0\n",
      " 80862/100000: episode: 826, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.190 [-0.584, 1.148], loss: 25.225698, mean_absolute_error: 52.173985, mean_q: 104.777481\n",
      "Episode 827 reward: 200.0\n",
      " 81062/100000: episode: 827, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.185 [-0.512, 1.155], loss: 30.035881, mean_absolute_error: 52.320591, mean_q: 104.901115\n",
      "Episode 828 reward: 200.0\n",
      " 81262/100000: episode: 828, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.165 [-0.587, 1.022], loss: 23.422197, mean_absolute_error: 52.250050, mean_q: 104.788284\n",
      "Episode 829 reward: 200.0\n",
      " 81462/100000: episode: 829, duration: 1.102s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.069 [-0.519, 0.591], loss: 29.671297, mean_absolute_error: 52.283672, mean_q: 104.867950\n",
      "Episode 830 reward: 200.0\n",
      " 81662/100000: episode: 830, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.056 [-0.565, 0.683], loss: 29.304571, mean_absolute_error: 52.433300, mean_q: 105.212898\n",
      "Episode 831 reward: 200.0\n",
      " 81862/100000: episode: 831, duration: 1.134s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.087 [-0.560, 0.625], loss: 29.585793, mean_absolute_error: 52.681767, mean_q: 105.638031\n",
      "Episode 832 reward: 200.0\n",
      " 82062/100000: episode: 832, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.149 [-0.553, 0.929], loss: 23.986029, mean_absolute_error: 52.654209, mean_q: 105.714218\n",
      "Episode 833 reward: 200.0\n",
      " 82262/100000: episode: 833, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.193 [-0.617, 1.182], loss: 25.841049, mean_absolute_error: 52.248722, mean_q: 104.962357\n",
      "Episode 834 reward: 200.0\n",
      " 82462/100000: episode: 834, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.090 [-0.565, 0.705], loss: 29.867851, mean_absolute_error: 52.646141, mean_q: 105.650978\n",
      "Episode 835 reward: 200.0\n",
      " 82662/100000: episode: 835, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.023 [-0.799, 0.608], loss: 26.063713, mean_absolute_error: 52.821766, mean_q: 106.015388\n",
      "Episode 836 reward: 200.0\n",
      " 82862/100000: episode: 836, duration: 1.181s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.029 [-0.396, 0.572], loss: 25.480503, mean_absolute_error: 52.917412, mean_q: 106.325897\n",
      "Episode 837 reward: 200.0\n",
      " 83062/100000: episode: 837, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.131 [-0.550, 0.818], loss: 25.736172, mean_absolute_error: 52.822041, mean_q: 106.201660\n",
      "Episode 838 reward: 200.0\n",
      " 83262/100000: episode: 838, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.154 [-0.521, 1.011], loss: 32.771694, mean_absolute_error: 52.809208, mean_q: 106.131378\n",
      "Episode 839 reward: 200.0\n",
      " 83462/100000: episode: 839, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.154 [-0.504, 1.043], loss: 37.144249, mean_absolute_error: 53.092072, mean_q: 106.429062\n",
      "Episode 840 reward: 200.0\n",
      " 83662/100000: episode: 840, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.116 [-0.541, 0.734], loss: 27.447683, mean_absolute_error: 52.794731, mean_q: 106.009628\n",
      "Episode 841 reward: 200.0\n",
      " 83862/100000: episode: 841, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.063 [-0.422, 0.580], loss: 33.890621, mean_absolute_error: 52.747414, mean_q: 105.785431\n",
      "Episode 842 reward: 200.0\n",
      " 84062/100000: episode: 842, duration: 1.107s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.019 [-1.115, 0.766], loss: 35.521862, mean_absolute_error: 52.863537, mean_q: 105.954407\n",
      "Episode 843 reward: 200.0\n",
      " 84262/100000: episode: 843, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.249 [-0.957, 1.481], loss: 21.071898, mean_absolute_error: 52.955650, mean_q: 106.534142\n",
      "Episode 844 reward: 200.0\n",
      " 84462/100000: episode: 844, duration: 1.107s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.145 [-0.555, 1.009], loss: 30.962029, mean_absolute_error: 53.007500, mean_q: 106.337402\n",
      "Episode 845 reward: 200.0\n",
      " 84662/100000: episode: 845, duration: 1.130s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.045 [-0.595, 0.566], loss: 27.379028, mean_absolute_error: 53.231369, mean_q: 106.994591\n",
      "Episode 846 reward: 200.0\n",
      " 84862/100000: episode: 846, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.145 [-0.567, 0.841], loss: 19.410088, mean_absolute_error: 52.898350, mean_q: 106.593575\n",
      "Episode 847 reward: 200.0\n",
      " 85062/100000: episode: 847, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.013 [-0.409, 0.616], loss: 28.300371, mean_absolute_error: 53.326393, mean_q: 107.292366\n",
      "Episode 848 reward: 200.0\n",
      " 85262/100000: episode: 848, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.590, 0.644], loss: 28.364069, mean_absolute_error: 53.280235, mean_q: 107.008224\n",
      "Episode 849 reward: 200.0\n",
      " 85462/100000: episode: 849, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-0.512, 0.924], loss: 23.119043, mean_absolute_error: 53.301376, mean_q: 107.132576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 850 reward: 200.0\n",
      " 85662/100000: episode: 850, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.160 [-0.616, 1.079], loss: 36.137497, mean_absolute_error: 53.565060, mean_q: 107.391075\n",
      "Episode 851 reward: 200.0\n",
      " 85862/100000: episode: 851, duration: 1.127s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.072 [-0.481, 0.691], loss: 32.197464, mean_absolute_error: 52.976559, mean_q: 106.214119\n",
      "Episode 852 reward: 200.0\n",
      " 86062/100000: episode: 852, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.155 [-0.620, 0.975], loss: 25.399239, mean_absolute_error: 53.135849, mean_q: 106.664589\n",
      "Episode 853 reward: 200.0\n",
      " 86262/100000: episode: 853, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.136 [-0.676, 0.947], loss: 24.511059, mean_absolute_error: 53.447632, mean_q: 107.305038\n",
      "Episode 854 reward: 200.0\n",
      " 86462/100000: episode: 854, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.518, 0.600], loss: 35.631699, mean_absolute_error: 53.327133, mean_q: 106.834724\n",
      "Episode 855 reward: 200.0\n",
      " 86662/100000: episode: 855, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.193 [-0.539, 1.243], loss: 33.666458, mean_absolute_error: 53.024990, mean_q: 106.244255\n",
      "Episode 856 reward: 200.0\n",
      " 86862/100000: episode: 856, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.146 [-0.566, 0.951], loss: 27.915752, mean_absolute_error: 53.117188, mean_q: 106.528473\n",
      "Episode 857 reward: 200.0\n",
      " 87062/100000: episode: 857, duration: 1.107s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.087 [-0.365, 0.755], loss: 26.738569, mean_absolute_error: 52.984917, mean_q: 106.231163\n",
      "Episode 858 reward: 200.0\n",
      " 87262/100000: episode: 858, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.136 [-0.591, 0.914], loss: 27.652800, mean_absolute_error: 53.060459, mean_q: 106.512604\n",
      "Episode 859 reward: 200.0\n",
      " 87462/100000: episode: 859, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.078 [-0.725, 0.731], loss: 24.570503, mean_absolute_error: 52.749859, mean_q: 105.791687\n",
      "Episode 860 reward: 200.0\n",
      " 87662/100000: episode: 860, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.049 [-0.460, 0.451], loss: 25.452530, mean_absolute_error: 52.833126, mean_q: 106.060860\n",
      "Episode 861 reward: 200.0\n",
      " 87862/100000: episode: 861, duration: 1.125s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.064 [-0.599, 0.555], loss: 23.330053, mean_absolute_error: 52.897274, mean_q: 106.222313\n",
      "Episode 862 reward: 200.0\n",
      " 88062/100000: episode: 862, duration: 1.187s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.452, 0.689], loss: 26.452168, mean_absolute_error: 52.824356, mean_q: 105.932396\n",
      "Episode 863 reward: 200.0\n",
      " 88262/100000: episode: 863, duration: 1.148s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.119 [-0.604, 0.837], loss: 35.368122, mean_absolute_error: 52.701283, mean_q: 105.584854\n",
      "Episode 864 reward: 200.0\n",
      " 88462/100000: episode: 864, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.833, 0.817], loss: 28.954208, mean_absolute_error: 52.536121, mean_q: 105.412376\n",
      "Episode 865 reward: 200.0\n",
      " 88662/100000: episode: 865, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.810, 0.567], loss: 28.716719, mean_absolute_error: 52.578697, mean_q: 105.477249\n",
      "Episode 866 reward: 200.0\n",
      " 88862/100000: episode: 866, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.095 [-0.550, 0.749], loss: 22.354752, mean_absolute_error: 52.472137, mean_q: 105.449043\n",
      "Episode 867 reward: 200.0\n",
      " 89062/100000: episode: 867, duration: 1.128s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.168 [-0.660, 1.031], loss: 20.920820, mean_absolute_error: 52.558067, mean_q: 105.598206\n",
      "Episode 868 reward: 200.0\n",
      " 89262/100000: episode: 868, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.080 [-0.572, 0.668], loss: 26.633936, mean_absolute_error: 52.294765, mean_q: 104.914787\n",
      "Episode 869 reward: 200.0\n",
      " 89462/100000: episode: 869, duration: 1.153s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.035 [-0.579, 0.536], loss: 32.418591, mean_absolute_error: 52.623497, mean_q: 105.500252\n",
      "Episode 870 reward: 200.0\n",
      " 89662/100000: episode: 870, duration: 1.169s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.154 [-0.539, 0.934], loss: 25.055584, mean_absolute_error: 52.442490, mean_q: 105.127243\n",
      "Episode 871 reward: 200.0\n",
      " 89862/100000: episode: 871, duration: 1.152s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.148 [-0.478, 0.951], loss: 25.056110, mean_absolute_error: 52.252251, mean_q: 104.823242\n",
      "Episode 872 reward: 200.0\n",
      " 90062/100000: episode: 872, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.022 [-0.464, 0.708], loss: 23.559555, mean_absolute_error: 52.405704, mean_q: 105.209824\n",
      "Episode 873 reward: 200.0\n",
      " 90262/100000: episode: 873, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.113 [-0.667, 0.696], loss: 24.402779, mean_absolute_error: 52.251488, mean_q: 104.885391\n",
      "Episode 874 reward: 200.0\n",
      " 90462/100000: episode: 874, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.533, 0.642], loss: 21.246218, mean_absolute_error: 52.426865, mean_q: 105.322746\n",
      "Episode 875 reward: 200.0\n",
      " 90662/100000: episode: 875, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-1.003, 0.946], loss: 23.849957, mean_absolute_error: 52.263538, mean_q: 104.915276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 876 reward: 200.0\n",
      " 90862/100000: episode: 876, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.108 [-0.514, 0.704], loss: 22.021021, mean_absolute_error: 52.178993, mean_q: 104.757591\n",
      "Episode 877 reward: 200.0\n",
      " 91062/100000: episode: 877, duration: 1.128s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.502, 0.663], loss: 25.356318, mean_absolute_error: 52.180176, mean_q: 104.770432\n",
      "Episode 878 reward: 200.0\n",
      " 91262/100000: episode: 878, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.097 [-0.780, 0.777], loss: 18.884434, mean_absolute_error: 52.132027, mean_q: 104.805046\n",
      "Episode 879 reward: 200.0\n",
      " 91462/100000: episode: 879, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.053 [-0.625, 0.835], loss: 23.964800, mean_absolute_error: 52.018909, mean_q: 104.451874\n",
      "Episode 880 reward: 200.0\n",
      " 91662/100000: episode: 880, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.193 [-0.559, 1.165], loss: 33.498116, mean_absolute_error: 52.260517, mean_q: 104.713890\n",
      "Episode 881 reward: 200.0\n",
      " 91862/100000: episode: 881, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.093 [-0.785, 0.593], loss: 29.183887, mean_absolute_error: 52.108471, mean_q: 104.356506\n",
      "Episode 882 reward: 200.0\n",
      " 92062/100000: episode: 882, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.133 [-0.586, 0.826], loss: 28.121025, mean_absolute_error: 52.217392, mean_q: 104.739105\n",
      "Episode 883 reward: 200.0\n",
      " 92262/100000: episode: 883, duration: 1.104s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.529, 0.461], loss: 22.110117, mean_absolute_error: 52.125404, mean_q: 104.638260\n",
      "Episode 884 reward: 200.0\n",
      " 92462/100000: episode: 884, duration: 1.130s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.117 [-0.552, 0.741], loss: 25.944502, mean_absolute_error: 51.914650, mean_q: 104.091522\n",
      "Episode 885 reward: 200.0\n",
      " 92662/100000: episode: 885, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.005 [-0.527, 0.654], loss: 21.724136, mean_absolute_error: 52.067551, mean_q: 104.464531\n",
      "Episode 886 reward: 200.0\n",
      " 92862/100000: episode: 886, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.087 [-0.602, 0.710], loss: 26.209389, mean_absolute_error: 52.027420, mean_q: 104.343407\n",
      "Episode 887 reward: 200.0\n",
      " 93062/100000: episode: 887, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.441, 0.413], loss: 18.029617, mean_absolute_error: 51.517014, mean_q: 103.502594\n",
      "Episode 888 reward: 200.0\n",
      " 93262/100000: episode: 888, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-0.564, 0.607], loss: 28.775484, mean_absolute_error: 51.596554, mean_q: 103.397072\n",
      "Episode 889 reward: 200.0\n",
      " 93462/100000: episode: 889, duration: 1.130s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.611, 0.504], loss: 26.065546, mean_absolute_error: 51.671337, mean_q: 103.469551\n",
      "Episode 890 reward: 200.0\n",
      " 93662/100000: episode: 890, duration: 1.108s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.150 [-0.528, 0.950], loss: 30.429600, mean_absolute_error: 51.654373, mean_q: 103.447052\n",
      "Episode 891 reward: 200.0\n",
      " 93862/100000: episode: 891, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.536, 0.590], loss: 22.833149, mean_absolute_error: 51.216095, mean_q: 102.763962\n",
      "Episode 892 reward: 200.0\n",
      " 94062/100000: episode: 892, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.143 [-0.574, 0.891], loss: 29.167862, mean_absolute_error: 51.649258, mean_q: 103.447556\n",
      "Episode 893 reward: 200.0\n",
      " 94262/100000: episode: 893, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.106 [-0.831, 0.768], loss: 25.075586, mean_absolute_error: 51.137993, mean_q: 102.480606\n",
      "Episode 894 reward: 200.0\n",
      " 94462/100000: episode: 894, duration: 1.108s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.798, 0.640], loss: 19.388805, mean_absolute_error: 51.358543, mean_q: 103.046387\n",
      "Episode 895 reward: 200.0\n",
      " 94662/100000: episode: 895, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.627, 0.588], loss: 17.710499, mean_absolute_error: 50.926144, mean_q: 102.263405\n",
      "Episode 896 reward: 200.0\n",
      " 94862/100000: episode: 896, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.179 [-0.769, 1.083], loss: 22.229115, mean_absolute_error: 51.532440, mean_q: 103.332443\n",
      "Episode 897 reward: 200.0\n",
      " 95062/100000: episode: 897, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.563, 0.632], loss: 22.037006, mean_absolute_error: 51.256889, mean_q: 102.836678\n",
      "Episode 898 reward: 200.0\n",
      " 95262/100000: episode: 898, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.005 [-0.770, 0.647], loss: 25.188862, mean_absolute_error: 51.056629, mean_q: 102.365524\n",
      "Episode 899 reward: 200.0\n",
      " 95462/100000: episode: 899, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-0.541, 0.626], loss: 25.530937, mean_absolute_error: 50.921894, mean_q: 102.036766\n",
      "Episode 900 reward: 200.0\n",
      " 95662/100000: episode: 900, duration: 1.133s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.681, 0.873], loss: 24.541893, mean_absolute_error: 51.192970, mean_q: 102.744629\n",
      "Episode 901 reward: 200.0\n",
      " 95862/100000: episode: 901, duration: 1.098s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.029 [-0.442, 0.636], loss: 35.109524, mean_absolute_error: 51.133324, mean_q: 102.329865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 902 reward: 200.0\n",
      " 96062/100000: episode: 902, duration: 1.114s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.589, 0.591], loss: 27.449696, mean_absolute_error: 50.959076, mean_q: 102.092064\n",
      "Episode 903 reward: 200.0\n",
      " 96262/100000: episode: 903, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.579, 0.591], loss: 22.757051, mean_absolute_error: 50.728466, mean_q: 101.835899\n",
      "Episode 904 reward: 200.0\n",
      " 96462/100000: episode: 904, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.587, 0.683], loss: 24.194386, mean_absolute_error: 50.470577, mean_q: 101.281013\n",
      "Episode 905 reward: 200.0\n",
      " 96662/100000: episode: 905, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.026 [-0.635, 0.835], loss: 19.812813, mean_absolute_error: 51.070538, mean_q: 102.486305\n",
      "Episode 906 reward: 200.0\n",
      " 96862/100000: episode: 906, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.561, 0.528], loss: 19.980690, mean_absolute_error: 50.714237, mean_q: 101.789215\n",
      "Episode 907 reward: 200.0\n",
      " 97062/100000: episode: 907, duration: 1.132s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-0.556, 0.733], loss: 24.884899, mean_absolute_error: 50.997929, mean_q: 102.194000\n",
      "Episode 908 reward: 200.0\n",
      " 97262/100000: episode: 908, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.043 [-0.548, 0.593], loss: 19.899706, mean_absolute_error: 50.686989, mean_q: 101.725868\n",
      "Episode 909 reward: 200.0\n",
      " 97462/100000: episode: 909, duration: 1.102s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.093 [-0.613, 0.679], loss: 27.093340, mean_absolute_error: 50.718418, mean_q: 101.657204\n",
      "Episode 910 reward: 200.0\n",
      " 97662/100000: episode: 910, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-0.648, 0.464], loss: 23.614531, mean_absolute_error: 50.845058, mean_q: 102.071243\n",
      "Episode 911 reward: 200.0\n",
      " 97862/100000: episode: 911, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.078 [-0.672, 0.635], loss: 23.216667, mean_absolute_error: 50.839718, mean_q: 101.933113\n",
      "Episode 912 reward: 200.0\n",
      " 98062/100000: episode: 912, duration: 1.136s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.173 [-0.514, 0.992], loss: 22.301289, mean_absolute_error: 50.904816, mean_q: 102.130119\n",
      "Episode 913 reward: 200.0\n",
      " 98262/100000: episode: 913, duration: 1.194s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.542, 0.624], loss: 29.176863, mean_absolute_error: 50.521534, mean_q: 101.124176\n",
      "Episode 914 reward: 200.0\n",
      " 98462/100000: episode: 914, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.633, 0.923], loss: 24.310284, mean_absolute_error: 50.770187, mean_q: 101.846306\n",
      "Episode 915 reward: 200.0\n",
      " 98662/100000: episode: 915, duration: 1.169s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.569, 0.638], loss: 21.198475, mean_absolute_error: 50.841206, mean_q: 102.043793\n",
      "Episode 916 reward: 200.0\n",
      " 98862/100000: episode: 916, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.213 [-1.381, 0.525], loss: 22.723082, mean_absolute_error: 50.467411, mean_q: 101.309563\n",
      "Episode 917 reward: 200.0\n",
      " 99062/100000: episode: 917, duration: 1.158s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.644, 0.555], loss: 25.951889, mean_absolute_error: 50.546757, mean_q: 101.452957\n",
      "Episode 918 reward: 200.0\n",
      " 99262/100000: episode: 918, duration: 1.106s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.154 [-0.587, 0.946], loss: 19.686491, mean_absolute_error: 50.618805, mean_q: 101.613991\n",
      "Episode 919 reward: 200.0\n",
      " 99462/100000: episode: 919, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.491, 0.661], loss: 25.274839, mean_absolute_error: 50.215031, mean_q: 100.715065\n",
      "Episode 920 reward: 200.0\n",
      " 99662/100000: episode: 920, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.114 [-0.445, 0.695], loss: 23.370583, mean_absolute_error: 50.690880, mean_q: 101.655624\n",
      "Episode 921 reward: 200.0\n",
      " 99862/100000: episode: 921, duration: 1.137s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.111 [-0.636, 0.712], loss: 16.375126, mean_absolute_error: 50.442940, mean_q: 101.290466\n",
      "done, took 562.482 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4170e4dcf8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "from rl.callbacks import Callback\n",
    "\n",
    "class EpisodeRewardCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super(EpisodeRewardCallback, self).__init__()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        episode_reward = logs['episode_reward']\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1} reward: {episode_reward}\")\n",
    "\n",
    "# Create an instance of the callback\n",
    "episode_callback = EpisodeRewardCallback()\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=int(1e5), window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "\n",
    "dqn.fit(env, nb_steps=int(1e5), visualize=False, verbose=2,callbacks=[episode_callback])\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All episode rewards: [82.0, 33.0, 52.0, 41.0, 56.0, 40.0, 43.0, 42.0, 38.0, 30.0, 44.0, 47.0, 54.0, 49.0, 31.0, 24.0, 58.0, 55.0, 25.0, 19.0, 16.0, 20.0, 16.0, 18.0, 24.0, 12.0, 18.0, 13.0, 15.0, 15.0, 17.0, 13.0, 16.0, 12.0, 12.0, 14.0, 15.0, 10.0, 15.0, 15.0, 11.0, 13.0, 11.0, 13.0, 11.0, 10.0, 12.0, 11.0, 10.0, 13.0, 11.0, 13.0, 15.0, 12.0, 10.0, 12.0, 11.0, 11.0, 9.0, 15.0, 10.0, 13.0, 14.0, 12.0, 13.0, 12.0, 13.0, 10.0, 11.0, 12.0, 10.0, 9.0, 11.0, 9.0, 10.0, 11.0, 13.0, 11.0, 11.0, 12.0, 12.0, 12.0, 13.0, 12.0, 11.0, 11.0, 16.0, 16.0, 13.0, 16.0, 14.0, 14.0, 16.0, 13.0, 10.0, 13.0, 15.0, 12.0, 12.0, 14.0, 14.0, 15.0, 16.0, 12.0, 18.0, 18.0, 16.0, 16.0, 11.0, 14.0, 14.0, 18.0, 14.0, 12.0, 12.0, 12.0, 18.0, 18.0, 19.0, 24.0, 17.0, 13.0, 20.0, 13.0, 17.0, 20.0, 19.0, 14.0, 13.0, 15.0, 18.0, 17.0, 18.0, 17.0, 22.0, 18.0, 24.0, 22.0, 17.0, 30.0, 20.0, 28.0, 14.0, 21.0, 18.0, 35.0, 43.0, 29.0, 69.0, 40.0, 35.0, 29.0, 31.0, 36.0, 37.0, 65.0, 36.0, 53.0, 27.0, 77.0, 55.0, 106.0, 30.0, 79.0, 22.0, 27.0, 25.0, 20.0, 16.0, 19.0, 20.0, 22.0, 14.0, 18.0, 18.0, 12.0, 14.0, 18.0, 16.0, 11.0, 17.0, 12.0, 12.0, 10.0, 13.0, 14.0, 9.0, 11.0, 12.0, 13.0, 11.0, 9.0, 10.0, 10.0, 9.0, 12.0, 10.0, 10.0, 10.0, 13.0, 10.0, 10.0, 10.0, 12.0, 10.0, 10.0, 10.0, 11.0, 9.0, 10.0, 9.0, 10.0, 9.0, 10.0, 12.0, 10.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 8.0, 11.0, 8.0, 11.0, 9.0, 10.0, 9.0, 12.0, 8.0, 11.0, 9.0, 10.0, 10.0, 12.0, 10.0, 8.0, 9.0, 9.0, 9.0, 10.0, 10.0, 13.0, 10.0, 12.0, 8.0, 9.0, 12.0, 9.0, 12.0, 14.0, 11.0, 11.0, 18.0, 14.0, 13.0, 13.0, 26.0, 26.0, 55.0, 15.0, 22.0, 22.0, 16.0, 26.0, 26.0, 80.0, 58.0, 35.0, 29.0, 48.0, 23.0, 35.0, 49.0, 28.0, 26.0, 27.0, 30.0, 24.0, 23.0, 38.0, 82.0, 112.0, 27.0, 33.0, 30.0, 47.0, 31.0, 131.0, 48.0, 47.0, 40.0, 43.0, 46.0, 72.0, 61.0, 67.0, 50.0, 40.0, 43.0, 54.0, 118.0, 62.0, 86.0, 38.0, 52.0, 31.0, 94.0, 56.0, 74.0, 68.0, 156.0, 46.0, 41.0, 43.0, 43.0, 131.0, 47.0, 48.0, 71.0, 42.0, 67.0, 74.0, 32.0, 101.0, 56.0, 69.0, 68.0, 37.0, 160.0, 55.0, 60.0, 84.0, 61.0, 54.0, 44.0, 66.0, 93.0, 61.0, 73.0, 72.0, 71.0, 69.0, 49.0, 67.0, 74.0, 200.0, 57.0, 32.0, 64.0, 78.0, 34.0, 43.0, 118.0, 87.0, 71.0, 71.0, 149.0, 43.0, 60.0, 65.0, 76.0, 78.0, 67.0, 106.0, 64.0, 99.0, 102.0, 91.0, 70.0, 91.0, 74.0, 66.0, 73.0, 69.0, 44.0, 93.0, 79.0, 83.0, 45.0, 200.0, 98.0, 90.0, 107.0, 68.0, 136.0, 55.0, 90.0, 74.0, 75.0, 80.0, 64.0, 86.0, 122.0, 97.0, 81.0, 101.0, 92.0, 77.0, 200.0, 111.0, 124.0, 71.0, 52.0, 72.0, 108.0, 181.0, 118.0, 107.0, 95.0, 80.0, 111.0, 118.0, 94.0, 97.0, 100.0, 158.0, 97.0, 97.0, 179.0, 200.0, 200.0, 167.0, 120.0, 103.0, 118.0, 49.0, 188.0, 87.0, 54.0, 57.0, 137.0, 41.0, 45.0, 67.0, 74.0, 65.0, 122.0, 66.0, 51.0, 163.0, 111.0, 59.0, 44.0, 68.0, 74.0, 45.0, 55.0, 155.0, 74.0, 131.0, 187.0, 75.0, 200.0, 67.0, 51.0, 57.0, 38.0, 93.0, 51.0, 92.0, 77.0, 70.0, 200.0, 55.0, 88.0, 67.0, 146.0, 56.0, 64.0, 77.0, 48.0, 58.0, 90.0, 58.0, 134.0, 168.0, 78.0, 136.0, 57.0, 193.0, 113.0, 55.0, 186.0, 200.0, 67.0, 56.0, 72.0, 69.0, 60.0, 110.0, 70.0, 149.0, 94.0, 80.0, 89.0, 83.0, 99.0, 200.0, 82.0, 62.0, 166.0, 152.0, 141.0, 118.0, 85.0, 75.0, 79.0, 141.0, 198.0, 200.0, 64.0, 69.0, 60.0, 58.0, 66.0, 48.0, 73.0, 61.0, 74.0, 168.0, 76.0, 94.0, 200.0, 104.0, 69.0, 107.0, 68.0, 70.0, 85.0, 81.0, 79.0, 69.0, 79.0, 68.0, 103.0, 95.0, 102.0, 63.0, 71.0, 69.0, 93.0, 117.0, 200.0, 163.0, 87.0, 102.0, 96.0, 200.0, 125.0, 122.0, 106.0, 116.0, 200.0, 111.0, 193.0, 200.0, 200.0, 95.0, 200.0, 116.0, 134.0, 136.0, 193.0, 171.0, 110.0, 200.0, 197.0, 95.0, 121.0, 151.0, 101.0, 200.0, 101.0, 112.0, 137.0, 150.0, 114.0, 200.0, 120.0, 159.0, 139.0, 200.0, 132.0, 138.0, 146.0, 143.0, 164.0, 151.0, 166.0, 200.0, 131.0, 200.0, 200.0, 200.0, 200.0, 194.0, 172.0, 155.0, 200.0, 170.0, 173.0, 187.0, 200.0, 200.0, 200.0, 200.0, 200.0, 197.0, 200.0, 200.0, 200.0, 200.0, 200.0, 171.0, 200.0, 200.0, 200.0, 176.0, 200.0, 200.0, 195.0, 200.0, 175.0, 191.0, 200.0, 200.0, 184.0, 180.0, 174.0, 192.0, 182.0, 180.0, 194.0, 198.0, 200.0, 200.0, 200.0, 200.0, 200.0, 191.0, 173.0, 176.0, 174.0, 200.0, 200.0, 200.0, 182.0, 196.0, 200.0, 180.0, 200.0, 200.0, 182.0, 184.0, 200.0, 200.0, 200.0, 200.0, 200.0, 189.0, 189.0, 193.0, 200.0, 200.0, 200.0, 183.0, 195.0, 200.0, 184.0, 195.0, 200.0, 194.0, 177.0, 192.0, 172.0, 200.0, 168.0, 170.0, 200.0, 179.0, 200.0, 200.0, 183.0, 187.0, 200.0, 188.0, 200.0, 200.0, 183.0, 179.0, 177.0, 200.0, 194.0, 200.0, 200.0, 200.0, 190.0, 189.0, 200.0, 200.0, 196.0, 189.0, 200.0, 198.0, 194.0, 200.0, 200.0, 200.0, 200.0, 200.0, 181.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 194.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 184.0, 200.0, 200.0, 200.0, 197.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"All episode rewards:\", episode_callback.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(running_mean(episode_callback.episode_rewards,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(rwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA algorithm\n",
    "def SARSA_algorithm(num_episodes):\n",
    "    allowed_actions=[0,1]\n",
    "    # Variables: alpha, gamma y epsilon.\n",
    "    alpha = 0.3\n",
    "    gamma = 1\n",
    "    epsilon = 0.1\n",
    "    cont = 0\n",
    "    #Para 5000 episodios de entrenamiento\n",
    "    for i in range(num_episodes):\n",
    "        # Inicializa las variables para cada episodio\n",
    "        state=env.reset()\n",
    "        num_steps = 0\n",
    "        state = discretize(state[0])\n",
    "\n",
    "\n",
    "        # Seleccion accion \"a\" de forma epsilon-greedy\n",
    "        if epsilon< np.random.uniform():        \n",
    "            act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "            action = allowed_actions[np.argmax(act_arg)]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Inicia el episodio\n",
    "        continue_episode = True\n",
    "        while continue_episode:        \n",
    "            # Obtengo s'\n",
    "            new_state, reward, done, _,_=env.step(action)\n",
    "            new_state = discretize(new_state)\n",
    "\n",
    "            # Revisa que new_state no sea un estado terminal\n",
    "            if done:\n",
    "                # Valor q(s',a') terminal\n",
    "                q_value_next_step = 0\n",
    "            else:\n",
    "                #Obtengo a' de s' con epsilon greedy\n",
    "                if epsilon< np.random.uniform():        \n",
    "                    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "                    new_action = allowed_actions[np.argmax(act_arg)]\n",
    "                else:\n",
    "                    new_action = env.action_space.sample()\n",
    "\n",
    "                # Valor q(s',a') no terminal\n",
    "                q_value_next_step = Q_table[tuple(new_state),new_action]\n",
    "\n",
    "\n",
    "            # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*q(s',a') - q(s,a))\n",
    "            Q_table[tuple(state), action] += alpha*(reward + gamma*q_value_next_step - Q_table[tuple(state),action])\n",
    "\n",
    "            # asigna a = a' y s = s'\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "\n",
    "\n",
    "            # Parte que termina el episodio si se llega a algun estado terminal\n",
    "            if done:\n",
    "                continue_episode = False\n",
    "        cont+=1\n",
    "        if cont%100==0:\n",
    "            print(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 400 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(400)\n",
    "SARSA_algorithm(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_actions= [0,1]\n",
    "prom=[]\n",
    "prom_rand=[]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 4000 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(4000)\n",
    "SARSA_algorithm(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom=[]\n",
    "prom_rand=[]\n",
    "allowed_actions= [0,1]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
