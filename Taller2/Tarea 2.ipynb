{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be9c669-72a3-47f0-a272-5b769e2dab78",
   "metadata": {},
   "source": [
    "# Escaleras y serpientes (tarea 2 RL)\n",
    "Juan Pablo Reyes Fajardo\n",
    "\n",
    "Santiago Rodríguez Ávila"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8cc8f-4816-42ce-aaf9-d0740bcc30c0",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/tablero.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ca8a0-e2eb-47bc-92c1-6dba3d191bbf",
   "metadata": {},
   "source": [
    "Se tiene el siguiente juego de escalera, hay serpientes que hacen descender y escaleras que hacen ascender. Los puntos rojos y azules son casillas en donde se va a terminar el juego, si cae en la casilla roja se pierde la partida, si cae en la casilla azul se gana la partida.\n",
    "\n",
    "La idea es utilizar diferentes métodos de aprendizaje por refuerzo para ver cómo se comporta con el juego."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c840d-22a9-463f-bd7d-f60dcf144b59",
   "metadata": {},
   "source": [
    "### Reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39002ae4-81f7-4cdc-b161-29f17c3ba71f",
   "metadata": {},
   "source": [
    "* El jugador decide si avanzar o retroceder la cantidad de casillas que indique el dado **Antes** de lanzarlo.\n",
    "* Si se llega a uno de los bordes de la escalera (1 o 100), el jugador \\\"rebotará\\\" en las casillas y retrocederá o avanzará (dependiendo el caso) el número de casillas restantes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfe0d6-a714-426b-9b0e-71b33a204080",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Modelando el MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd49816-bfd7-47e3-a563-5be37e5b4aea",
   "metadata": {},
   "source": [
    "* Estados -> 100. Estos corresponden a las posiciones posibles del tablero. Cuando el agente llega al inicio de una escalera o en la cabeza de una serpiente, el estado al que va a seguir el agente va a ser desde el otro lado de la escalera/serpiente.\n",
    "* Recompensas -> 1. si el jugador llega a un punto azul, -1 si el jugador llega a un punto rojo. -0.1 de lo contrario.\n",
    "* Acciones -> 2. El agente solo se puede mover en dos direcciones: adelante (aumentando casillas) o atrás (retrocediendo).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9454a6d-b02b-46a1-ae1d-1fbc1622713e",
   "metadata": {},
   "source": [
    "**Nota**: La recompensa de paso negativo se colocó para que el agente no tratara de evitar los puntos rojos, si no que se enfoque en llegar a algún punto terminal bueno. Por eso es que la recompensa de paso es un poco negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663bd9b-befa-4d08-8b6e-beca8d2e2685",
   "metadata": {},
   "source": [
    "A continuación se utilizará el módulo de *Python* creado por los estudiantes llamado *escalerasyserpientes.py* para crear el tablero, los puntos terminales (positivos como negativos) y el valor de la recompensa del paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c1c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from escalerasyserpientes import EscalerasSerpientes\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9be020",
   "metadata": {},
   "source": [
    "Inicializar el MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75920e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EscalerasSerpientes([80,100],[23,37,45,67,89],-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdd571",
   "metadata": {},
   "source": [
    "Visualizar los estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16757cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "print(es.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da771157",
   "metadata": {},
   "source": [
    "Visualizar la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f7be134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Ad', 2: 'Ad', 3: 'Ad', 4: 'Ad', 5: 'Ad', 6: 'Ad', 7: 'Ad', 8: 'Ad', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'Ad', 16: 'Ad', 17: 'Ad', 18: 'Ad', 19: 'Ad', 20: 'Ad', 21: 'Ad', 22: 'Ad', 23: 'Ad', 24: 'Ad', 25: 'Ad', 26: 'Ad', 27: 'Ad', 28: 'Ad', 29: 'Ad', 30: 'Ad', 31: 'Ad', 32: 'Ad', 33: 'Ad', 34: 'Ad', 35: 'Ad', 36: 'Ad', 37: 'Ad', 38: 'Ad', 39: 'Ad', 40: 'Ad', 41: 'Ad', 42: 'Ad', 43: 'Ad', 44: 'Ad', 45: 'Ad', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'Ad', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'Ad', 57: 'Ad', 58: 'Ad', 59: 'Ad', 60: 'Ad', 61: 'Ad', 62: 'Ad', 63: 'Ad', 64: 'Ad', 65: 'Ad', 66: 'Ad', 67: 'Ad', 68: 'Ad', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: 'Ad', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'Ad', 78: 'Ad', 79: 'Ad', 80: 'Ad', 81: 'Ad', 82: 'Ad', 83: 'Ad', 84: 'Ad', 85: 'Ad', 86: 'Ad', 87: 'Ad', 88: 'Ad', 89: 'Ad', 90: 'Ad', 91: 'Ad', 92: 'Ad', 93: 'Ad', 94: 'Ad', 95: 'Ad', 96: 'Ad', 97: 'Ad', 98: 'Ad', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "print(es.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22681608-71cf-4b4c-b45e-992f83c513d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Revisión del módulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd3626a",
   "metadata": {},
   "source": [
    "Para revisar que el módulo funciona correctamente, se va a probar la función de pasos para estados resultantes, para esto se tendrá un valor de estado y un valor de lanzado de dado, también se hará el paso hacia adelante. El resultado fue el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "922a2865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, -0.1, 5, False)\n"
     ]
    }
   ],
   "source": [
    "state_prueba=98\n",
    "action_prueba=5\n",
    "print(es.step(state_prueba, action_prueba, random=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2726cce-11df-4eb7-8b07-c7c9228df7b7",
   "metadata": {},
   "source": [
    "Se puede ver que está bien implementado ya que se tiene el valor de la casilla de \"rebote\" una vez el agente llega a la casilla 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb7c09",
   "metadata": {},
   "source": [
    "Ahora se hará una prueba para probar función de pasos para acciones aleatorias del agente (se va a lanzar un dado y ver cómo se comporta):\n",
    "\n",
    "Estructura de retorno:\n",
    "* Estado siguiente, recompensa estado, Accion tomada, Estado terminal (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d015abbd-8e8c-4cf6-bc3e-fa518c65f110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, -0.1, 4, False)\n",
      "(97, -0.1, 5, False)\n",
      "(96, -0.1, 6, False)\n",
      "(99, -0.1, 3, False)\n",
      "(28, -0.1, 4, False)\n",
      "(96, -0.1, 6, False)\n",
      "(100, -0.1, 2, False)\n",
      "(28, -0.1, 4, False)\n",
      "(99, -0.1, 1, False)\n",
      "(96, -0.1, 6, False)\n",
      "(28, -0.1, 4, False)\n",
      "(99, -0.1, 3, False)\n",
      "(28, -0.1, 4, False)\n",
      "(99, -0.1, 1, False)\n",
      "(99, -0.1, 1, False)\n",
      "(99, -0.1, 1, False)\n",
      "(96, -0.1, 6, False)\n",
      "(100, -0.1, 2, False)\n"
     ]
    }
   ],
   "source": [
    "state_prueba=98\n",
    "action_prueba='Ad'\n",
    "for i in range(6*3):\n",
    "    print(es.step(state_prueba, action_prueba, random=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47002972-2534-4d00-b2d8-93f228042181",
   "metadata": {},
   "source": [
    "Nuevamente, se puede comprobar que el módulo está correctamente implementado porque se tienen los valores esperados dependiendo del valor del dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d7c2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Actualización de valores y programación dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f431c8f-f723-4b87-bb3d-c12d4f84e60a",
   "metadata": {},
   "source": [
    "A continuación se implementará un algoritmo de estimación de valores que se hará con programación dinámica, luego se hará una prueba con un factor de descuento  $\\gamma$ y un horizonte H determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826b3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(es, gamma):\n",
    "    # Arreglos para los valores\n",
    "    value = dict.fromkeys(es.states , 0.0)\n",
    "    new_values = dict.fromkeys(es.states , 0.0)\n",
    "    \n",
    "    for state in es.states: \n",
    "        if state in es.snakes:\n",
    "            new_values[state]=es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            new_values[state]=es.state_values[es.stairs[state]]\n",
    "        else:\n",
    "            # Tomar acción de politica dada\n",
    "            action = es.policy[state]\n",
    "            prob=es.action_probabilities[0]\n",
    "            \n",
    "            real_actions=es.real_actions[action]\n",
    "            for real_action in real_actions:\n",
    "                # Dar un paso\n",
    "                new_state, reward, _, done = es.step(state, real_action, random=False)\n",
    "                # Actualizar valores\n",
    "                if(done):\n",
    "                    # actualización para un estado terminal\n",
    "                    new_values[state] = reward\n",
    "                else:\n",
    "                    # actualización para un estado no terminal\n",
    "                    new_values[state] += prob*(reward + gamma*es.state_values[new_state]) #borrar\n",
    "\n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    es.state_values = copy.deepcopy(new_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a6eb3",
   "metadata": {},
   "source": [
    "Gamma en 0.9, horizonte en 2 y política siempre adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b031fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "H = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d6275",
   "metadata": {},
   "source": [
    "Inicializa valores y corre programación dinámica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196ac5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es.state_values=es.init_values()\n",
    "es.update_values = MethodType(update_values, es)\n",
    "es.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f27633",
   "metadata": {},
   "source": [
    "Función de valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae627731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(1) = -0.190\n",
      "V(2) = -0.190\n",
      "V(3) = -0.190\n",
      "V(4) = -0.190\n",
      "V(5) = -0.190\n",
      "V(6) = -0.190\n",
      "V(7) = -0.190\n",
      "V(8) = -0.100\n",
      "V(9) = -0.190\n",
      "V(10) = -0.190\n",
      "V(11) = -0.190\n",
      "V(12) = -0.190\n",
      "V(13) = -0.190\n",
      "V(14) = -0.190\n",
      "V(15) = -0.190\n",
      "V(16) = -0.190\n",
      "V(17) = -0.325\n",
      "V(18) = -0.325\n",
      "V(19) = -0.325\n",
      "V(20) = -0.325\n",
      "V(21) = -0.100\n",
      "V(22) = -0.325\n",
      "V(23) = -1.000\n",
      "V(24) = -0.190\n",
      "V(25) = -0.190\n",
      "V(26) = -0.190\n",
      "V(27) = -0.190\n",
      "V(28) = -0.190\n",
      "V(29) = -0.190\n",
      "V(30) = -0.190\n",
      "V(31) = -0.325\n",
      "V(32) = -0.325\n",
      "V(33) = -0.325\n",
      "V(34) = -0.325\n",
      "V(35) = -0.325\n",
      "V(36) = -0.325\n",
      "V(37) = -1.000\n",
      "V(38) = -0.190\n",
      "V(39) = -0.325\n",
      "V(40) = -0.325\n",
      "V(41) = -0.325\n",
      "V(42) = -0.325\n",
      "V(43) = -0.100\n",
      "V(44) = -0.100\n",
      "V(45) = -1.000\n",
      "V(46) = -0.100\n",
      "V(47) = -0.190\n",
      "V(48) = -0.100\n",
      "V(49) = -0.190\n",
      "V(50) = -0.100\n",
      "V(51) = -0.190\n",
      "V(52) = -0.100\n",
      "V(53) = -0.190\n",
      "V(54) = -0.100\n",
      "V(55) = -0.100\n",
      "V(56) = -0.190\n",
      "V(57) = -0.190\n",
      "V(58) = -0.190\n",
      "V(59) = -0.100\n",
      "V(60) = -0.190\n",
      "V(61) = -0.325\n",
      "V(62) = -0.100\n",
      "V(63) = -0.325\n",
      "V(64) = -0.100\n",
      "V(65) = -0.325\n",
      "V(66) = -0.100\n",
      "V(67) = -1.000\n",
      "V(68) = -0.190\n",
      "V(69) = -0.100\n",
      "V(70) = -0.190\n",
      "V(71) = -0.190\n",
      "V(72) = -0.190\n",
      "V(73) = -0.100\n",
      "V(74) = -0.025\n",
      "V(75) = -0.025\n",
      "V(76) = -0.025\n",
      "V(77) = -0.025\n",
      "V(78) = -0.025\n",
      "V(79) = -0.025\n",
      "V(80) = 1.000\n",
      "V(81) = -0.190\n",
      "V(82) = -0.190\n",
      "V(83) = -0.100\n",
      "V(84) = -0.325\n",
      "V(85) = -0.325\n",
      "V(86) = -0.325\n",
      "V(87) = -0.325\n",
      "V(88) = -0.325\n",
      "V(89) = -1.000\n",
      "V(90) = -0.190\n",
      "V(91) = -0.190\n",
      "V(92) = -0.100\n",
      "V(93) = -0.190\n",
      "V(94) = -0.025\n",
      "V(95) = -0.100\n",
      "V(96) = -0.025\n",
      "V(97) = -0.025\n",
      "V(98) = -0.100\n",
      "V(99) = -0.025\n",
      "V(100) = 1.000\n"
     ]
    }
   ],
   "source": [
    "for state in es.states:\n",
    "    print(f'V({state}) = {es.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af1701",
   "metadata": {},
   "source": [
    "Ahora se volverá a hacer la prueba pero con otra política, esta vez, aleatoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "672b651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in es.policy: \n",
    "    rand=np.random.uniform()\n",
    "    if rand<0.5:\n",
    "        es.policy[i]='Ad'\n",
    "    else:\n",
    "        es.policy[i]='At'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143952e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Ad', 2: 'At', 3: 'At', 4: 'At', 5: 'Ad', 6: 'At', 7: 'At', 8: 'At', 9: 'At', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'At', 14: 'At', 15: 'Ad', 16: 'At', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: 'At', 22: 'At', 23: 'At', 24: 'At', 25: 'At', 26: 'At', 27: 'Ad', 28: 'Ad', 29: 'Ad', 30: 'At', 31: 'At', 32: 'Ad', 33: 'At', 34: 'Ad', 35: 'Ad', 36: 'Ad', 37: 'Ad', 38: 'At', 39: 'At', 40: 'At', 41: 'Ad', 42: 'Ad', 43: 'At', 44: 'At', 45: 'At', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'At', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'Ad', 57: 'At', 58: 'At', 59: 'Ad', 60: 'At', 61: 'Ad', 62: 'Ad', 63: 'Ad', 64: 'Ad', 65: 'Ad', 66: 'Ad', 67: 'Ad', 68: 'At', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'At', 73: 'At', 74: 'Ad', 75: 'Ad', 76: 'At', 77: 'At', 78: 'Ad', 79: 'Ad', 80: 'At', 81: 'At', 82: 'Ad', 83: 'Ad', 84: 'Ad', 85: 'Ad', 86: 'Ad', 87: 'Ad', 88: 'Ad', 89: 'At', 90: 'Ad', 91: 'At', 92: 'At', 93: 'At', 94: 'Ad', 95: 'Ad', 96: 'At', 97: 'At', 98: 'Ad', 99: 'At', 100: 'At'}\n"
     ]
    }
   ],
   "source": [
    "print(es.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9d0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "H = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ea999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.state_values=es.init_values()\n",
    "es.update_values = MethodType(update_values, es)\n",
    "es.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3bdae-6173-402b-8533-4f7320234352",
   "metadata": {},
   "source": [
    "Función de valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e638f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(1) = -0.190\n",
      "V(2) = -0.190\n",
      "V(3) = -0.190\n",
      "V(4) = -0.190\n",
      "V(5) = -0.190\n",
      "V(6) = -0.190\n",
      "V(7) = -0.190\n",
      "V(8) = -0.100\n",
      "V(9) = -0.190\n",
      "V(10) = -0.190\n",
      "V(11) = -0.190\n",
      "V(12) = -0.190\n",
      "V(13) = -0.190\n",
      "V(14) = -0.190\n",
      "V(15) = -0.190\n",
      "V(16) = -0.190\n",
      "V(17) = -0.190\n",
      "V(18) = -0.190\n",
      "V(19) = -0.190\n",
      "V(20) = -0.190\n",
      "V(21) = -0.100\n",
      "V(22) = -0.190\n",
      "V(23) = -1.000\n",
      "V(24) = -0.325\n",
      "V(25) = -0.325\n",
      "V(26) = -0.325\n",
      "V(27) = -0.190\n",
      "V(28) = -0.190\n",
      "V(29) = -0.190\n",
      "V(30) = -0.190\n",
      "V(31) = -0.190\n",
      "V(32) = -0.325\n",
      "V(33) = -0.190\n",
      "V(34) = -0.325\n",
      "V(35) = -0.325\n",
      "V(36) = -0.325\n",
      "V(37) = -1.000\n",
      "V(38) = -0.325\n",
      "V(39) = -0.325\n",
      "V(40) = -0.325\n",
      "V(41) = -0.325\n",
      "V(42) = -0.325\n",
      "V(43) = -0.100\n",
      "V(44) = -0.100\n",
      "V(45) = -1.000\n",
      "V(46) = -0.100\n",
      "V(47) = -0.190\n",
      "V(48) = -0.100\n",
      "V(49) = -0.190\n",
      "V(50) = -0.100\n",
      "V(51) = -0.190\n",
      "V(52) = -0.100\n",
      "V(53) = -0.190\n",
      "V(54) = -0.100\n",
      "V(55) = -0.100\n",
      "V(56) = -0.190\n",
      "V(57) = -0.190\n",
      "V(58) = -0.190\n",
      "V(59) = -0.100\n",
      "V(60) = -0.190\n",
      "V(61) = -0.325\n",
      "V(62) = -0.100\n",
      "V(63) = -0.325\n",
      "V(64) = -0.100\n",
      "V(65) = -0.325\n",
      "V(66) = -0.100\n",
      "V(67) = -1.000\n",
      "V(68) = -0.325\n",
      "V(69) = -0.100\n",
      "V(70) = -0.190\n",
      "V(71) = -0.190\n",
      "V(72) = -0.325\n",
      "V(73) = -0.100\n",
      "V(74) = -0.025\n",
      "V(75) = -0.025\n",
      "V(76) = -0.190\n",
      "V(77) = -0.190\n",
      "V(78) = -0.025\n",
      "V(79) = -0.025\n",
      "V(80) = 1.000\n",
      "V(81) = -0.025\n",
      "V(82) = -0.190\n",
      "V(83) = -0.100\n",
      "V(84) = -0.325\n",
      "V(85) = -0.325\n",
      "V(86) = -0.325\n",
      "V(87) = -0.325\n",
      "V(88) = -0.325\n",
      "V(89) = -1.000\n",
      "V(90) = -0.190\n",
      "V(91) = -0.325\n",
      "V(92) = -0.100\n",
      "V(93) = -0.325\n",
      "V(94) = -0.025\n",
      "V(95) = -0.100\n",
      "V(96) = -0.190\n",
      "V(97) = -0.190\n",
      "V(98) = -0.100\n",
      "V(99) = -0.190\n",
      "V(100) = 1.000\n"
     ]
    }
   ],
   "source": [
    "for state in es.states:\n",
    "    print(f'V({state}) = {es.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cad89c-6c55-43ed-8c56-a7172ba67026",
   "metadata": {},
   "source": [
    "Ahora se volverá a la politica de siempre adelante y se probarán otros valores de gamma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc7c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(1) = -0.195\n",
      "V(2) = -0.195\n",
      "V(3) = -0.195\n",
      "V(4) = -0.195\n",
      "V(5) = -0.195\n",
      "V(6) = -0.195\n",
      "V(7) = -0.195\n",
      "V(8) = -0.100\n",
      "V(9) = -0.195\n",
      "V(10) = -0.195\n",
      "V(11) = -0.195\n",
      "V(12) = -0.195\n",
      "V(13) = -0.195\n",
      "V(14) = -0.195\n",
      "V(15) = -0.195\n",
      "V(16) = -0.195\n",
      "V(17) = -0.338\n",
      "V(18) = -0.338\n",
      "V(19) = -0.337\n",
      "V(20) = -0.337\n",
      "V(21) = -0.100\n",
      "V(22) = -0.337\n",
      "V(23) = -1.000\n",
      "V(24) = -0.195\n",
      "V(25) = -0.195\n",
      "V(26) = -0.195\n",
      "V(27) = -0.195\n",
      "V(28) = -0.195\n",
      "V(29) = -0.195\n",
      "V(30) = -0.195\n",
      "V(31) = -0.338\n",
      "V(32) = -0.338\n",
      "V(33) = -0.337\n",
      "V(34) = -0.337\n",
      "V(35) = -0.337\n",
      "V(36) = -0.337\n",
      "V(37) = -1.000\n",
      "V(38) = -0.195\n",
      "V(39) = -0.338\n",
      "V(40) = -0.338\n",
      "V(41) = -0.337\n",
      "V(42) = -0.337\n",
      "V(43) = -0.100\n",
      "V(44) = -0.100\n",
      "V(45) = -1.000\n",
      "V(46) = -0.100\n",
      "V(47) = -0.195\n",
      "V(48) = -0.100\n",
      "V(49) = -0.195\n",
      "V(50) = -0.100\n",
      "V(51) = -0.195\n",
      "V(52) = -0.100\n",
      "V(53) = -0.195\n",
      "V(54) = -0.100\n",
      "V(55) = -0.100\n",
      "V(56) = -0.195\n",
      "V(57) = -0.195\n",
      "V(58) = -0.195\n",
      "V(59) = -0.100\n",
      "V(60) = -0.195\n",
      "V(61) = -0.338\n",
      "V(62) = -0.100\n",
      "V(63) = -0.337\n",
      "V(64) = -0.100\n",
      "V(65) = -0.337\n",
      "V(66) = -0.100\n",
      "V(67) = -1.000\n",
      "V(68) = -0.195\n",
      "V(69) = -0.100\n",
      "V(70) = -0.195\n",
      "V(71) = -0.195\n",
      "V(72) = -0.195\n",
      "V(73) = -0.100\n",
      "V(74) = -0.021\n",
      "V(75) = -0.021\n",
      "V(76) = -0.021\n",
      "V(77) = -0.021\n",
      "V(78) = -0.021\n",
      "V(79) = -0.021\n",
      "V(80) = 1.000\n",
      "V(81) = -0.195\n",
      "V(82) = -0.195\n",
      "V(83) = -0.100\n",
      "V(84) = -0.338\n",
      "V(85) = -0.337\n",
      "V(86) = -0.337\n",
      "V(87) = -0.337\n",
      "V(88) = -0.337\n",
      "V(89) = -1.000\n",
      "V(90) = -0.195\n",
      "V(91) = -0.195\n",
      "V(92) = -0.100\n",
      "V(93) = -0.195\n",
      "V(94) = -0.021\n",
      "V(95) = -0.100\n",
      "V(96) = -0.021\n",
      "V(97) = -0.021\n",
      "V(98) = -0.100\n",
      "V(99) = -0.021\n",
      "V(100) = 1.000\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.95\n",
    "H = 2\n",
    "for i in es.policy: \n",
    "    es.policy[i]='Ad'\n",
    "es.state_values=es.init_values()\n",
    "es.update_values = MethodType(update_values, es)\n",
    "es.solve_dynamic_programming(gamma=gamma, horizon=H)\n",
    "\n",
    "for state in es.states:\n",
    "    print(f'V({state}) = {es.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa45555-7dd4-458c-8e4c-0f270ad20ac8",
   "metadata": {},
   "source": [
    "Ahora con la política aleatoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be6003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(1) = -0.195\n",
      "V(2) = -0.195\n",
      "V(3) = -0.195\n",
      "V(4) = -0.195\n",
      "V(5) = -0.195\n",
      "V(6) = -0.195\n",
      "V(7) = -0.195\n",
      "V(8) = -0.100\n",
      "V(9) = -0.195\n",
      "V(10) = -0.195\n",
      "V(11) = -0.195\n",
      "V(12) = -0.195\n",
      "V(13) = -0.195\n",
      "V(14) = -0.195\n",
      "V(15) = -0.195\n",
      "V(16) = -0.195\n",
      "V(17) = -0.338\n",
      "V(18) = -0.338\n",
      "V(19) = -0.337\n",
      "V(20) = -0.195\n",
      "V(21) = -0.100\n",
      "V(22) = -0.195\n",
      "V(23) = -1.000\n",
      "V(24) = -0.195\n",
      "V(25) = -0.337\n",
      "V(26) = -0.195\n",
      "V(27) = -0.195\n",
      "V(28) = -0.195\n",
      "V(29) = -0.338\n",
      "V(30) = -0.195\n",
      "V(31) = -0.195\n",
      "V(32) = -0.195\n",
      "V(33) = -0.337\n",
      "V(34) = -0.337\n",
      "V(35) = -0.195\n",
      "V(36) = -0.337\n",
      "V(37) = -1.000\n",
      "V(38) = -0.195\n",
      "V(39) = -0.338\n",
      "V(40) = -0.338\n",
      "V(41) = -0.337\n",
      "V(42) = -0.337\n",
      "V(43) = -0.100\n",
      "V(44) = -0.100\n",
      "V(45) = -1.000\n",
      "V(46) = -0.100\n",
      "V(47) = -0.337\n",
      "V(48) = -0.100\n",
      "V(49) = -0.195\n",
      "V(50) = -0.100\n",
      "V(51) = -0.338\n",
      "V(52) = -0.100\n",
      "V(53) = -0.195\n",
      "V(54) = -0.100\n",
      "V(55) = -0.100\n",
      "V(56) = -0.195\n",
      "V(57) = -0.195\n",
      "V(58) = -0.195\n",
      "V(59) = -0.100\n",
      "V(60) = -0.195\n",
      "V(61) = -0.338\n",
      "V(62) = -0.100\n",
      "V(63) = -0.337\n",
      "V(64) = -0.100\n",
      "V(65) = -0.337\n",
      "V(66) = -0.100\n",
      "V(67) = -1.000\n",
      "V(68) = -0.337\n",
      "V(69) = -0.100\n",
      "V(70) = -0.337\n",
      "V(71) = -0.337\n",
      "V(72) = -0.195\n",
      "V(73) = -0.100\n",
      "V(74) = -0.021\n",
      "V(75) = -0.021\n",
      "V(76) = -0.195\n",
      "V(77) = -0.195\n",
      "V(78) = -0.195\n",
      "V(79) = -0.021\n",
      "V(80) = 1.000\n",
      "V(81) = -0.195\n",
      "V(82) = -0.195\n",
      "V(83) = -0.100\n",
      "V(84) = -0.338\n",
      "V(85) = -0.021\n",
      "V(86) = -0.337\n",
      "V(87) = -0.337\n",
      "V(88) = -0.195\n",
      "V(89) = -1.000\n",
      "V(90) = -0.195\n",
      "V(91) = -0.337\n",
      "V(92) = -0.100\n",
      "V(93) = -0.195\n",
      "V(94) = -0.338\n",
      "V(95) = -0.100\n",
      "V(96) = -0.021\n",
      "V(97) = -0.195\n",
      "V(98) = -0.100\n",
      "V(99) = -0.195\n",
      "V(100) = 1.000\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.95\n",
    "H = 2\n",
    "for i in es.policy: \n",
    "    rand=np.random.uniform()\n",
    "    if rand<0.5:\n",
    "        es.policy[i]='Ad'\n",
    "    else:\n",
    "        es.policy[i]='At'\n",
    "es.state_values=es.init_values()\n",
    "es.update_values = MethodType(update_values, es)\n",
    "es.solve_dynamic_programming(gamma=gamma, horizon=H)\n",
    "\n",
    "for state in es.states:\n",
    "    print(f'V({state}) = {es.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86071be9-45f1-4703-b1c4-3dbed777033e",
   "metadata": {},
   "source": [
    "### Conclusión  Actualización de valores y programación dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e08785-02c1-4904-9c5c-d3cf38bc97f8",
   "metadata": {},
   "source": [
    "De esta parte del documento se puede concluir que los valores más cercanos a 0 se encuentran cerca de los puntos terminales positivos (-0.021). También cuando hay casillas que se encuentran lejos de los estados terminales tienen un valor parecido (-0.195) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81d554-2fd7-4567-b1b8-4387b5c3230c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Value iteration  $V^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76583cf5-3e64-494c-8c6d-528193f5447c",
   "metadata": {},
   "source": [
    "Para esta parte, lo que se hizo fue implementar el algoritmo de *Value iteration* e implementarlo en una función para poder hacer pruebas de forma sencilla.\n",
    "\n",
    "El algoritmo tiene la siguiente estructura:\n",
    "$$V^{*}(s)=\\max_{\\pi} \\mathbb{E}\\left[ \\sum_{t=0}^{H} \\gamma^{t} R(s_t,a_t,s_{t+1}) | \\pi, s_0=s \\right]$$\n",
    "\n",
    "El objetivo de *value iteration* es tratar de encontrar la mejor política calculando el valor de cada una de las posibles acciones en el estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9756a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(es, gamma):    \n",
    "    for state in es.states:       \n",
    "        q = dict.fromkeys(es.get_allowed_actions(state), 0.0) \n",
    "        \n",
    "        # TO DO: Actualice los valores q para Value iteration\n",
    "        if state in es.snakes:\n",
    "            es.policy[state]=''\n",
    "            es.state_values[state] =es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            es.policy[state]=''\n",
    "            es.state_values[state] = es.state_values[es.stairs[state]]\n",
    "        else:\n",
    "            for target_action in q: #Acción deseada\n",
    "                for action in es.real_actions[target_action]: #Acciones resultantes para una acción deseada\n",
    "\n",
    "                    new_state, reward, _, done = es.step(state, action, random=False) #Prueba cada acción\n",
    "                    prob = es.action_probabilities[es.real_actions[target_action].index(action)] #Probabilidad de la acción resultante\n",
    "\n",
    "                    if(done):\n",
    "                        q[target_action] += prob*reward\n",
    "\n",
    "                    else:\n",
    "                        q[target_action] += prob*(reward + gamma*es.state_values[new_state])\n",
    "\n",
    "\n",
    "            es.policy[state], es.state_values[state] =  es.key_max(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84c5e5-c35e-45f5-9ce4-29e696e7cb84",
   "metadata": {},
   "source": [
    "Se procede a inicializar los valores en 0 de la función de valor para hacer las pruebas respectivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c40f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.state_values = es.init_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda17f5c-4c23-47a5-bba1-ad824ab3d8dd",
   "metadata": {},
   "source": [
    "Ahora se utilizará la función anteriormente creada con un horizonte de 15 y un $\\gamma$ de 0.9: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f3dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=15\n",
    "gamma=0.9\n",
    "es.value_iteration = MethodType(value_iteration, es)\n",
    "es.solve_value_iteration(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c1e2d-2b25-4a62-b25d-b515fb8b9a04",
   "metadata": {},
   "source": [
    "Cabe resaltar que lo que se hizo es no colocar ningun valor en los estados donde hay un inicio de escalera o donde se encuentra una cabeza de serpiente. El resultado de la política luego de utilizar el algoritmo de *Value iteration* fue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3420b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Ad', 2: 'Ad', 3: 'Ad', 4: 'Ad', 5: 'Ad', 6: 'Ad', 7: 'Ad', 8: '', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'Ad', 16: 'Ad', 17: 'Ad', 18: 'Ad', 19: 'Ad', 20: 'Ad', 21: '', 22: 'At', 23: 'Ad', 24: 'At', 25: 'At', 26: 'At', 27: 'At', 28: 'At', 29: 'At', 30: 'At', 31: 'At', 32: 'At', 33: 'At', 34: 'At', 35: 'Ad', 36: 'Ad', 37: 'Ad', 38: 'Ad', 39: 'Ad', 40: 'Ad', 41: 'Ad', 42: 'Ad', 43: '', 44: '', 45: 'Ad', 46: '', 47: 'Ad', 48: '', 49: 'Ad', 50: '', 51: 'Ad', 52: '', 53: 'Ad', 54: '', 55: '', 56: 'Ad', 57: 'Ad', 58: 'Ad', 59: '', 60: 'Ad', 61: 'Ad', 62: '', 63: 'At', 64: '', 65: 'Ad', 66: '', 67: 'Ad', 68: 'Ad', 69: '', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: '', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'Ad', 78: 'Ad', 79: 'Ad', 80: '', 81: 'At', 82: 'At', 83: '', 84: 'At', 85: 'At', 86: 'At', 87: 'At', 88: 'At', 89: 'Ad', 90: 'At', 91: 'At', 92: '', 93: 'Ad', 94: 'Ad', 95: '', 96: 'Ad', 97: 'Ad', 98: '', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "print(es.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e915d6-b9d6-4877-bfbc-5326a47bebf5",
   "metadata": {},
   "source": [
    "Aquí ya es posible ver el comportamiento del agente.\n",
    "\n",
    "Por ejemplo, se tiene que la casilla **23** es un estado terminal con recompensa negativa, pero en la casilla **21** se encuetra una escalera que llega a un punto muy favorable para terminar el episodio con recompensa positiva. Por ese motivo, desde la casilla **35** se ve que la mejor acción que puede tomar el agente es *Atrás*. Porque a pesar de que hay una casilla terminal negativa cerca, la escalera en la casilla **21** tiene un mejor resultado.\n",
    "\n",
    "También se puede ver que todas las casillas antes de la casilla **21** tienen como acción óptima *Adelante*, se puede asumir que es por lo mismo explicado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426eb0de-1690-4c22-845f-1bf563f4b466",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prueba Value iteration con Estados terminales aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b3ea8-461a-44d8-9606-5d03555da517",
   "metadata": {},
   "source": [
    "A continuación se realizará un procedimiento parecido al numeral anterior, solo que ahora se colocarán puntos terminales de forma aleatoria y se observará el comportamiento del agente en esos casos. Se debe tener en cuenta que los estados terminales deben cumplir con lo siguiente:\n",
    "* Se van a tener dos casillas azules (recompensa positiva) elegidas **aleatoriamente** en las primeras dos filas del tablero\n",
    "* Se van a tener siete casillas rojas (recompensa negativa) elegidas **aleatoriamente** en las primeras nueve filas del tablero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3be364b-9490-4c6c-8ead-deed0c3e2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de casillas azules:  [10  6] ; valores de casillas rojas:  [80 34 46 83 49 58 37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generar dos valores aleatorios diferentes entre 1 y 20, valores dentro de las dos primeras filas del tablero\n",
    "casillas_azules = np.random.choice(np.arange(1, 21), size=2, replace=False)\n",
    "\n",
    "# Generar siete valores aleatorios diferentes entre 1 y 90, valores dentro de las nueve primeras filas del tablero\n",
    "casillas_rojas = np.random.choice(np.arange(1, 91), size=7, replace=False)\n",
    "\n",
    "print(\"Valores de casillas azules: \", casillas_azules, \"; valores de casillas rojas: \", casillas_rojas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670205a-df5f-44b7-ab4c-b7288b2251c0",
   "metadata": {},
   "source": [
    "Creación del nuevo tablero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bc78b78-5b0b-4ac1-b862-ea1a0babed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EscalerasSerpientes(casillas_azules, casillas_rojas,-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280aab5-3669-4b73-8fa2-42618c1adcd1",
   "metadata": {},
   "source": [
    "Realizar la prueba de iteración de valor con el nuevo tablero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4910d3f-bcb2-4ffa-a7da-2910b2d26e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=15\n",
    "gamma=0.9\n",
    "es.value_iteration = MethodType(value_iteration, es)\n",
    "es.solve_value_iteration(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df24dd9e-fa5e-4db4-8ec2-777b60e952bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Ad', 2: 'At', 3: 'Ad', 4: 'Ad', 5: 'Ad', 6: 'Ad', 7: 'At', 8: '', 9: 'Ad', 10: 'Ad', 11: 'At', 12: 'At', 13: 'At', 14: 'At', 15: 'At', 16: 'At', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: '', 22: 'At', 23: 'At', 24: 'At', 25: 'At', 26: 'At', 27: 'At', 28: 'At', 29: 'At', 30: 'At', 31: 'At', 32: 'At', 33: 'At', 34: 'Ad', 35: 'Ad', 36: 'Ad', 37: 'Ad', 38: 'Ad', 39: 'Ad', 40: 'Ad', 41: 'Ad', 42: 'Ad', 43: '', 44: '', 45: 'At', 46: '', 47: 'At', 48: '', 49: 'Ad', 50: '', 51: 'Ad', 52: '', 53: 'At', 54: '', 55: '', 56: 'At', 57: 'At', 58: 'Ad', 59: '', 60: 'At', 61: 'At', 62: '', 63: 'Ad', 64: '', 65: 'At', 66: '', 67: 'Ad', 68: 'Ad', 69: '', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: '', 74: 'At', 75: 'At', 76: 'At', 77: 'At', 78: 'At', 79: 'At', 80: '', 81: 'Ad', 82: 'Ad', 83: '', 84: 'At', 85: 'At', 86: 'At', 87: 'At', 88: 'At', 89: 'At', 90: 'Ad', 91: 'Ad', 92: '', 93: 'Ad', 94: 'Ad', 95: '', 96: 'At', 97: 'At', 98: '', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "print(es.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d8276-b285-408a-a287-f47630d72e68",
   "metadata": {},
   "source": [
    "Con el ejemplo (azules casillas 10 y 6, rojas casillas 80, 34, 46, 83, 49, 58 y 37) Se puede ver que si el agente  se encuentra en casillas anteriores a las de recompensa azul, tienden a ir hacia adelante, cuando ya están en valores más altos de 20, la acción predominante es *Atrás*, ya que quieren acercarse a los puntos azules.\n",
    "\n",
    "Una curiosidad que se puede ver, es que el agente tiende a buscar las cabezas de las serpientes cuando ya está en valores muy alejados de la recompensa. Por ejemplo, la casilla **73** es la cabeza de una serpiente que lleva hasta la casilla **1**. Cuando el agente está por debajo de esa casilla, elije la acción *Adelante* (casilla **70**). Cuando el agente está por encima de esa casilla, el agente elije la acción *Atrás* (casilla **77**).\n",
    "\n",
    "Esto da a entender que el agente quiere llegar lo más rápido a los puntos de casilas azules, esto buscando las cabezas de las serpientes que lo lleven a las primeras dos filas del tablero.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
