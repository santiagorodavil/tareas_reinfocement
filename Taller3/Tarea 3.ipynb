{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8fcea4-7982-43e1-b52c-a98d5574c7b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tarea 3 Reinforcement Learning\n",
    "### (Montecarlo, SARSA, Q-Learning)\n",
    "Juan Pablo Reyes Fajardo\n",
    "\n",
    "Santiago Rodríguez Ávila\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da0d5-cdc1-4825-90c0-6ad7a3e2c003",
   "metadata": {},
   "source": [
    "Para esta tarea se quiere estudiar el comportamiento de los algoritmos de aprendizaje por refuerzo de: Montecarlo, SARSA y Q-learning. Se observarán características, comportamiento y composición de estos. Se hablará de políticas on y off (que el agente sigue la misma política a evaluar o una diferente), todo esto partiendo del escenario de la tarea anterior, el juego de escaleras y serpientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2308a3-7d67-4fac-a21c-c81bce01f8cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Planteamiento del MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbbf2f-641a-406d-ba1c-67609a91a3f2",
   "metadata": {},
   "source": [
    "En esta parte, se pedía hacer las respectivas modificaciones con el fin de que el módulo de python creado tuviera lo siguiente:\n",
    "* Dada una acción en un estado, retornar la recompensa y el estado resultante en esa acción. Considere el caso especial del estado terminal.\n",
    "    \n",
    "* Que sea posible ejecutar una política arbitraria.\n",
    "\n",
    "Para esto, se creará un escenario y se utilizará para hacer las pruebas respectivas que se pedían anteriormente. Se procede a crear el escenario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f87e32-e6dc-47f7-a488-b4d0e32683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from escalerasyserpientes import EscalerasSerpientes\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ec77cf-fbea-471d-9f39-0b3eb1463381",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EscalerasSerpientes([80,100],[23,37,45,67,89],-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b5567-809b-415a-837b-52842c8eed36",
   "metadata": {},
   "source": [
    "Luego de crearlo, se mostrará la forma en que el MDP retorna la información luego de realizar un paso, el primer caso será para mostrar la forma de retorno, el segundo va a ser teniendo en cuenta algún estado terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a882c9f5-d2a8-4346-965e-c82462b185c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, -0.01, 5, False)\n"
     ]
    }
   ],
   "source": [
    "# Muestra de como funciona el MDP\n",
    "state_prueba=16\n",
    "action_prueba=5\n",
    "print(es.step(state_prueba, action_prueba, random=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3954126-d5ba-42ec-85fb-792bf933f9d3",
   "metadata": {},
   "source": [
    "La estructura de retorno del MDP es la siguiente:\n",
    "Casilla de llegada, valor de la recompensa obtenida en el paso, acción tomada en ese estado (valor del dado), variable que muestra si el estado al que se llegó es un estado terminal. En ese orden de ideas, la prueba teniendo en cuenta algún estado terminal sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b8008a-e7fa-4f4a-bccd-775c3c5a51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Azul', 1.0, None, True)\n"
     ]
    }
   ],
   "source": [
    "# Muestra de como funciona el MDP en estado terminal\n",
    "state_prueba=100\n",
    "action_prueba=-4\n",
    "print(es.step(state_prueba, action_prueba, random=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f399a77-5c12-4f88-b2a7-77fe360cb860",
   "metadata": {},
   "source": [
    "Aquí se puede ver que cuando el agente llega a un estado terminal e intenta avanzar, el escenario no le permite saltar hacia ningún lado, en este caso, se recibe \"Azul\"(estado terminal bueno), la recompensa del estado terminal y la variable que informa que se llegó a un estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d45b0-3e0e-436e-b0cf-39a2ae74aa1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Algotimo Montecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe92caa-08a9-4a08-973f-92c26b00ba00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algoritmo SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1889a-e463-42bb-afb4-450d6fcf7036",
   "metadata": {},
   "source": [
    "En esta parte de la tarea se va a analizar y desarrollar el algoritmo SARSA. Como se pudo ver en clase, este es un algoritmo *on-policy*, esto quiere decir que el algoritmo aprende directamente de la política actual del agente. Otra cosa que vale la pena nombrar es que este algoritmo utiliza la **función Q** para actualizar el valor del par estado, acción a evaluar. La norma de actualización de SARSA es la siguiente:\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A)+\\alpha*(R+\\gamma*Q(S',A') - Q(S,A))$$\n",
    "\n",
    "Donde $S'$ es el estado siguiente al que se llega luego de elegir $A$ de forma $\\epsilon-greedy$ , y $A'$ es la acción $\\epsilon-greedy$ elegida desde el estado $A'$. Finalmente, se repite esta regla de actualización actualizando los valores de forma: $S\\leftarrow S'$ y $A\\leftarrow A'$ hasta que se llegue a un estado terminal y ahí se termina el episodio.\n",
    "\n",
    "A continuación, se presenta la implementación del algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c308966-a519-441d-a87a-b5739771bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA algorithm\n",
    "\n",
    "# Variables: alpha, gamma y epsilon.\n",
    "alpha = 0.3\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#Para 10000 episodios de entrenamiento\n",
    "for i in range(10000):\n",
    "    # Inicializa las variables para cada episodio\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    # Seleccion accion \"a\" de forma epsilon-greedy\n",
    "    if epsilon< np.random.uniform():        \n",
    "        act_arg = np.array([es.q_values[(state, act)] for act in es.allowed_actions[state]])\n",
    "        action = es.allowed_actions[state][np.argmax(act_arg)]\n",
    "    else:\n",
    "        actions = es.allowed_actions[state]\n",
    "        action = actions[np.random.randint(0,2)]\n",
    "    \n",
    "    # Inicia el episodio\n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "        \n",
    "        # Obtengo s'\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        # Revisa que new_state no sea un estado terminal\n",
    "        if type(new_state) is str:\n",
    "            # Valor q(s',a') terminal\n",
    "            q_value_next_step = 0\n",
    "        else:\n",
    "            #Obtengo a' de s' con epsilon greedy\n",
    "            if np.random.uniform()< epsilon:     \n",
    "                actions = es.allowed_actions[new_state]\n",
    "                new_action = actions[np.random.randint(0,2)]\n",
    "            else:\n",
    "                act_arg = np.array([es.q_values[(new_state, act)] for act in es.allowed_actions[state]])\n",
    "                new_action = es.allowed_actions[new_state][np.argmax(act_arg)]\n",
    "\n",
    "            # Valor q(s',a') no terminal\n",
    "            q_value_next_step = es.q_values[(new_state,new_action)]\n",
    "        \n",
    "        \n",
    "        # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*q(s',a') - q(s,a))\n",
    "        es.q_values[(state, action)] += alpha*(reward + gamma*q_value_next_step - es.q_values[(state,action)])\n",
    "        \n",
    "        # asigna a = a' y s = s'\n",
    "        state = new_state\n",
    "        action = new_action\n",
    "      \n",
    "\n",
    "        # Parte que termina el episodio si se llega a algun estado terminal\n",
    "        if done:\n",
    "            continue_episode = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131b04f-6967-4ea5-835f-b343da25e3f0",
   "metadata": {},
   "source": [
    "Luego de correr la implementación del algoritmo SARSA, la política que se va a utilizar va a ser el valor máximo de la función Q de cada par estado acción por estado. Por ejemplo, la política en el estado 1 va a ser el valor máximo de la **función Q** entre *q(1,\"adelante\")* y *q(1,\"atras\")*. \n",
    "\n",
    "Teniendo esto en cuenta, la política del algoritmo SARSA es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0df1d8f-17ae-447a-aa77-ca9a3f1793d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica obtenida Algoritmo SARSA:  {1: 'Ad', 2: 'At', 3: 'Ad', 4: 'Ad', 5: 'Ad', 6: 'Ad', 7: 'Ad', 8: 'Ad', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'Ad', 16: 'Ad', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: 'Ad', 22: 'At', 23: 'Ad', 24: 'Ad', 25: 'Ad', 26: 'At', 27: 'Ad', 28: 'Ad', 29: 'Ad', 30: 'Ad', 31: 'At', 32: 'At', 33: 'At', 34: 'At', 35: 'Ad', 36: 'Ad', 37: 'Ad', 38: 'Ad', 39: 'Ad', 40: 'Ad', 41: 'Ad', 42: 'Ad', 43: 'Ad', 44: 'Ad', 45: 'Ad', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'Ad', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'Ad', 57: 'At', 58: 'At', 59: 'Ad', 60: 'At', 61: 'At', 62: 'Ad', 63: 'At', 64: 'Ad', 65: 'At', 66: 'Ad', 67: 'At', 68: 'Ad', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: 'Ad', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'At', 78: 'Ad', 79: 'Ad', 80: 'Ad', 81: 'At', 82: 'At', 83: 'Ad', 84: 'At', 85: 'At', 86: 'At', 87: 'At', 88: 'At', 89: 'Ad', 90: 'Ad', 91: 'Ad', 92: 'Ad', 93: 'Ad', 94: 'Ad', 95: 'Ad', 96: 'At', 97: 'At', 98: 'Ad', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "SARSA_policy = {}\n",
    "for i in es.states:\n",
    "    max_val = np.argmax([es.q_values[(i,act)] for act in es.allowed_actions[i]])\n",
    "    SARSA_policy[i] = es.allowed_actions[i][max_val]\n",
    "print(\"Politica obtenida Algoritmo SARSA: \",SARSA_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb9de-d9b0-4331-99a3-10e2c1fe641c",
   "metadata": {},
   "source": [
    "Luego de obtener la política, se procede a hacer una prueba de esta con un total de 1000 episodios. La idea es obtener el promedio de pasos obtenido por cada episodio y el porcentaje de victoria (donde el estado terminal sea \"Azul\") de los 1000 episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed77570-350d-4562-ade4-0cd30b0ff6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de politica en SARSA\n",
    "num_episodios = 1000\n",
    "win_per = 0\n",
    "prom_step = 0\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    states = []\n",
    "    \n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "            \n",
    "        # Toma la accion de la politica\n",
    "        action = SARSA_policy[state]\n",
    "        # Da un paso en direccion de la politica\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        state = new_state\n",
    "        states.append(state)\n",
    "        num_steps+=1\n",
    "        \n",
    "        if done or num_steps >500:\n",
    "            continue_episode = False\n",
    "            if state == 'Azul':\n",
    "                win_per+=1\n",
    "\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01952e-54ca-46eb-8dea-382502049c02",
   "metadata": {},
   "source": [
    "El resultado de la prueba de los episodios es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3863396f-190e-4b19-9621-7f5373d0f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio pasos por episodio:  40.33399999999996\n",
      "Porcentaje victorias:  93.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Promedio pasos por episodio: \",prom_step)\n",
    "print(\"Porcentaje victorias: \", win_per*100/num_episodios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d08696-9da6-4ee7-9d12-a5ceb0fb4478",
   "metadata": {},
   "source": [
    "Si el porcentaje de victorias no aceptable, se puede reiniciar y volver a correr todas las celdas del cuaderno. De esta forma es posible obtener mejores resultados.\n",
    "\n",
    "Se hizo una prueba de correr 100 veces el cuaderno y observar las recompensas obtenidas. De las 100 veces, se obtuvo un resultado de 80% de resultados favorables, eso quiere decir que probabilísitcamente se puede obtener un buen resultado si se vuelve a correr todo el cuaderno. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac4434-724c-4115-92d2-91cadaa223d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d127b5-10d1-4338-988e-f7844b588a56",
   "metadata": {},
   "source": [
    "En esta parte de la tarea se va a analizar y desarrollar el algoritmo *Q-Learning*. Como se pudo ver en clase, este es un algoritmo *off-policy*, esto quiere decir que el algoritmo aprende de una función de valor que puede usarse para derivar la política óptima en un entorno dado, es decir, no aprende directamente de la política que se está evaluando. Otra cosa que vale la pena nombrar es que este algoritmo también utiliza la **función Q** para actualizar el valor del par estado, acción a evaluar. La norma de actualización de *Q-learning* es la siguiente:\n",
    "\n",
    "$$Q(S,A) \\leftarrow Q(S,A)+\\alpha*(R+\\gamma*\\max_{a}Q(S',a) - Q(S,A))$$\n",
    "\n",
    "\n",
    "Donde $S'$ es el estado siguiente al que se llega luego de elegir $A$ de forma $\\epsilon-greedy$. Luego, se busca la acción con mayor valor del par estado acción $q(S',a)$, donde $a$ puede ser *\"Adelante\"* o *\"Atras\"*.\n",
    "\n",
    "Finalmente, se repite esta regla de actualización actualizando los valores de forma: $S\\leftarrow S'$ hasta que se llegue a un estado terminal y ahí se termina el episodio.\n",
    "\n",
    "A continuación, se presenta la implementación del algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0273a5-bb04-4703-9fb8-9d31cfcc698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning algorithm\n",
    "\n",
    "# Variables: alpha , gamma y epsilon.\n",
    "alpha = 0.25\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#Para 1000 episodio\n",
    "for i in range(10000):\n",
    "    # Inicializa las variables para cada episodio\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    \n",
    "    # Inicia el episodio\n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "\n",
    "        # Tomar acción de forma epsilon-greedy\n",
    "        if np.random.uniform()<epsilon:\n",
    "            # paso aleatorio\n",
    "            A = np.random.randint(0,2)\n",
    "            actions = es.allowed_actions[1]\n",
    "            action = actions[A]\n",
    "\n",
    "        else:\n",
    "            # paso con accion greedy\n",
    "            act_arg = np.array([es.q_values[(state, act)] for act in es.allowed_actions[state]])\n",
    "            action = es.allowed_actions[state][np.argmax(act_arg)]\n",
    "        \n",
    "        # Obtengo s'\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "\n",
    "        # Valor de max q(s',a). Si es terminal el estado, el valor es 0\n",
    "        if type(new_state) is str:\n",
    "            # Valor max q(s',a) terminal\n",
    "            max_q_val = 0\n",
    "        else:\n",
    "            # Valor max q(s',a) no terminal\n",
    "            action_arg = np.array([es.q_values[(new_state, act)] for act in es.allowed_actions[state]])\n",
    "            new_action = es.allowed_actions[new_state][np.argmax(action_arg)]\n",
    "            max_q_val = es.q_values[(new_state,new_action)]\n",
    "        \n",
    "        # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*max q(s',a) - q(s,a))\n",
    "        es.q_values[(state, action)] += alpha*(reward + gamma*max_q_val - es.q_values[(state,action)])\n",
    "        \n",
    "        # asigna s = s'\n",
    "        state = new_state\n",
    "     \n",
    "\n",
    "        # Parte que termina el episodio si se llega a algun estado terminal\n",
    "        if done:\n",
    "            continue_episode = False\n",
    "\n",
    "                \n",
    "    # Promedio de pasos por episodio\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7298b6-359b-4d5c-ae7a-d298c55f294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica obtenida Algoritmo Q-learning:  {1: 'At', 2: 'At', 3: 'At', 4: 'Ad', 5: 'Ad', 6: 'At', 7: 'Ad', 8: 'Ad', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'Ad', 16: 'Ad', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: 'Ad', 22: 'At', 23: 'Ad', 24: 'Ad', 25: 'At', 26: 'Ad', 27: 'Ad', 28: 'Ad', 29: 'Ad', 30: 'Ad', 31: 'At', 32: 'At', 33: 'At', 34: 'At', 35: 'At', 36: 'At', 37: 'Ad', 38: 'Ad', 39: 'Ad', 40: 'At', 41: 'At', 42: 'Ad', 43: 'Ad', 44: 'Ad', 45: 'Ad', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'Ad', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'At', 57: 'At', 58: 'At', 59: 'Ad', 60: 'Ad', 61: 'At', 62: 'Ad', 63: 'At', 64: 'Ad', 65: 'At', 66: 'Ad', 67: 'Ad', 68: 'Ad', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: 'Ad', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'Ad', 78: 'Ad', 79: 'At', 80: 'Ad', 81: 'At', 82: 'At', 83: 'Ad', 84: 'At', 85: 'At', 86: 'At', 87: 'At', 88: 'At', 89: 'At', 90: 'Ad', 91: 'Ad', 92: 'Ad', 93: 'Ad', 94: 'Ad', 95: 'Ad', 96: 'At', 97: 'At', 98: 'Ad', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "Qlearning_policy = {}\n",
    "for i in es.states:\n",
    "    #print(i)\n",
    "    max_val = np.argmax([es.q_values[(i,act)] for act in es.allowed_actions[i]])\n",
    "    Qlearning_policy[i] = es.allowed_actions[i][max_val]\n",
    "print(\"Politica obtenida Algoritmo Q-learning: \",Qlearning_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caccab3-ff49-4c9d-80bc-6a570fa9458b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99fa169-dd53-4988-9fd2-c980e4d055f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de polica Qlearning\n",
    "num_episodios = 1000\n",
    "win_per = 0\n",
    "prom_step = 0\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    states = []\n",
    "    \n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "            \n",
    "        # Toma la accion de la politica\n",
    "        action = Qlearning_policy[state]\n",
    "        # Da un paso en direccion de la politica\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        state = new_state\n",
    "        states.append(state)\n",
    "        num_steps+=1\n",
    "        #print(action, state)\n",
    "        \n",
    "        if done or num_steps >500:\n",
    "            continue_episode = False\n",
    "            if state == 'Azul':\n",
    "                win_per+=1\n",
    "    #print(states)\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61abd953-8099-4ef8-baf4-595e229ba189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio pasos por episodio: 54.15599999999998\n",
      "Porcentaje victorias: 95.3\n"
     ]
    }
   ],
   "source": [
    "print('Promedio pasos por episodio:',prom_step)\n",
    "print('Porcentaje victorias:' ,win_per*100/num_episodios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
