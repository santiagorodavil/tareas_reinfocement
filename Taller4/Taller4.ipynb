{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Reinforcement Learning: Taller 4**\n",
    "## Estudiantes: Juan Pablo Reyes Fajardo y Santiago Rodríguez Ávila "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install tqdm\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion Auxiliar para diccionarios\n",
    "def key_max(d):\n",
    "        return max(d.items(), key=operator.itemgetter(1))\n",
    "def key_min(d):\n",
    "        return min(d.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RL Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Política $\\epsilon$ - Greedy \n",
    "\n",
    "(Útil más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q_, state,epsilon=0.1):\n",
    "    rand = np.random.uniform()\n",
    "    if rand>epsilon:\n",
    "        return key_max(Q_[state])[0],1-epsilon\n",
    "    else:\n",
    "        return random.choice(list(Q_[state])),epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretización\n",
    "\n",
    "Inicialmente se realizan múltiples experimentos para determinar límites razonables para las variables a discretizar (aquellas cuyo espacio de observación es infinito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de velocidad lineal absoluta máxima:       3.1324585240114797       \n",
      "Promedio de velocidad angular absoluta máxima:       3.435788314235238\n"
     ]
    }
   ],
   "source": [
    "velocidades_absolutas_maximas={\"Lineal\":[],\"Angular\":[]}\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "abs_lineal=abs(observation[1])\n",
    "abs_angular=abs(observation[3])\n",
    "\n",
    "for _ in range(int(1e6)):\n",
    "    \n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated = env.step(action)\n",
    "    \n",
    "    if abs(observation[1])>abs_lineal:\n",
    "        abs_lineal=abs(observation[1])\n",
    "    if abs(observation[3])>abs_angular:\n",
    "        abs_angular=abs(observation[3])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation= env.reset()\n",
    "        velocidades_absolutas_maximas[\"Lineal\"].append(abs_lineal)\n",
    "        velocidades_absolutas_maximas[\"Angular\"].append(abs_angular)\n",
    "\n",
    "env.close()\n",
    "\n",
    "vel_lin_abs_max=np.mean(velocidades_absolutas_maximas[\"Lineal\"])\n",
    "vel_ang_abs_max=np.mean(velocidades_absolutas_maximas[\"Angular\"])\n",
    "print(f'Promedio de velocidad lineal absoluta máxima: \\\n",
    "      {vel_lin_abs_max} \\\n",
    "      \\nPromedio de velocidad angular absoluta máxima: \\\n",
    "      {vel_ang_abs_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretización de estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Límites del espacio de observación del MDP real\n",
    "cart_high_var = env.observation_space.high\n",
    "cart_low_var = env.observation_space.low\n",
    "\n",
    "# Espacios de observación discretizados\n",
    "observation_space_discrete_400=[np.linspace(cart_low_var[0], cart_high_var[0], num= 5),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 4),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 5),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 4)]\\\n",
    "\n",
    "observation_space_discrete_4096=[np.linspace(cart_low_var[0], cart_high_var[0], num= 8),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 8),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 8),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 8)]\n",
    "\n",
    "# Uso de iteradores para obtener todos los estados a partir del espacio de obsevación discreto\n",
    "states_400=list(itertools.product(*observation_space_discrete_400))\n",
    "states_4096=list(itertools.product(*observation_space_discrete_4096))\n",
    "\n",
    "def init_Q_400():\n",
    "    Q_table={}\n",
    "    for i in states_400:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "\n",
    "def init_Q_4096():\n",
    "    Q_table={}\n",
    "    for i in states_4096:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "         \n",
    "def discretize_400(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_400[i]]\n",
    "        discretizacion[i] = observation_space_discrete_400[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)\n",
    "\n",
    "def discretize_4096(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_4096[i]]\n",
    "        discretizacion[i] = observation_space_discrete_4096[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación de Q con Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 Estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_400(gamma,alpha,epsilon):\n",
    "    terminated=False\n",
    "    observation = env.reset()\n",
    "    observation=discretize_400(observation)\n",
    "    G=0\n",
    "    while True:\n",
    "        #env.render(mode = \"human\")\n",
    "        action,_=eps_greedy(Q,observation,0.1)\n",
    "        \n",
    "        observation_, reward, terminated, truncated = env.step(action)\n",
    "        observation_=discretize_400(observation_)\n",
    "        \n",
    "        G+=reward\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_\n",
    "        if terminated:\n",
    "            return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4c6086d92bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mGs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_Learning_400\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Alpha: {alpha} Retorno: {np.mean(Gs)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alpha' is not defined"
     ]
    }
   ],
   "source": [
    "Q=init_Q_400()\n",
    "policy=dict.fromkeys(states_400, 0)\n",
    "Gs=[]\n",
    "epsilons=np.linspace(0.9,0.1,int(1e4))\n",
    "\n",
    "for i in tqdm(range(len(epsilons))):\n",
    "    Gs.append(Q_Learning_400(0.9,alpha,epsilons[i]))\n",
    "print(f'Alpha: {alpha} Retorno: {np.mean(Gs)}')\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcdb9803828>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(Gs,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_4096(gamma,alpha):\n",
    "    terminated=False\n",
    "    observation = env.reset()\n",
    "    observation=discretize_4096(observation)\n",
    "    G=0\n",
    "    while True:\n",
    "        \n",
    "        action,_=eps_greedy(Q,observation,0.25)\n",
    "        \n",
    "        observation_, reward, terminated, truncated = env.step(action)\n",
    "        observation_=discretize_4096(observation_)\n",
    "        \n",
    "        G+=reward\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_\n",
    "        if terminated:\n",
    "            return G\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7693933/100000000 [4:22:39<52:31:14, 488.20it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-d4877b2fb266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mGs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_Learning_4096\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Retorno: {np.mean(Gs)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-ab54da8ede83>\u001b[0m in \u001b[0;36mQ_Learning_4096\u001b[0;34m(gamma, alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscretize_4096\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q=init_Q_4096()\n",
    "policy=dict.fromkeys(states_4096, 0)\n",
    "epsilons=np.linspace(0.9,0.01,int(1e8))\n",
    "Gs=[]\n",
    "\n",
    "for i in tqdm(range(len(epsilons))):\n",
    "    Gs.append(Q_Learning_4096(0.95,0.1))\n",
    "print(f' Retorno: {np.mean(Gs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc3887209e8>]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_mean(Gs,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state_4096 in states_4096:\n",
    "    policy[state_4096],_=eps_greedy(Q,tuple(state_4096),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c741931dc779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "rwds=[]\n",
    "for _ in range(100):\n",
    "    observation = env.reset()\n",
    "    r=0\n",
    "    while True:\n",
    "        action = policy[discretize_4096(observation)]\n",
    "        env.render(mode = \"human\")\n",
    "        observation, reward, terminated, truncated = env.step(action)\n",
    "        r+=reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation = env.reset()\n",
    "            rwds.append(r)\n",
    "            break\n",
    "env.close()\n",
    "print(np.mean(rwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100 steps ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44036\\1937588419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mvisualkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayered_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         inputs, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1304\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m         )\n\u001b[0;32m   1306\u001b[0m         \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m         )\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2694\u001b[0m                 \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2696\u001b[1;33m                 \u001b[0mexception_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2697\u001b[0m             )\n\u001b[0;32m   2698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    737\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[1;34m\" but got array with shape \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                         )\n\u001b[0;32m    741\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(2, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=100, visualize=True, verbose=2)\n",
    "\n",
    "visualkeras.layered_view(model)\n",
    "\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36332\\2881083884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python --version'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NumPy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "!python --version\n",
    "print('NumPy', np.__version__)\n",
    "print('Tensorflow', tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "Episode 1 reward: 10.0\n",
      "    10/100000: episode: 1, duration: 1.360s, episode steps: 10, steps per second: 7, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.478, 1.530], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 reward: 9.0\n",
      "    19/100000: episode: 2, duration: 1.201s, episode steps: 9, steps per second: 7, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.888, 1.809], loss: 0.500943, mean_absolute_error: 0.531230, mean_q: -0.009534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 reward: 10.0\n",
      "    29/100000: episode: 3, duration: 0.740s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.081, 2.001], loss: 0.484710, mean_absolute_error: 0.518994, mean_q: 0.002447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 reward: 8.0\n",
      "    37/100000: episode: 4, duration: 0.609s, episode steps: 8, steps per second: 13, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.557, 1.609], loss: 0.471954, mean_absolute_error: 0.508764, mean_q: 0.014994\n",
      "Episode 5 reward: 11.0\n",
      "    48/100000: episode: 5, duration: 0.764s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.818, 1.745], loss: 0.461572, mean_absolute_error: 0.496645, mean_q: 0.029505\n",
      "Episode 6 reward: 9.0\n",
      "    57/100000: episode: 6, duration: 0.625s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.824, 1.790], loss: 0.451451, mean_absolute_error: 0.487885, mean_q: 0.044462\n",
      "Episode 7 reward: 11.0\n",
      "    68/100000: episode: 7, duration: 0.684s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.937, 1.931], loss: 0.443427, mean_absolute_error: 0.485186, mean_q: 0.059065\n",
      "Episode 8 reward: 10.0\n",
      "    78/100000: episode: 8, duration: 0.621s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.018, 1.956], loss: 0.437108, mean_absolute_error: 0.487466, mean_q: 0.072393\n",
      "Episode 9 reward: 9.0\n",
      "    87/100000: episode: 9, duration: 0.630s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.770, 1.718], loss: 0.429070, mean_absolute_error: 0.488376, mean_q: 0.087381\n",
      "Episode 10 reward: 11.0\n",
      "    98/100000: episode: 10, duration: 0.705s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.115 [-3.319, 2.195], loss: 0.427192, mean_absolute_error: 0.492117, mean_q: 0.096903\n",
      "Episode 11 reward: 10.0\n",
      "   108/100000: episode: 11, duration: 0.685s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.544, 1.561], loss: 0.421652, mean_absolute_error: 0.493441, mean_q: 0.109587\n",
      "Episode 12 reward: 12.0\n",
      "   120/100000: episode: 12, duration: 0.748s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.108 [-3.021, 1.921], loss: 0.417218, mean_absolute_error: 0.495765, mean_q: 0.124508\n",
      "Episode 13 reward: 10.0\n",
      "   130/100000: episode: 13, duration: 0.618s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-3.002, 1.958], loss: 0.410841, mean_absolute_error: 0.497299, mean_q: 0.140686\n",
      "Episode 14 reward: 11.0\n",
      "   141/100000: episode: 14, duration: 0.689s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.278, 1.403], loss: 0.401005, mean_absolute_error: 0.496253, mean_q: 0.161307\n",
      "Episode 15 reward: 11.0\n",
      "   152/100000: episode: 15, duration: 0.675s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.100 [-2.772, 1.789], loss: 0.394897, mean_absolute_error: 0.499120, mean_q: 0.181747\n",
      "Episode 16 reward: 10.0\n",
      "   162/100000: episode: 16, duration: 0.667s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.957, 1.939], loss: 0.372223, mean_absolute_error: 0.491936, mean_q: 0.217029\n",
      "Episode 17 reward: 9.0\n",
      "   171/100000: episode: 17, duration: 0.615s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.759, 1.766], loss: 0.322464, mean_absolute_error: 0.470510, mean_q: 0.284302\n",
      "Episode 18 reward: 11.0\n",
      "   182/100000: episode: 18, duration: 0.698s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.128 [-2.792, 1.788], loss: 0.247324, mean_absolute_error: 0.432212, mean_q: 0.397496\n",
      "Episode 19 reward: 10.0\n",
      "   192/100000: episode: 19, duration: 0.667s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-3.098, 1.928], loss: 0.199720, mean_absolute_error: 0.409343, mean_q: 0.498499\n",
      "Episode 20 reward: 9.0\n",
      "   201/100000: episode: 20, duration: 0.570s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.253, 1.378], loss: 0.152883, mean_absolute_error: 0.377998, mean_q: 0.592056\n",
      "Episode 21 reward: 10.0\n",
      "   211/100000: episode: 21, duration: 0.631s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.052, 1.904], loss: 0.122772, mean_absolute_error: 0.356121, mean_q: 0.672606\n",
      "Episode 22 reward: 9.0\n",
      "   220/100000: episode: 22, duration: 0.568s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.823, 1.762], loss: 0.097268, mean_absolute_error: 0.331969, mean_q: 0.764036\n",
      "Episode 23 reward: 8.0\n",
      "   228/100000: episode: 23, duration: 0.501s, episode steps: 8, steps per second: 16, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-2.511, 1.577], loss: 0.116895, mean_absolute_error: 0.373030, mean_q: 0.795868\n",
      "Episode 24 reward: 9.0\n",
      "   237/100000: episode: 24, duration: 0.673s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.330, 1.377], loss: 0.109919, mean_absolute_error: 0.372793, mean_q: 0.844541\n",
      "Episode 25 reward: 10.0\n",
      "   247/100000: episode: 25, duration: 0.774s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-3.084, 1.940], loss: 0.109944, mean_absolute_error: 0.381818, mean_q: 0.884558\n",
      "Episode 26 reward: 11.0\n",
      "   258/100000: episode: 26, duration: 0.958s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.130 [-2.770, 1.722], loss: 0.109507, mean_absolute_error: 0.395859, mean_q: 0.929614\n",
      "Episode 27 reward: 9.0\n",
      "   267/100000: episode: 27, duration: 0.721s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.811, 1.745], loss: 0.117392, mean_absolute_error: 0.423559, mean_q: 1.000819\n",
      "Episode 28 reward: 10.0\n",
      "   277/100000: episode: 28, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.333, 1.518], loss: 0.131708, mean_absolute_error: 0.453344, mean_q: 1.061353\n",
      "Episode 29 reward: 9.0\n",
      "   286/100000: episode: 29, duration: 0.621s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.834, 1.797], loss: 0.147747, mean_absolute_error: 0.488539, mean_q: 1.091428\n",
      "Episode 30 reward: 9.0\n",
      "   295/100000: episode: 30, duration: 0.741s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.814, 1.798], loss: 0.154362, mean_absolute_error: 0.505701, mean_q: 1.137323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 reward: 10.0\n",
      "   305/100000: episode: 31, duration: 0.980s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-3.038, 1.996], loss: 0.162517, mean_absolute_error: 0.527149, mean_q: 1.164331\n",
      "Episode 32 reward: 10.0\n",
      "   315/100000: episode: 32, duration: 1.078s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.160 [-2.650, 1.533], loss: 0.166583, mean_absolute_error: 0.544501, mean_q: 1.246124\n",
      "Episode 33 reward: 11.0\n",
      "   326/100000: episode: 33, duration: 1.213s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.123 [-2.760, 1.770], loss: 0.173514, mean_absolute_error: 0.571950, mean_q: 1.319345\n",
      "Episode 34 reward: 12.0\n",
      "   338/100000: episode: 34, duration: 1.241s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.099 [-2.539, 1.579], loss: 0.186113, mean_absolute_error: 0.599769, mean_q: 1.372250\n",
      "Episode 35 reward: 13.0\n",
      "   351/100000: episode: 35, duration: 1.123s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.827, 1.778], loss: 0.214345, mean_absolute_error: 0.644225, mean_q: 1.419525\n",
      "Episode 36 reward: 9.0\n",
      "   360/100000: episode: 36, duration: 0.863s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.241, 1.348], loss: 0.250234, mean_absolute_error: 0.685252, mean_q: 1.459480\n",
      "Episode 37 reward: 9.0\n",
      "   369/100000: episode: 37, duration: 0.817s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.180 [-2.859, 1.727], loss: 0.227662, mean_absolute_error: 0.687269, mean_q: 1.520616\n",
      "Episode 38 reward: 9.0\n",
      "   378/100000: episode: 38, duration: 0.713s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.892, 1.789], loss: 0.246005, mean_absolute_error: 0.721133, mean_q: 1.593421\n",
      "Episode 39 reward: 12.0\n",
      "   390/100000: episode: 39, duration: 1.319s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.120 [-3.073, 1.926], loss: 0.281905, mean_absolute_error: 0.756376, mean_q: 1.625177\n",
      "Episode 40 reward: 9.0\n",
      "   399/100000: episode: 40, duration: 0.901s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.126 [-2.805, 1.785], loss: 0.289030, mean_absolute_error: 0.790421, mean_q: 1.700309\n",
      "Episode 41 reward: 9.0\n",
      "   408/100000: episode: 41, duration: 0.866s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.805, 1.775], loss: 0.308565, mean_absolute_error: 0.820554, mean_q: 1.761635\n",
      "Episode 42 reward: 11.0\n",
      "   419/100000: episode: 42, duration: 0.807s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.157 [-2.904, 1.741], loss: 0.288402, mean_absolute_error: 0.822816, mean_q: 1.774463\n",
      "Episode 43 reward: 8.0\n",
      "   427/100000: episode: 43, duration: 0.668s, episode steps: 8, steps per second: 12, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.550, 1.540], loss: 0.296080, mean_absolute_error: 0.845955, mean_q: 1.901841\n",
      "Episode 44 reward: 9.0\n",
      "   436/100000: episode: 44, duration: 0.670s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.780, 1.722], loss: 0.327292, mean_absolute_error: 0.872898, mean_q: 1.895266\n",
      "Episode 45 reward: 10.0\n",
      "   446/100000: episode: 45, duration: 0.846s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.014, 1.904], loss: 0.344726, mean_absolute_error: 0.894662, mean_q: 1.967222\n",
      "Episode 46 reward: 9.0\n",
      "   455/100000: episode: 46, duration: 0.685s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.176 [-2.878, 1.751], loss: 0.361270, mean_absolute_error: 0.931962, mean_q: 2.022079\n",
      "Episode 47 reward: 9.0\n",
      "   464/100000: episode: 47, duration: 0.725s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.769, 1.797], loss: 0.352159, mean_absolute_error: 0.931303, mean_q: 2.054256\n",
      "Episode 48 reward: 10.0\n",
      "   474/100000: episode: 48, duration: 0.808s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.072, 1.987], loss: 0.378453, mean_absolute_error: 0.960965, mean_q: 2.086779\n",
      "Episode 49 reward: 8.0\n",
      "   482/100000: episode: 49, duration: 0.551s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.123 [-2.206, 1.398], loss: 0.447576, mean_absolute_error: 1.011427, mean_q: 2.198242\n",
      "Episode 50 reward: 9.0\n",
      "   491/100000: episode: 50, duration: 0.663s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.120 [-2.218, 1.383], loss: 0.386472, mean_absolute_error: 0.987202, mean_q: 2.171663\n",
      "Episode 51 reward: 10.0\n",
      "   501/100000: episode: 51, duration: 0.658s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.017, 1.966], loss: 0.450051, mean_absolute_error: 1.036175, mean_q: 2.216800\n",
      "Episode 52 reward: 8.0\n",
      "   509/100000: episode: 52, duration: 0.535s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.558, 1.601], loss: 0.492038, mean_absolute_error: 1.086703, mean_q: 2.332356\n",
      "Episode 53 reward: 9.0\n",
      "   518/100000: episode: 53, duration: 0.598s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.908, 1.802], loss: 0.517607, mean_absolute_error: 1.120362, mean_q: 2.337172\n",
      "Episode 54 reward: 9.0\n",
      "   527/100000: episode: 54, duration: 0.652s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.177 [-2.900, 1.749], loss: 0.532420, mean_absolute_error: 1.121221, mean_q: 2.358789\n",
      "Episode 55 reward: 9.0\n",
      "   536/100000: episode: 55, duration: 0.665s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.801, 1.791], loss: 0.472215, mean_absolute_error: 1.111242, mean_q: 2.356088\n",
      "Episode 56 reward: 9.0\n",
      "   545/100000: episode: 56, duration: 0.641s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.343, 1.411], loss: 0.479845, mean_absolute_error: 1.130018, mean_q: 2.442487\n",
      "Episode 57 reward: 10.0\n",
      "   555/100000: episode: 57, duration: 0.707s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.429, 1.526], loss: 0.614219, mean_absolute_error: 1.189488, mean_q: 2.493322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58 reward: 10.0\n",
      "   565/100000: episode: 58, duration: 0.828s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.957, 1.913], loss: 0.572590, mean_absolute_error: 1.220687, mean_q: 2.569561\n",
      "Episode 59 reward: 9.0\n",
      "   574/100000: episode: 59, duration: 0.663s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.137 [-2.298, 1.393], loss: 0.558294, mean_absolute_error: 1.218393, mean_q: 2.572239\n",
      "Episode 60 reward: 9.0\n",
      "   583/100000: episode: 60, duration: 0.703s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.172 [-2.844, 1.743], loss: 0.541396, mean_absolute_error: 1.197361, mean_q: 2.556074\n",
      "Episode 61 reward: 10.0\n",
      "   593/100000: episode: 61, duration: 0.755s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.085, 1.986], loss: 0.668874, mean_absolute_error: 1.285789, mean_q: 2.710037\n",
      "Episode 62 reward: 12.0\n",
      "   605/100000: episode: 62, duration: 1.006s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.095 [-2.678, 1.788], loss: 0.626540, mean_absolute_error: 1.292016, mean_q: 2.694822\n",
      "Episode 63 reward: 10.0\n",
      "   615/100000: episode: 63, duration: 0.862s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.636, 1.608], loss: 0.705765, mean_absolute_error: 1.341468, mean_q: 2.741151\n",
      "Episode 64 reward: 11.0\n",
      "   626/100000: episode: 64, duration: 0.912s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.133 [-2.805, 1.783], loss: 0.669294, mean_absolute_error: 1.348150, mean_q: 2.779839\n",
      "Episode 65 reward: 10.0\n",
      "   636/100000: episode: 65, duration: 0.695s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.151 [-2.537, 1.556], loss: 0.664583, mean_absolute_error: 1.360390, mean_q: 2.811599\n",
      "Episode 66 reward: 12.0\n",
      "   648/100000: episode: 66, duration: 0.979s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.102 [-2.952, 1.931], loss: 0.678376, mean_absolute_error: 1.386897, mean_q: 2.879088\n",
      "Episode 67 reward: 9.0\n",
      "   657/100000: episode: 67, duration: 0.606s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.880, 1.732], loss: 0.806088, mean_absolute_error: 1.447556, mean_q: 2.957178\n",
      "Episode 68 reward: 10.0\n",
      "   667/100000: episode: 68, duration: 0.722s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.135 [-2.543, 1.543], loss: 0.726577, mean_absolute_error: 1.441833, mean_q: 2.945878\n",
      "Episode 69 reward: 11.0\n",
      "   678/100000: episode: 69, duration: 0.837s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.138 [-2.779, 1.734], loss: 0.747805, mean_absolute_error: 1.476163, mean_q: 3.014958\n",
      "Episode 70 reward: 9.0\n",
      "   687/100000: episode: 70, duration: 0.692s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.832, 1.805], loss: 0.702001, mean_absolute_error: 1.477105, mean_q: 3.073727\n",
      "Episode 71 reward: 10.0\n",
      "   697/100000: episode: 71, duration: 0.744s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.047, 1.908], loss: 0.729057, mean_absolute_error: 1.496883, mean_q: 3.127406\n",
      "Episode 72 reward: 13.0\n",
      "   710/100000: episode: 72, duration: 0.971s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.119 [-2.827, 1.756], loss: 0.809939, mean_absolute_error: 1.536429, mean_q: 3.175960\n",
      "Episode 73 reward: 9.0\n",
      "   719/100000: episode: 73, duration: 0.629s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.879, 1.739], loss: 0.699908, mean_absolute_error: 1.515369, mean_q: 3.186546\n",
      "Episode 74 reward: 10.0\n",
      "   729/100000: episode: 74, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.057, 1.912], loss: 0.789881, mean_absolute_error: 1.577375, mean_q: 3.296852\n",
      "Episode 75 reward: 9.0\n",
      "   738/100000: episode: 75, duration: 0.671s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.812, 1.746], loss: 0.836375, mean_absolute_error: 1.592278, mean_q: 3.307348\n",
      "Episode 76 reward: 10.0\n",
      "   748/100000: episode: 76, duration: 0.648s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.527, 1.527], loss: 0.795944, mean_absolute_error: 1.580607, mean_q: 3.327515\n",
      "Episode 77 reward: 10.0\n",
      "   758/100000: episode: 77, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.052, 1.999], loss: 0.894970, mean_absolute_error: 1.630369, mean_q: 3.396198\n",
      "Episode 78 reward: 10.0\n",
      "   768/100000: episode: 78, duration: 0.669s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.045, 1.982], loss: 0.722996, mean_absolute_error: 1.594384, mean_q: 3.362590\n",
      "Episode 79 reward: 10.0\n",
      "   778/100000: episode: 79, duration: 0.639s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.105, 1.974], loss: 0.829665, mean_absolute_error: 1.634604, mean_q: 3.446388\n",
      "Episode 80 reward: 9.0\n",
      "   787/100000: episode: 80, duration: 0.507s, episode steps: 9, steps per second: 18, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.133 [-2.259, 1.352], loss: 1.052175, mean_absolute_error: 1.735646, mean_q: 3.517252\n",
      "Episode 81 reward: 11.0\n",
      "   798/100000: episode: 81, duration: 0.629s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.445, 1.516], loss: 0.821463, mean_absolute_error: 1.672645, mean_q: 3.502242\n",
      "Episode 82 reward: 10.0\n",
      "   808/100000: episode: 82, duration: 0.632s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-2.971, 1.942], loss: 1.016886, mean_absolute_error: 1.757953, mean_q: 3.625790\n",
      "Episode 83 reward: 9.0\n",
      "   817/100000: episode: 83, duration: 0.625s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.826, 1.803], loss: 0.916292, mean_absolute_error: 1.742107, mean_q: 3.619028\n",
      "Episode 84 reward: 10.0\n",
      "   827/100000: episode: 84, duration: 0.611s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.035, 1.928], loss: 0.928057, mean_absolute_error: 1.742866, mean_q: 3.643595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85 reward: 9.0\n",
      "   836/100000: episode: 85, duration: 0.606s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.815, 1.778], loss: 1.038852, mean_absolute_error: 1.808471, mean_q: 3.666715\n",
      "Episode 86 reward: 8.0\n",
      "   844/100000: episode: 86, duration: 0.529s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.551, 1.612], loss: 1.015519, mean_absolute_error: 1.788114, mean_q: 3.714601\n",
      "Episode 87 reward: 9.0\n",
      "   853/100000: episode: 87, duration: 0.596s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.212, 1.323], loss: 1.238331, mean_absolute_error: 1.870138, mean_q: 3.746100\n",
      "Episode 88 reward: 10.0\n",
      "   863/100000: episode: 88, duration: 0.619s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.070, 1.918], loss: 1.052145, mean_absolute_error: 1.835165, mean_q: 3.733829\n",
      "Episode 89 reward: 11.0\n",
      "   874/100000: episode: 89, duration: 0.734s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.459, 1.600], loss: 1.203310, mean_absolute_error: 1.891613, mean_q: 3.766586\n",
      "Episode 90 reward: 10.0\n",
      "   884/100000: episode: 90, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.997, 1.968], loss: 0.821028, mean_absolute_error: 1.796278, mean_q: 3.719004\n",
      "Episode 91 reward: 10.0\n",
      "   894/100000: episode: 91, duration: 0.730s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.112, 1.943], loss: 0.940096, mean_absolute_error: 1.825210, mean_q: 3.795369\n",
      "Episode 92 reward: 8.0\n",
      "   902/100000: episode: 92, duration: 0.562s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.514, 1.578], loss: 0.913178, mean_absolute_error: 1.851253, mean_q: 3.879891\n",
      "Episode 93 reward: 9.0\n",
      "   911/100000: episode: 93, duration: 0.616s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.444, 1.562], loss: 0.809219, mean_absolute_error: 1.850117, mean_q: 3.932910\n",
      "Episode 94 reward: 9.0\n",
      "   920/100000: episode: 94, duration: 0.566s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.796, 1.803], loss: 1.133110, mean_absolute_error: 1.975043, mean_q: 4.037186\n",
      "Episode 95 reward: 8.0\n",
      "   928/100000: episode: 95, duration: 0.582s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.583, 1.612], loss: 1.093653, mean_absolute_error: 1.931670, mean_q: 4.062773\n",
      "Episode 96 reward: 9.0\n",
      "   937/100000: episode: 96, duration: 0.636s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.816, 1.762], loss: 1.286369, mean_absolute_error: 2.015640, mean_q: 4.098502\n",
      "Episode 97 reward: 10.0\n",
      "   947/100000: episode: 97, duration: 0.693s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.095, 1.992], loss: 0.991500, mean_absolute_error: 1.942562, mean_q: 3.998603\n",
      "Episode 98 reward: 10.0\n",
      "   957/100000: episode: 98, duration: 0.723s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.109 [-2.956, 1.954], loss: 0.975874, mean_absolute_error: 1.956527, mean_q: 4.053260\n",
      "Episode 99 reward: 9.0\n",
      "   966/100000: episode: 99, duration: 0.750s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.176 [-2.907, 1.741], loss: 0.933424, mean_absolute_error: 1.928691, mean_q: 4.076519\n",
      "Episode 100 reward: 11.0\n",
      "   977/100000: episode: 100, duration: 0.728s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.141 [-2.826, 1.725], loss: 0.961261, mean_absolute_error: 1.988900, mean_q: 4.144976\n",
      "Episode 101 reward: 12.0\n",
      "   989/100000: episode: 101, duration: 0.793s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.131 [-3.097, 1.919], loss: 0.938976, mean_absolute_error: 1.991112, mean_q: 4.190708\n",
      "Episode 102 reward: 11.0\n",
      "  1000/100000: episode: 102, duration: 0.771s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.138 [-2.838, 1.795], loss: 0.959739, mean_absolute_error: 2.022629, mean_q: 4.290367\n",
      "Episode 103 reward: 17.0\n",
      "  1017/100000: episode: 103, duration: 1.436s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.824 [0.000, 1.000], mean observation: -0.072 [-3.220, 2.116], loss: 0.916752, mean_absolute_error: 2.035038, mean_q: 4.351649\n",
      "Episode 104 reward: 10.0\n",
      "  1027/100000: episode: 104, duration: 0.804s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-3.073, 1.918], loss: 0.971007, mean_absolute_error: 2.044755, mean_q: 4.391558\n",
      "Episode 105 reward: 12.0\n",
      "  1039/100000: episode: 105, duration: 0.865s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.121 [-2.516, 1.538], loss: 0.945063, mean_absolute_error: 2.040293, mean_q: 4.437886\n",
      "Episode 106 reward: 9.0\n",
      "  1048/100000: episode: 106, duration: 0.554s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.184 [-2.896, 1.745], loss: 1.092789, mean_absolute_error: 2.108877, mean_q: 4.451818\n",
      "Episode 107 reward: 14.0\n",
      "  1062/100000: episode: 107, duration: 0.873s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.118 [-3.035, 1.918], loss: 0.936760, mean_absolute_error: 2.108910, mean_q: 4.505950\n",
      "Episode 108 reward: 10.0\n",
      "  1072/100000: episode: 108, duration: 0.600s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.052, 1.912], loss: 1.034047, mean_absolute_error: 2.134938, mean_q: 4.589990\n",
      "Episode 109 reward: 12.0\n",
      "  1084/100000: episode: 109, duration: 0.787s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.124 [-3.094, 1.969], loss: 1.319374, mean_absolute_error: 2.193498, mean_q: 4.576766\n",
      "Episode 110 reward: 9.0\n",
      "  1093/100000: episode: 110, duration: 0.639s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.811, 1.802], loss: 0.968383, mean_absolute_error: 2.147423, mean_q: 4.533653\n",
      "Episode 111 reward: 9.0\n",
      "  1102/100000: episode: 111, duration: 0.693s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.813, 1.806], loss: 1.136940, mean_absolute_error: 2.217265, mean_q: 4.581460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 112 reward: 10.0\n",
      "  1112/100000: episode: 112, duration: 0.699s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.120 [-2.625, 1.617], loss: 1.095783, mean_absolute_error: 2.204419, mean_q: 4.581924\n",
      "Episode 113 reward: 10.0\n",
      "  1122/100000: episode: 113, duration: 0.635s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.055, 1.999], loss: 1.010240, mean_absolute_error: 2.185998, mean_q: 4.621438\n",
      "Episode 114 reward: 8.0\n",
      "  1130/100000: episode: 114, duration: 0.543s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.572, 1.563], loss: 1.100965, mean_absolute_error: 2.225883, mean_q: 4.637769\n",
      "Episode 115 reward: 9.0\n",
      "  1139/100000: episode: 115, duration: 0.529s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.857, 1.756], loss: 1.137169, mean_absolute_error: 2.250197, mean_q: 4.668421\n",
      "Episode 116 reward: 10.0\n",
      "  1149/100000: episode: 116, duration: 0.693s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.045, 1.914], loss: 1.176889, mean_absolute_error: 2.280255, mean_q: 4.682691\n",
      "Episode 117 reward: 13.0\n",
      "  1162/100000: episode: 117, duration: 0.845s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.118 [-2.897, 1.795], loss: 1.102929, mean_absolute_error: 2.272965, mean_q: 4.691845\n",
      "Episode 118 reward: 10.0\n",
      "  1172/100000: episode: 118, duration: 0.639s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.024, 1.955], loss: 1.144244, mean_absolute_error: 2.321811, mean_q: 4.741970\n",
      "Episode 119 reward: 9.0\n",
      "  1181/100000: episode: 119, duration: 0.684s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.838, 1.741], loss: 1.152228, mean_absolute_error: 2.307524, mean_q: 4.767499\n",
      "Episode 120 reward: 12.0\n",
      "  1193/100000: episode: 120, duration: 0.876s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.114 [-3.034, 1.972], loss: 0.906195, mean_absolute_error: 2.271031, mean_q: 4.795909\n",
      "Episode 121 reward: 9.0\n",
      "  1202/100000: episode: 121, duration: 0.767s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.899, 1.774], loss: 1.060357, mean_absolute_error: 2.320746, mean_q: 4.862136\n",
      "Episode 122 reward: 9.0\n",
      "  1211/100000: episode: 122, duration: 0.620s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.854, 1.769], loss: 1.044441, mean_absolute_error: 2.323606, mean_q: 4.899831\n",
      "Episode 123 reward: 11.0\n",
      "  1222/100000: episode: 123, duration: 0.652s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.138 [-2.873, 1.786], loss: 1.188842, mean_absolute_error: 2.376101, mean_q: 4.908852\n",
      "Episode 124 reward: 10.0\n",
      "  1232/100000: episode: 124, duration: 0.587s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.021, 1.945], loss: 1.001310, mean_absolute_error: 2.354555, mean_q: 4.901286\n",
      "Episode 125 reward: 11.0\n",
      "  1243/100000: episode: 125, duration: 0.685s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.130 [-2.182, 1.327], loss: 1.109748, mean_absolute_error: 2.401526, mean_q: 4.910534\n",
      "Episode 126 reward: 8.0\n",
      "  1251/100000: episode: 126, duration: 0.580s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.152 [-2.229, 1.324], loss: 0.825005, mean_absolute_error: 2.352880, mean_q: 4.941724\n",
      "Episode 127 reward: 8.0\n",
      "  1259/100000: episode: 127, duration: 0.558s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.512, 1.560], loss: 1.125430, mean_absolute_error: 2.429158, mean_q: 4.974197\n",
      "Episode 128 reward: 9.0\n",
      "  1268/100000: episode: 128, duration: 0.585s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.845, 1.718], loss: 0.857475, mean_absolute_error: 2.374073, mean_q: 5.002922\n",
      "Episode 129 reward: 8.0\n",
      "  1276/100000: episode: 129, duration: 0.736s, episode steps: 8, steps per second: 11, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.545, 1.587], loss: 0.916587, mean_absolute_error: 2.395364, mean_q: 5.046657\n",
      "Episode 130 reward: 10.0\n",
      "  1286/100000: episode: 130, duration: 0.670s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.527, 1.571], loss: 0.980033, mean_absolute_error: 2.451840, mean_q: 5.064139\n",
      "Episode 131 reward: 9.0\n",
      "  1295/100000: episode: 131, duration: 0.552s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.852, 1.809], loss: 0.957544, mean_absolute_error: 2.438456, mean_q: 5.092467\n",
      "Episode 132 reward: 8.0\n",
      "  1303/100000: episode: 132, duration: 0.548s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.529, 1.596], loss: 1.143439, mean_absolute_error: 2.481292, mean_q: 5.066268\n",
      "Episode 133 reward: 10.0\n",
      "  1313/100000: episode: 133, duration: 0.712s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.144 [-2.568, 1.564], loss: 1.124865, mean_absolute_error: 2.497689, mean_q: 5.059049\n",
      "Episode 134 reward: 10.0\n",
      "  1323/100000: episode: 134, duration: 0.799s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.054, 1.918], loss: 0.977825, mean_absolute_error: 2.491855, mean_q: 5.001859\n",
      "Episode 135 reward: 10.0\n",
      "  1333/100000: episode: 135, duration: 0.713s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.091, 1.923], loss: 0.870790, mean_absolute_error: 2.477505, mean_q: 5.076496\n",
      "Episode 136 reward: 9.0\n",
      "  1342/100000: episode: 136, duration: 0.603s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.875, 1.787], loss: 0.952810, mean_absolute_error: 2.500363, mean_q: 5.110723\n",
      "Episode 137 reward: 10.0\n",
      "  1352/100000: episode: 137, duration: 0.630s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.113 [-2.530, 1.572], loss: 0.746679, mean_absolute_error: 2.459004, mean_q: 5.173589\n",
      "Episode 138 reward: 10.0\n",
      "  1362/100000: episode: 138, duration: 1.203s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.016, 1.943], loss: 1.009520, mean_absolute_error: 2.524683, mean_q: 5.212717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 139 reward: 9.0\n",
      "  1371/100000: episode: 139, duration: 0.626s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.833, 1.777], loss: 0.752943, mean_absolute_error: 2.468673, mean_q: 5.235169\n",
      "Episode 140 reward: 9.0\n",
      "  1380/100000: episode: 140, duration: 0.616s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.796, 1.731], loss: 0.850005, mean_absolute_error: 2.516801, mean_q: 5.245233\n",
      "Episode 141 reward: 12.0\n",
      "  1392/100000: episode: 141, duration: 0.946s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.105 [-2.972, 1.908], loss: 0.829213, mean_absolute_error: 2.515098, mean_q: 5.247439\n",
      "Episode 142 reward: 9.0\n",
      "  1401/100000: episode: 142, duration: 0.934s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.877, 1.757], loss: 0.833697, mean_absolute_error: 2.532343, mean_q: 5.297575\n",
      "Episode 143 reward: 9.0\n",
      "  1410/100000: episode: 143, duration: 0.813s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.780, 1.746], loss: 0.902003, mean_absolute_error: 2.587229, mean_q: 5.245493\n",
      "Episode 144 reward: 11.0\n",
      "  1421/100000: episode: 144, duration: 0.939s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.777, 1.740], loss: 0.629007, mean_absolute_error: 2.501888, mean_q: 5.325832\n",
      "Episode 145 reward: 9.0\n",
      "  1430/100000: episode: 145, duration: 0.667s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.786, 1.794], loss: 0.935959, mean_absolute_error: 2.589433, mean_q: 5.331030\n",
      "Episode 146 reward: 8.0\n",
      "  1438/100000: episode: 146, duration: 0.685s, episode steps: 8, steps per second: 12, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.564, 1.552], loss: 0.923134, mean_absolute_error: 2.597179, mean_q: 5.304755\n",
      "Episode 147 reward: 11.0\n",
      "  1449/100000: episode: 147, duration: 1.003s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.142 [-2.829, 1.755], loss: 0.792692, mean_absolute_error: 2.599819, mean_q: 5.340947\n",
      "Episode 148 reward: 12.0\n",
      "  1461/100000: episode: 148, duration: 0.927s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.104 [-2.999, 1.976], loss: 0.867378, mean_absolute_error: 2.636861, mean_q: 5.344297\n",
      "Episode 149 reward: 10.0\n",
      "  1471/100000: episode: 149, duration: 0.844s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.720, 1.797], loss: 0.834798, mean_absolute_error: 2.644696, mean_q: 5.360837\n",
      "Episode 150 reward: 10.0\n",
      "  1481/100000: episode: 150, duration: 0.806s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.728, 1.799], loss: 0.689462, mean_absolute_error: 2.635137, mean_q: 5.396052\n",
      "Episode 151 reward: 10.0\n",
      "  1491/100000: episode: 151, duration: 0.807s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.560, 1.572], loss: 0.773296, mean_absolute_error: 2.650318, mean_q: 5.379637\n",
      "Episode 152 reward: 9.0\n",
      "  1500/100000: episode: 152, duration: 0.710s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.120 [-2.199, 1.415], loss: 0.807831, mean_absolute_error: 2.671223, mean_q: 5.423139\n",
      "Episode 153 reward: 9.0\n",
      "  1509/100000: episode: 153, duration: 0.733s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.117 [-2.440, 1.592], loss: 0.699347, mean_absolute_error: 2.639709, mean_q: 5.394248\n",
      "Episode 154 reward: 11.0\n",
      "  1520/100000: episode: 154, duration: 0.756s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.105 [-2.401, 1.607], loss: 0.584970, mean_absolute_error: 2.615832, mean_q: 5.415295\n",
      "Episode 155 reward: 8.0\n",
      "  1528/100000: episode: 155, duration: 0.585s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.592, 1.603], loss: 0.794983, mean_absolute_error: 2.673943, mean_q: 5.329318\n",
      "Episode 156 reward: 10.0\n",
      "  1538/100000: episode: 156, duration: 0.670s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.700, 1.715], loss: 0.834473, mean_absolute_error: 2.701575, mean_q: 5.369681\n",
      "Episode 157 reward: 9.0\n",
      "  1547/100000: episode: 157, duration: 0.612s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.517, 1.565], loss: 0.626092, mean_absolute_error: 2.678363, mean_q: 5.424198\n",
      "Episode 158 reward: 10.0\n",
      "  1557/100000: episode: 158, duration: 0.865s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.154 [-2.583, 1.547], loss: 0.599029, mean_absolute_error: 2.666843, mean_q: 5.478180\n",
      "Episode 159 reward: 9.0\n",
      "  1566/100000: episode: 159, duration: 0.740s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.144 [-2.413, 1.544], loss: 0.558739, mean_absolute_error: 2.640197, mean_q: 5.502630\n",
      "Episode 160 reward: 12.0\n",
      "  1578/100000: episode: 160, duration: 0.992s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.354, 1.560], loss: 0.721443, mean_absolute_error: 2.718554, mean_q: 5.513906\n",
      "Episode 161 reward: 10.0\n",
      "  1588/100000: episode: 161, duration: 0.702s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.008, 1.177], loss: 0.489573, mean_absolute_error: 2.666682, mean_q: 5.572516\n",
      "Episode 162 reward: 9.0\n",
      "  1597/100000: episode: 162, duration: 0.653s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.509, 1.549], loss: 0.609555, mean_absolute_error: 2.715993, mean_q: 5.624923\n",
      "Episode 163 reward: 10.0\n",
      "  1607/100000: episode: 163, duration: 0.682s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.117 [-2.394, 1.592], loss: 0.650641, mean_absolute_error: 2.725605, mean_q: 5.549624\n",
      "Episode 164 reward: 10.0\n",
      "  1617/100000: episode: 164, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.365, 1.541], loss: 0.635421, mean_absolute_error: 2.728324, mean_q: 5.565563\n",
      "Episode 165 reward: 8.0\n",
      "  1625/100000: episode: 165, duration: 0.549s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.144 [-2.254, 1.390], loss: 0.610181, mean_absolute_error: 2.696788, mean_q: 5.562695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 166 reward: 10.0\n",
      "  1635/100000: episode: 166, duration: 0.782s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.728, 1.763], loss: 0.537272, mean_absolute_error: 2.742883, mean_q: 5.700036\n",
      "Episode 167 reward: 9.0\n",
      "  1644/100000: episode: 167, duration: 0.749s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.291, 1.367], loss: 0.478651, mean_absolute_error: 2.714902, mean_q: 5.720697\n",
      "Episode 168 reward: 9.0\n",
      "  1653/100000: episode: 168, duration: 0.665s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.151 [-2.297, 1.339], loss: 0.410346, mean_absolute_error: 2.686745, mean_q: 5.706908\n",
      "Episode 169 reward: 10.0\n",
      "  1663/100000: episode: 169, duration: 0.737s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.107 [-2.344, 1.584], loss: 0.582970, mean_absolute_error: 2.745415, mean_q: 5.730424\n",
      "Episode 170 reward: 8.0\n",
      "  1671/100000: episode: 170, duration: 0.545s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-2.522, 1.525], loss: 0.482236, mean_absolute_error: 2.711543, mean_q: 5.887379\n",
      "Episode 171 reward: 10.0\n",
      "  1681/100000: episode: 171, duration: 0.684s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.751, 1.771], loss: 0.488097, mean_absolute_error: 2.749732, mean_q: 5.870777\n",
      "Episode 172 reward: 13.0\n",
      "  1694/100000: episode: 172, duration: 0.880s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.093 [-2.218, 1.371], loss: 0.573190, mean_absolute_error: 2.734193, mean_q: 5.664609\n",
      "Episode 173 reward: 9.0\n",
      "  1703/100000: episode: 173, duration: 0.616s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.121 [-2.204, 1.378], loss: 0.549031, mean_absolute_error: 2.762037, mean_q: 5.802445\n",
      "Episode 174 reward: 9.0\n",
      "  1712/100000: episode: 174, duration: 0.707s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.263, 1.378], loss: 0.464174, mean_absolute_error: 2.737012, mean_q: 5.819181\n",
      "Episode 175 reward: 12.0\n",
      "  1724/100000: episode: 175, duration: 0.844s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.115 [-2.129, 1.337], loss: 0.577068, mean_absolute_error: 2.797671, mean_q: 5.840692\n",
      "Episode 176 reward: 11.0\n",
      "  1735/100000: episode: 176, duration: 0.975s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-1.803, 1.001], loss: 0.578271, mean_absolute_error: 2.784243, mean_q: 5.816834\n",
      "Episode 177 reward: 10.0\n",
      "  1745/100000: episode: 177, duration: 2.208s, episode steps: 10, steps per second: 5, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.184, 1.391], loss: 0.511583, mean_absolute_error: 2.795680, mean_q: 5.915937\n",
      "Episode 178 reward: 12.0\n",
      "  1757/100000: episode: 178, duration: 1.121s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.113 [-2.160, 1.224], loss: 0.533375, mean_absolute_error: 2.796039, mean_q: 5.949112\n",
      "Episode 179 reward: 9.0\n",
      "  1766/100000: episode: 179, duration: 0.554s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.137 [-2.269, 1.377], loss: 0.543442, mean_absolute_error: 2.820075, mean_q: 5.982906\n",
      "Episode 180 reward: 11.0\n",
      "  1777/100000: episode: 180, duration: 0.689s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.095 [-2.282, 1.594], loss: 0.496532, mean_absolute_error: 2.821589, mean_q: 6.045520\n",
      "Episode 181 reward: 9.0\n",
      "  1786/100000: episode: 181, duration: 0.508s, episode steps: 9, steps per second: 18, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.248, 1.350], loss: 0.502286, mean_absolute_error: 2.814260, mean_q: 6.042624\n",
      "Episode 182 reward: 12.0\n",
      "  1798/100000: episode: 182, duration: 0.669s, episode steps: 12, steps per second: 18, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-2.325, 1.518], loss: 0.507487, mean_absolute_error: 2.810821, mean_q: 5.953596\n",
      "Episode 183 reward: 10.0\n",
      "  1808/100000: episode: 183, duration: 0.566s, episode steps: 10, steps per second: 18, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.116 [-2.377, 1.609], loss: 0.572105, mean_absolute_error: 2.816482, mean_q: 5.919274\n",
      "Episode 184 reward: 11.0\n",
      "  1819/100000: episode: 184, duration: 0.774s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.127 [-2.421, 1.524], loss: 0.501978, mean_absolute_error: 2.835022, mean_q: 6.004393\n",
      "Episode 185 reward: 8.0\n",
      "  1827/100000: episode: 185, duration: 0.599s, episode steps: 8, steps per second: 13, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.556, 1.555], loss: 0.414069, mean_absolute_error: 2.842369, mean_q: 6.201606\n",
      "Episode 186 reward: 9.0\n",
      "  1836/100000: episode: 186, duration: 0.601s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.118 [-2.133, 1.407], loss: 0.512895, mean_absolute_error: 2.856309, mean_q: 6.054320\n",
      "Episode 187 reward: 10.0\n",
      "  1846/100000: episode: 187, duration: 0.711s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.138 [-2.201, 1.344], loss: 0.483855, mean_absolute_error: 2.864660, mean_q: 6.066176\n",
      "Episode 188 reward: 10.0\n",
      "  1856/100000: episode: 188, duration: 0.769s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.167, 1.384], loss: 0.585109, mean_absolute_error: 2.886673, mean_q: 6.003827\n",
      "Episode 189 reward: 9.0\n",
      "  1865/100000: episode: 189, duration: 0.597s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.268, 1.383], loss: 0.470701, mean_absolute_error: 2.886935, mean_q: 6.101114\n",
      "Episode 190 reward: 10.0\n",
      "  1875/100000: episode: 190, duration: 0.656s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-1.968, 1.225], loss: 0.503131, mean_absolute_error: 2.880091, mean_q: 6.082998\n",
      "Episode 191 reward: 11.0\n",
      "  1886/100000: episode: 191, duration: 0.656s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.102 [-2.172, 1.395], loss: 0.526820, mean_absolute_error: 2.935669, mean_q: 6.177869\n",
      "Episode 192 reward: 10.0\n",
      "  1896/100000: episode: 192, duration: 0.622s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.166, 1.333], loss: 0.498407, mean_absolute_error: 2.920739, mean_q: 6.193845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 193 reward: 11.0\n",
      "  1907/100000: episode: 193, duration: 0.775s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.090 [-2.134, 1.411], loss: 0.396265, mean_absolute_error: 2.864617, mean_q: 6.162915\n",
      "Episode 194 reward: 9.0\n",
      "  1916/100000: episode: 194, duration: 0.579s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-1.886, 1.127], loss: 0.555488, mean_absolute_error: 2.902906, mean_q: 6.035409\n",
      "Episode 195 reward: 11.0\n",
      "  1927/100000: episode: 195, duration: 0.693s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-2.114, 1.371], loss: 0.449594, mean_absolute_error: 2.945508, mean_q: 6.261103\n",
      "Episode 196 reward: 9.0\n",
      "  1936/100000: episode: 196, duration: 0.506s, episode steps: 9, steps per second: 18, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.125 [-1.945, 1.168], loss: 0.624557, mean_absolute_error: 3.004370, mean_q: 6.233121\n",
      "Episode 197 reward: 11.0\n",
      "  1947/100000: episode: 197, duration: 0.636s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-2.053, 1.384], loss: 0.564458, mean_absolute_error: 2.964179, mean_q: 6.070262\n",
      "Episode 198 reward: 13.0\n",
      "  1960/100000: episode: 198, duration: 0.739s, episode steps: 13, steps per second: 18, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.091 [-1.918, 1.219], loss: 0.536744, mean_absolute_error: 2.985527, mean_q: 6.318192\n",
      "Episode 199 reward: 9.0\n",
      "  1969/100000: episode: 199, duration: 0.522s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.132 [-1.909, 1.177], loss: 0.649801, mean_absolute_error: 3.014438, mean_q: 6.186369\n",
      "Episode 200 reward: 11.0\n",
      "  1980/100000: episode: 200, duration: 0.622s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-2.028, 1.350], loss: 0.510017, mean_absolute_error: 2.935760, mean_q: 6.092102\n",
      "Episode 201 reward: 9.0\n",
      "  1989/100000: episode: 201, duration: 0.601s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.132 [-2.145, 1.335], loss: 0.362297, mean_absolute_error: 2.989681, mean_q: 6.458821\n",
      "Episode 202 reward: 13.0\n",
      "  2002/100000: episode: 202, duration: 0.743s, episode steps: 13, steps per second: 17, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.085 [-2.023, 1.388], loss: 0.525653, mean_absolute_error: 3.000210, mean_q: 6.325386\n",
      "Episode 203 reward: 10.0\n",
      "  2012/100000: episode: 203, duration: 0.619s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.166, 1.334], loss: 0.585633, mean_absolute_error: 3.031112, mean_q: 6.330942\n",
      "Episode 204 reward: 10.0\n",
      "  2022/100000: episode: 204, duration: 0.705s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.117 [-2.095, 1.410], loss: 0.504867, mean_absolute_error: 2.999287, mean_q: 6.257448\n",
      "Episode 205 reward: 10.0\n",
      "  2032/100000: episode: 205, duration: 0.659s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.137 [-2.073, 1.356], loss: 0.559341, mean_absolute_error: 3.043677, mean_q: 6.362489\n",
      "Episode 206 reward: 10.0\n",
      "  2042/100000: episode: 206, duration: 0.686s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.104 [-1.848, 1.223], loss: 0.452092, mean_absolute_error: 2.979715, mean_q: 6.285120\n",
      "Episode 207 reward: 9.0\n",
      "  2051/100000: episode: 207, duration: 0.582s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.137 [-1.928, 1.204], loss: 0.597504, mean_absolute_error: 3.036604, mean_q: 6.384686\n",
      "Episode 208 reward: 11.0\n",
      "  2062/100000: episode: 208, duration: 0.673s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.131 [-1.982, 1.176], loss: 0.477029, mean_absolute_error: 3.019748, mean_q: 6.493580\n",
      "Episode 209 reward: 9.0\n",
      "  2071/100000: episode: 209, duration: 0.555s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-1.953, 1.174], loss: 0.522470, mean_absolute_error: 3.030497, mean_q: 6.438375\n",
      "Episode 210 reward: 13.0\n",
      "  2084/100000: episode: 210, duration: 0.839s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.084 [-2.129, 1.397], loss: 0.527793, mean_absolute_error: 3.060680, mean_q: 6.391766\n",
      "Episode 211 reward: 10.0\n",
      "  2094/100000: episode: 211, duration: 0.671s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.151 [-2.096, 1.325], loss: 0.530093, mean_absolute_error: 3.079184, mean_q: 6.510194\n",
      "Episode 212 reward: 9.0\n",
      "  2103/100000: episode: 212, duration: 0.604s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.124 [-1.876, 1.200], loss: 0.493775, mean_absolute_error: 3.019480, mean_q: 6.309248\n",
      "Episode 213 reward: 10.0\n",
      "  2113/100000: episode: 213, duration: 0.627s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.144 [-1.913, 1.180], loss: 0.514865, mean_absolute_error: 3.072617, mean_q: 6.433977\n",
      "Episode 214 reward: 13.0\n",
      "  2126/100000: episode: 214, duration: 0.756s, episode steps: 13, steps per second: 17, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.098 [-2.134, 1.414], loss: 0.530040, mean_absolute_error: 3.116291, mean_q: 6.497016\n",
      "Episode 215 reward: 11.0\n",
      "  2137/100000: episode: 215, duration: 0.621s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.128 [-1.891, 1.174], loss: 0.460210, mean_absolute_error: 3.101938, mean_q: 6.579172\n",
      "Episode 216 reward: 11.0\n",
      "  2148/100000: episode: 216, duration: 0.707s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-2.034, 1.367], loss: 0.490216, mean_absolute_error: 3.112563, mean_q: 6.547775\n",
      "Episode 217 reward: 9.0\n",
      "  2157/100000: episode: 217, duration: 0.517s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.122 [-1.861, 1.193], loss: 0.455918, mean_absolute_error: 3.125907, mean_q: 6.618894\n",
      "Episode 218 reward: 12.0\n",
      "  2169/100000: episode: 218, duration: 0.761s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.097 [-1.970, 1.367], loss: 0.534792, mean_absolute_error: 3.115629, mean_q: 6.514771\n",
      "Episode 219 reward: 11.0\n",
      "  2180/100000: episode: 219, duration: 0.653s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.113 [-1.982, 1.196], loss: 0.479395, mean_absolute_error: 3.158077, mean_q: 6.734860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220 reward: 13.0\n",
      "  2193/100000: episode: 220, duration: 0.784s, episode steps: 13, steps per second: 17, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-2.046, 1.352], loss: 0.527908, mean_absolute_error: 3.166790, mean_q: 6.669654\n",
      "Episode 221 reward: 9.0\n",
      "  2202/100000: episode: 221, duration: 0.555s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.154 [-2.317, 1.393], loss: 0.403569, mean_absolute_error: 3.199518, mean_q: 6.882751\n",
      "Episode 222 reward: 10.0\n",
      "  2212/100000: episode: 222, duration: 0.605s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.106, 1.333], loss: 0.504262, mean_absolute_error: 3.220884, mean_q: 6.841367\n",
      "Episode 223 reward: 11.0\n",
      "  2223/100000: episode: 223, duration: 0.653s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.094 [-2.029, 1.401], loss: 0.493094, mean_absolute_error: 3.166586, mean_q: 6.753554\n",
      "Episode 224 reward: 9.0\n",
      "  2232/100000: episode: 224, duration: 0.577s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-1.950, 1.159], loss: 0.419016, mean_absolute_error: 3.179123, mean_q: 6.811261\n",
      "Episode 225 reward: 12.0\n",
      "  2244/100000: episode: 225, duration: 0.678s, episode steps: 12, steps per second: 18, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.105 [-1.928, 1.195], loss: 0.603772, mean_absolute_error: 3.251885, mean_q: 6.914327\n",
      "Episode 226 reward: 11.0\n",
      "  2255/100000: episode: 226, duration: 0.676s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.141 [-2.093, 1.374], loss: 0.581547, mean_absolute_error: 3.161893, mean_q: 6.622960\n",
      "Episode 227 reward: 12.0\n",
      "  2267/100000: episode: 227, duration: 0.675s, episode steps: 12, steps per second: 18, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.129 [-2.701, 1.723], loss: 0.583025, mean_absolute_error: 3.101658, mean_q: 6.278822\n",
      "Episode 228 reward: 9.0\n",
      "  2276/100000: episode: 228, duration: 0.550s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.124 [-1.933, 1.198], loss: 0.419590, mean_absolute_error: 3.206855, mean_q: 6.859092\n",
      "Episode 229 reward: 12.0\n",
      "  2288/100000: episode: 229, duration: 0.710s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.109 [-2.024, 1.318], loss: 0.539237, mean_absolute_error: 3.202663, mean_q: 6.729500\n",
      "Episode 230 reward: 12.0\n",
      "  2300/100000: episode: 230, duration: 0.730s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.088 [-2.157, 1.387], loss: 0.453968, mean_absolute_error: 3.222691, mean_q: 6.819855\n",
      "Episode 231 reward: 9.0\n",
      "  2309/100000: episode: 231, duration: 0.614s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.141 [-1.940, 1.147], loss: 0.432362, mean_absolute_error: 3.244589, mean_q: 6.915802\n",
      "Episode 232 reward: 11.0\n",
      "  2320/100000: episode: 232, duration: 0.683s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.103 [-2.027, 1.365], loss: 0.608099, mean_absolute_error: 3.281569, mean_q: 6.778204\n",
      "Episode 233 reward: 11.0\n",
      "  2331/100000: episode: 233, duration: 0.688s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.128 [-1.959, 1.166], loss: 0.558002, mean_absolute_error: 3.275401, mean_q: 6.825395\n",
      "Episode 234 reward: 12.0\n",
      "  2343/100000: episode: 234, duration: 0.728s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-2.088, 1.320], loss: 0.617545, mean_absolute_error: 3.302406, mean_q: 6.816141\n",
      "Episode 235 reward: 9.0\n",
      "  2352/100000: episode: 235, duration: 0.535s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-1.887, 1.180], loss: 0.563277, mean_absolute_error: 3.302283, mean_q: 6.934072\n",
      "Episode 236 reward: 11.0\n",
      "  2363/100000: episode: 236, duration: 0.647s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.102 [-2.020, 1.376], loss: 0.509655, mean_absolute_error: 3.248042, mean_q: 6.816295\n",
      "Episode 237 reward: 10.0\n",
      "  2373/100000: episode: 237, duration: 0.596s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.130 [-1.861, 1.159], loss: 0.514995, mean_absolute_error: 3.304297, mean_q: 6.937567\n",
      "Episode 238 reward: 10.0\n",
      "  2383/100000: episode: 238, duration: 0.585s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.124 [-1.872, 1.191], loss: 0.530452, mean_absolute_error: 3.340886, mean_q: 7.003139\n",
      "Episode 239 reward: 10.0\n",
      "  2393/100000: episode: 239, duration: 0.690s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.114 [-1.880, 1.202], loss: 0.456761, mean_absolute_error: 3.337344, mean_q: 6.991326\n",
      "Episode 240 reward: 9.0\n",
      "  2402/100000: episode: 240, duration: 0.542s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.869, 1.737], loss: 0.579428, mean_absolute_error: 3.374949, mean_q: 7.076135\n",
      "Episode 241 reward: 12.0\n",
      "  2414/100000: episode: 241, duration: 0.749s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.102 [-2.005, 1.331], loss: 0.494740, mean_absolute_error: 3.385389, mean_q: 7.201982\n",
      "Episode 242 reward: 8.0\n",
      "  2422/100000: episode: 242, duration: 0.446s, episode steps: 8, steps per second: 18, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.146 [-2.029, 1.224], loss: 0.743194, mean_absolute_error: 3.433134, mean_q: 7.095177\n",
      "Episode 243 reward: 12.0\n",
      "  2434/100000: episode: 243, duration: 0.687s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-1.968, 1.349], loss: 0.637854, mean_absolute_error: 3.313258, mean_q: 6.795776\n",
      "Episode 244 reward: 11.0\n",
      "  2445/100000: episode: 244, duration: 0.702s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.113 [-2.033, 1.372], loss: 0.532768, mean_absolute_error: 3.282942, mean_q: 6.811296\n",
      "Episode 245 reward: 9.0\n",
      "  2454/100000: episode: 245, duration: 0.532s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-1.919, 1.145], loss: 0.505080, mean_absolute_error: 3.311500, mean_q: 6.889246\n",
      "Episode 246 reward: 10.0\n",
      "  2464/100000: episode: 246, duration: 0.612s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.122 [-1.886, 1.196], loss: 0.506391, mean_absolute_error: 3.410453, mean_q: 7.176416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 247 reward: 11.0\n",
      "  2475/100000: episode: 247, duration: 0.756s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.817, 1.149], loss: 0.601023, mean_absolute_error: 3.322456, mean_q: 6.871834\n",
      "Episode 248 reward: 11.0\n",
      "  2486/100000: episode: 248, duration: 0.669s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.118 [-2.167, 1.329], loss: 0.599711, mean_absolute_error: 3.431423, mean_q: 7.097363\n",
      "Episode 249 reward: 10.0\n",
      "  2496/100000: episode: 249, duration: 0.650s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.107 [-1.850, 1.216], loss: 0.456110, mean_absolute_error: 3.316706, mean_q: 6.948709\n",
      "Episode 250 reward: 9.0\n",
      "  2505/100000: episode: 250, duration: 0.547s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.139 [-1.920, 1.175], loss: 0.517765, mean_absolute_error: 3.413759, mean_q: 7.152728\n",
      "Episode 251 reward: 11.0\n",
      "  2516/100000: episode: 251, duration: 0.660s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.096 [-1.984, 1.361], loss: 0.567412, mean_absolute_error: 3.378850, mean_q: 6.935475\n",
      "Episode 252 reward: 10.0\n",
      "  2526/100000: episode: 252, duration: 0.603s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.150 [-1.905, 1.127], loss: 0.522894, mean_absolute_error: 3.412654, mean_q: 7.107638\n",
      "Episode 253 reward: 10.0\n",
      "  2536/100000: episode: 253, duration: 0.590s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.149, 1.323], loss: 0.547253, mean_absolute_error: 3.336287, mean_q: 6.875048\n",
      "Episode 254 reward: 10.0\n",
      "  2546/100000: episode: 254, duration: 0.591s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.154 [-2.265, 1.364], loss: 0.695807, mean_absolute_error: 3.375699, mean_q: 6.875041\n",
      "Episode 255 reward: 11.0\n",
      "  2557/100000: episode: 255, duration: 0.769s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-2.063, 1.334], loss: 0.571872, mean_absolute_error: 3.472980, mean_q: 7.261603\n",
      "Episode 256 reward: 9.0\n",
      "  2566/100000: episode: 256, duration: 0.532s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.163 [-1.946, 1.138], loss: 0.543529, mean_absolute_error: 3.434816, mean_q: 7.123126\n",
      "Episode 257 reward: 12.0\n",
      "  2578/100000: episode: 257, duration: 0.733s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.085 [-1.822, 1.184], loss: 0.576477, mean_absolute_error: 3.378451, mean_q: 6.939337\n",
      "Episode 258 reward: 11.0\n",
      "  2589/100000: episode: 258, duration: 0.784s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.099 [-1.844, 1.205], loss: 0.715039, mean_absolute_error: 3.479959, mean_q: 7.082124\n",
      "Episode 259 reward: 9.0\n",
      "  2598/100000: episode: 259, duration: 0.681s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.115 [-1.903, 1.191], loss: 0.579761, mean_absolute_error: 3.443757, mean_q: 7.117378\n",
      "Episode 260 reward: 9.0\n",
      "  2607/100000: episode: 260, duration: 0.650s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.129 [-1.877, 1.169], loss: 0.584216, mean_absolute_error: 3.373998, mean_q: 6.938009\n",
      "Episode 261 reward: 10.0\n",
      "  2617/100000: episode: 261, duration: 0.668s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.145 [-2.087, 1.325], loss: 0.536971, mean_absolute_error: 3.452895, mean_q: 7.165553\n",
      "Episode 262 reward: 9.0\n",
      "  2626/100000: episode: 262, duration: 0.678s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.128 [-1.824, 1.170], loss: 0.451698, mean_absolute_error: 3.457862, mean_q: 7.227858\n",
      "Episode 263 reward: 11.0\n",
      "  2637/100000: episode: 263, duration: 0.875s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.130 [-2.060, 1.351], loss: 0.371480, mean_absolute_error: 3.411145, mean_q: 7.174161\n",
      "Episode 264 reward: 12.0\n",
      "  2649/100000: episode: 264, duration: 0.991s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-2.023, 1.372], loss: 0.499254, mean_absolute_error: 3.486162, mean_q: 7.186994\n",
      "Episode 265 reward: 9.0\n",
      "  2658/100000: episode: 265, duration: 0.808s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.147 [-1.884, 1.190], loss: 0.639797, mean_absolute_error: 3.536195, mean_q: 7.262182\n",
      "Episode 266 reward: 11.0\n",
      "  2669/100000: episode: 266, duration: 0.871s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-2.001, 1.357], loss: 0.477183, mean_absolute_error: 3.499551, mean_q: 7.265608\n",
      "Episode 267 reward: 10.0\n",
      "  2679/100000: episode: 267, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.122 [-1.806, 1.174], loss: 0.532366, mean_absolute_error: 3.479960, mean_q: 7.134528\n",
      "Episode 268 reward: 10.0\n",
      "  2689/100000: episode: 268, duration: 0.900s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.120 [-1.858, 1.214], loss: 0.711045, mean_absolute_error: 3.444396, mean_q: 6.890841\n",
      "Episode 269 reward: 9.0\n",
      "  2698/100000: episode: 269, duration: 0.654s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.132 [-1.900, 1.173], loss: 0.530395, mean_absolute_error: 3.441144, mean_q: 7.064108\n",
      "Episode 270 reward: 10.0\n",
      "  2708/100000: episode: 270, duration: 0.843s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.129 [-1.827, 1.201], loss: 0.549646, mean_absolute_error: 3.554655, mean_q: 7.311160\n",
      "Episode 271 reward: 9.0\n",
      "  2717/100000: episode: 271, duration: 0.722s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-1.881, 1.166], loss: 0.703238, mean_absolute_error: 3.486975, mean_q: 6.976042\n",
      "Episode 272 reward: 12.0\n",
      "  2729/100000: episode: 272, duration: 1.018s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.767, 1.168], loss: 0.620921, mean_absolute_error: 3.467492, mean_q: 6.992268\n",
      "Episode 273 reward: 10.0\n",
      "  2739/100000: episode: 273, duration: 0.775s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.151 [-1.879, 1.127], loss: 0.533927, mean_absolute_error: 3.489783, mean_q: 7.072936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 274 reward: 9.0\n",
      "  2748/100000: episode: 274, duration: 0.729s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.147 [-1.881, 1.133], loss: 0.579186, mean_absolute_error: 3.584188, mean_q: 7.282844\n",
      "Episode 275 reward: 12.0\n",
      "  2760/100000: episode: 275, duration: 1.032s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-1.716, 1.164], loss: 0.608239, mean_absolute_error: 3.516295, mean_q: 7.118983\n",
      "Episode 276 reward: 11.0\n",
      "  2771/100000: episode: 276, duration: 0.762s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.119 [-1.575, 0.984], loss: 0.513602, mean_absolute_error: 3.518442, mean_q: 7.182050\n",
      "Episode 277 reward: 12.0\n",
      "  2783/100000: episode: 277, duration: 0.803s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.084 [-1.802, 1.214], loss: 0.516163, mean_absolute_error: 3.502436, mean_q: 7.154470\n",
      "Episode 278 reward: 9.0\n",
      "  2792/100000: episode: 278, duration: 0.567s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.160 [-1.944, 1.142], loss: 0.717696, mean_absolute_error: 3.439451, mean_q: 6.798275\n",
      "Episode 279 reward: 11.0\n",
      "  2803/100000: episode: 279, duration: 0.667s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.131 [-1.806, 1.152], loss: 0.502344, mean_absolute_error: 3.551408, mean_q: 7.297363\n",
      "Episode 280 reward: 10.0\n",
      "  2813/100000: episode: 280, duration: 0.610s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.114 [-1.619, 1.025], loss: 0.602320, mean_absolute_error: 3.586028, mean_q: 7.256722\n",
      "Episode 281 reward: 11.0\n",
      "  2824/100000: episode: 281, duration: 0.733s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.100 [-1.633, 1.008], loss: 0.692962, mean_absolute_error: 3.538259, mean_q: 7.087000\n",
      "Episode 282 reward: 10.0\n",
      "  2834/100000: episode: 282, duration: 0.603s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.136 [-1.828, 1.167], loss: 0.456178, mean_absolute_error: 3.571557, mean_q: 7.318006\n",
      "Episode 283 reward: 10.0\n",
      "  2844/100000: episode: 283, duration: 0.622s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.116 [-1.781, 1.187], loss: 0.619171, mean_absolute_error: 3.482920, mean_q: 6.903261\n",
      "Episode 284 reward: 14.0\n",
      "  2858/100000: episode: 284, duration: 0.867s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.095 [-1.764, 1.186], loss: 0.613883, mean_absolute_error: 3.571009, mean_q: 7.195134\n",
      "Episode 285 reward: 10.0\n",
      "  2868/100000: episode: 285, duration: 0.590s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.110 [-1.569, 1.017], loss: 0.702142, mean_absolute_error: 3.583481, mean_q: 7.131250\n",
      "Episode 286 reward: 9.0\n",
      "  2877/100000: episode: 286, duration: 0.546s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.600, 1.004], loss: 0.610571, mean_absolute_error: 3.549529, mean_q: 7.108940\n",
      "Episode 287 reward: 11.0\n",
      "  2888/100000: episode: 287, duration: 0.661s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.115 [-1.630, 0.940], loss: 0.543758, mean_absolute_error: 3.630828, mean_q: 7.312933\n",
      "Episode 288 reward: 11.0\n",
      "  2899/100000: episode: 288, duration: 0.675s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.119 [-1.800, 1.183], loss: 0.551032, mean_absolute_error: 3.629763, mean_q: 7.316175\n",
      "Episode 289 reward: 9.0\n",
      "  2908/100000: episode: 289, duration: 0.626s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.139 [-1.873, 1.151], loss: 0.553720, mean_absolute_error: 3.693565, mean_q: 7.463066\n",
      "Episode 290 reward: 13.0\n",
      "  2921/100000: episode: 290, duration: 0.807s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.123 [-2.320, 1.350], loss: 0.499887, mean_absolute_error: 3.664793, mean_q: 7.402367\n",
      "Episode 291 reward: 11.0\n",
      "  2932/100000: episode: 291, duration: 0.716s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.119 [-1.791, 1.158], loss: 0.513125, mean_absolute_error: 3.718880, mean_q: 7.444343\n",
      "Episode 292 reward: 9.0\n",
      "  2941/100000: episode: 292, duration: 0.563s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.145 [-1.964, 1.172], loss: 0.566268, mean_absolute_error: 3.640143, mean_q: 7.220715\n",
      "Episode 293 reward: 12.0\n",
      "  2953/100000: episode: 293, duration: 0.757s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.088 [-1.705, 1.216], loss: 0.669185, mean_absolute_error: 3.697337, mean_q: 7.309624\n",
      "Episode 294 reward: 9.0\n",
      "  2962/100000: episode: 294, duration: 0.587s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.128 [-1.586, 0.945], loss: 0.516086, mean_absolute_error: 3.691370, mean_q: 7.387919\n",
      "Episode 295 reward: 11.0\n",
      "  2973/100000: episode: 295, duration: 0.672s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.097 [-1.819, 1.217], loss: 0.618105, mean_absolute_error: 3.755270, mean_q: 7.518935\n",
      "Episode 296 reward: 11.0\n",
      "  2984/100000: episode: 296, duration: 0.751s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.114 [-1.796, 1.136], loss: 0.436119, mean_absolute_error: 3.689991, mean_q: 7.457148\n",
      "Episode 297 reward: 14.0\n",
      "  2998/100000: episode: 297, duration: 0.947s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.740, 1.173], loss: 0.545807, mean_absolute_error: 3.717746, mean_q: 7.412911\n",
      "Episode 298 reward: 10.0\n",
      "  3008/100000: episode: 298, duration: 0.750s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.127 [-1.594, 0.950], loss: 0.495527, mean_absolute_error: 3.733243, mean_q: 7.466348\n",
      "Episode 299 reward: 10.0\n",
      "  3018/100000: episode: 299, duration: 0.687s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.095 [-1.512, 1.019], loss: 0.530413, mean_absolute_error: 3.770072, mean_q: 7.500915\n",
      "Episode 300 reward: 14.0\n",
      "  3032/100000: episode: 300, duration: 1.094s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.101 [-1.749, 1.166], loss: 0.659366, mean_absolute_error: 3.728839, mean_q: 7.346810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 301 reward: 13.0\n",
      "  3045/100000: episode: 301, duration: 0.844s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.089 [-1.749, 1.202], loss: 0.441595, mean_absolute_error: 3.751009, mean_q: 7.515048\n",
      "Episode 302 reward: 12.0\n",
      "  3057/100000: episode: 302, duration: 0.863s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.104 [-1.535, 0.994], loss: 0.526548, mean_absolute_error: 3.804760, mean_q: 7.582945\n",
      "Episode 303 reward: 11.0\n",
      "  3068/100000: episode: 303, duration: 0.769s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.109 [-1.542, 0.953], loss: 0.663403, mean_absolute_error: 3.838862, mean_q: 7.561824\n",
      "Episode 304 reward: 13.0\n",
      "  3081/100000: episode: 304, duration: 0.873s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.090 [-1.695, 1.159], loss: 0.656994, mean_absolute_error: 3.766661, mean_q: 7.385010\n",
      "Episode 305 reward: 11.0\n",
      "  3092/100000: episode: 305, duration: 0.709s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.115 [-1.379, 0.804], loss: 0.581564, mean_absolute_error: 3.734016, mean_q: 7.351604\n",
      "Episode 306 reward: 12.0\n",
      "  3104/100000: episode: 306, duration: 0.773s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.124 [-1.513, 0.949], loss: 0.652606, mean_absolute_error: 3.892777, mean_q: 7.655293\n",
      "Episode 307 reward: 13.0\n",
      "  3117/100000: episode: 307, duration: 0.794s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.094 [-1.684, 1.138], loss: 0.599081, mean_absolute_error: 3.881058, mean_q: 7.668976\n",
      "Episode 308 reward: 14.0\n",
      "  3131/100000: episode: 308, duration: 0.948s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.590, 0.975], loss: 0.592496, mean_absolute_error: 3.794613, mean_q: 7.484702\n",
      "Episode 309 reward: 14.0\n",
      "  3145/100000: episode: 309, duration: 0.932s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.101 [-1.678, 1.146], loss: 0.486508, mean_absolute_error: 3.810298, mean_q: 7.524524\n",
      "Episode 310 reward: 12.0\n",
      "  3157/100000: episode: 310, duration: 0.884s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.121 [-1.763, 1.152], loss: 0.507054, mean_absolute_error: 3.836814, mean_q: 7.552113\n",
      "Episode 311 reward: 10.0\n",
      "  3167/100000: episode: 311, duration: 0.669s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.124 [-1.513, 0.981], loss: 0.949941, mean_absolute_error: 3.807973, mean_q: 7.290672\n",
      "Episode 312 reward: 12.0\n",
      "  3179/100000: episode: 312, duration: 0.765s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.087 [-1.315, 0.823], loss: 0.803792, mean_absolute_error: 3.885963, mean_q: 7.525551\n",
      "Episode 313 reward: 13.0\n",
      "  3192/100000: episode: 313, duration: 0.804s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.091 [-1.501, 1.021], loss: 0.732098, mean_absolute_error: 3.857376, mean_q: 7.466576\n",
      "Episode 314 reward: 11.0\n",
      "  3203/100000: episode: 314, duration: 0.785s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.134 [-1.539, 0.938], loss: 0.657754, mean_absolute_error: 3.884370, mean_q: 7.571525\n",
      "Episode 315 reward: 13.0\n",
      "  3216/100000: episode: 315, duration: 0.857s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.101 [-1.533, 0.981], loss: 0.370108, mean_absolute_error: 3.875388, mean_q: 7.749444\n",
      "Episode 316 reward: 13.0\n",
      "  3229/100000: episode: 316, duration: 0.874s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.103 [-1.482, 0.961], loss: 0.573444, mean_absolute_error: 3.908026, mean_q: 7.658242\n",
      "Episode 317 reward: 10.0\n",
      "  3239/100000: episode: 317, duration: 0.661s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.117 [-1.347, 0.816], loss: 0.587795, mean_absolute_error: 4.005524, mean_q: 7.866214\n",
      "Episode 318 reward: 10.0\n",
      "  3249/100000: episode: 318, duration: 0.650s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.103 [-1.505, 0.967], loss: 0.717686, mean_absolute_error: 3.911409, mean_q: 7.612093\n",
      "Episode 319 reward: 11.0\n",
      "  3260/100000: episode: 319, duration: 0.738s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.126 [-1.843, 1.138], loss: 0.481867, mean_absolute_error: 3.969284, mean_q: 7.809052\n",
      "Episode 320 reward: 11.0\n",
      "  3271/100000: episode: 320, duration: 0.679s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.109 [-1.769, 1.202], loss: 0.630266, mean_absolute_error: 3.950060, mean_q: 7.678823\n",
      "Episode 321 reward: 11.0\n",
      "  3282/100000: episode: 321, duration: 0.868s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.126 [-1.501, 0.933], loss: 0.788608, mean_absolute_error: 3.919693, mean_q: 7.581573\n",
      "Episode 322 reward: 11.0\n",
      "  3293/100000: episode: 322, duration: 0.746s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.101 [-1.313, 0.816], loss: 0.508413, mean_absolute_error: 3.901364, mean_q: 7.645212\n",
      "Episode 323 reward: 10.0\n",
      "  3303/100000: episode: 323, duration: 0.740s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.132 [-1.327, 0.758], loss: 0.472739, mean_absolute_error: 3.962184, mean_q: 7.833421\n",
      "Episode 324 reward: 14.0\n",
      "  3317/100000: episode: 324, duration: 0.929s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.090 [-1.690, 1.181], loss: 0.809689, mean_absolute_error: 3.941024, mean_q: 7.561072\n",
      "Episode 325 reward: 12.0\n",
      "  3329/100000: episode: 325, duration: 0.767s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.082 [-1.439, 0.975], loss: 0.633567, mean_absolute_error: 3.918813, mean_q: 7.610604\n",
      "Episode 326 reward: 10.0\n",
      "  3339/100000: episode: 326, duration: 0.635s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.132 [-1.319, 0.792], loss: 0.861636, mean_absolute_error: 3.935214, mean_q: 7.495343\n",
      "Episode 327 reward: 16.0\n",
      "  3355/100000: episode: 327, duration: 1.170s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.061 [-1.424, 1.016], loss: 0.823395, mean_absolute_error: 4.014098, mean_q: 7.675359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 328 reward: 12.0\n",
      "  3367/100000: episode: 328, duration: 0.933s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.122 [-1.461, 0.931], loss: 0.800741, mean_absolute_error: 3.994731, mean_q: 7.669652\n",
      "Episode 329 reward: 15.0\n",
      "  3382/100000: episode: 329, duration: 1.112s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.082 [-1.420, 0.945], loss: 0.681990, mean_absolute_error: 4.010434, mean_q: 7.706423\n",
      "Episode 330 reward: 15.0\n",
      "  3397/100000: episode: 330, duration: 1.097s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.067 [-1.398, 1.017], loss: 0.802753, mean_absolute_error: 3.964822, mean_q: 7.545300\n",
      "Episode 331 reward: 10.0\n",
      "  3407/100000: episode: 331, duration: 0.739s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.136 [-1.275, 0.742], loss: 0.504716, mean_absolute_error: 4.035511, mean_q: 7.823306\n",
      "Episode 332 reward: 13.0\n",
      "  3420/100000: episode: 332, duration: 1.132s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.099 [-1.225, 0.786], loss: 0.886631, mean_absolute_error: 4.067787, mean_q: 7.742281\n",
      "Episode 333 reward: 11.0\n",
      "  3431/100000: episode: 333, duration: 0.735s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.120 [-1.306, 0.794], loss: 0.541327, mean_absolute_error: 4.052441, mean_q: 7.829426\n",
      "Episode 334 reward: 15.0\n",
      "  3446/100000: episode: 334, duration: 1.002s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.086 [-1.214, 0.810], loss: 0.919060, mean_absolute_error: 4.063909, mean_q: 7.699570\n",
      "Episode 335 reward: 16.0\n",
      "  3462/100000: episode: 335, duration: 1.024s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.080 [-1.416, 0.980], loss: 0.591012, mean_absolute_error: 4.063143, mean_q: 7.840895\n",
      "Episode 336 reward: 13.0\n",
      "  3475/100000: episode: 336, duration: 0.968s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.114 [-1.450, 0.939], loss: 0.618328, mean_absolute_error: 4.129824, mean_q: 7.957174\n",
      "Episode 337 reward: 11.0\n",
      "  3486/100000: episode: 337, duration: 0.706s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.135 [-1.295, 0.739], loss: 0.617351, mean_absolute_error: 4.193332, mean_q: 8.121155\n",
      "Episode 338 reward: 12.0\n",
      "  3498/100000: episode: 338, duration: 0.765s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.107 [-1.236, 0.781], loss: 0.704709, mean_absolute_error: 4.114116, mean_q: 7.864757\n",
      "Episode 339 reward: 13.0\n",
      "  3511/100000: episode: 339, duration: 0.762s, episode steps: 13, steps per second: 17, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.120 [-1.227, 0.748], loss: 0.832383, mean_absolute_error: 4.238060, mean_q: 8.113650\n",
      "Episode 340 reward: 18.0\n",
      "  3529/100000: episode: 340, duration: 1.141s, episode steps: 18, steps per second: 16, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.159, 0.823], loss: 0.864555, mean_absolute_error: 4.129324, mean_q: 7.839481\n",
      "Episode 341 reward: 13.0\n",
      "  3542/100000: episode: 341, duration: 0.799s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.111 [-1.244, 0.800], loss: 0.852200, mean_absolute_error: 4.184264, mean_q: 7.965937\n",
      "Episode 342 reward: 17.0\n",
      "  3559/100000: episode: 342, duration: 1.147s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.069 [-1.390, 1.017], loss: 0.771038, mean_absolute_error: 4.196845, mean_q: 8.031519\n",
      "Episode 343 reward: 16.0\n",
      "  3575/100000: episode: 343, duration: 1.121s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.088 [-1.178, 0.790], loss: 0.799554, mean_absolute_error: 4.150297, mean_q: 7.908354\n",
      "Episode 344 reward: 17.0\n",
      "  3592/100000: episode: 344, duration: 1.101s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.075 [-1.367, 1.001], loss: 0.751798, mean_absolute_error: 4.136038, mean_q: 7.873411\n",
      "Episode 345 reward: 13.0\n",
      "  3605/100000: episode: 345, duration: 0.820s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.104 [-1.275, 0.774], loss: 1.022380, mean_absolute_error: 4.133274, mean_q: 7.785966\n",
      "Episode 346 reward: 14.0\n",
      "  3619/100000: episode: 346, duration: 0.933s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.123 [-1.247, 0.748], loss: 0.814912, mean_absolute_error: 4.252145, mean_q: 8.135768\n",
      "Episode 347 reward: 16.0\n",
      "  3635/100000: episode: 347, duration: 0.960s, episode steps: 16, steps per second: 17, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.172, 0.786], loss: 0.906424, mean_absolute_error: 4.179958, mean_q: 7.905140\n",
      "Episode 348 reward: 17.0\n",
      "  3652/100000: episode: 348, duration: 1.131s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.060 [-1.145, 0.823], loss: 0.765475, mean_absolute_error: 4.193783, mean_q: 7.982204\n",
      "Episode 349 reward: 12.0\n",
      "  3664/100000: episode: 349, duration: 0.730s, episode steps: 12, steps per second: 16, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.191, 0.819], loss: 0.767442, mean_absolute_error: 4.287024, mean_q: 8.201385\n",
      "Episode 350 reward: 11.0\n",
      "  3675/100000: episode: 350, duration: 0.708s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.147 [-1.491, 0.748], loss: 0.797671, mean_absolute_error: 4.296147, mean_q: 8.243609\n",
      "Episode 351 reward: 14.0\n",
      "  3689/100000: episode: 351, duration: 0.873s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.335, 0.798], loss: 0.697391, mean_absolute_error: 4.246669, mean_q: 8.145421\n",
      "Episode 352 reward: 16.0\n",
      "  3705/100000: episode: 352, duration: 0.954s, episode steps: 16, steps per second: 17, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-1.144, 0.821], loss: 0.723983, mean_absolute_error: 4.163146, mean_q: 7.927440\n",
      "Episode 353 reward: 16.0\n",
      "  3721/100000: episode: 353, duration: 1.009s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.385, 0.963], loss: 0.752066, mean_absolute_error: 4.221111, mean_q: 8.090578\n",
      "Episode 354 reward: 12.0\n",
      "  3733/100000: episode: 354, duration: 0.884s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.549, 1.006], loss: 0.825164, mean_absolute_error: 4.292064, mean_q: 8.116879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 355 reward: 17.0\n",
      "  3750/100000: episode: 355, duration: 1.303s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.081 [-1.179, 0.817], loss: 0.824735, mean_absolute_error: 4.290228, mean_q: 8.137053\n",
      "Episode 356 reward: 11.0\n",
      "  3761/100000: episode: 356, duration: 0.743s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.129 [-1.256, 0.739], loss: 0.855875, mean_absolute_error: 4.224800, mean_q: 8.012106\n",
      "Episode 357 reward: 20.0\n",
      "  3781/100000: episode: 357, duration: 1.240s, episode steps: 20, steps per second: 16, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-1.114, 0.828], loss: 0.856455, mean_absolute_error: 4.304248, mean_q: 8.142698\n",
      "Episode 358 reward: 16.0\n",
      "  3797/100000: episode: 358, duration: 1.175s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.214, 0.770], loss: 1.030026, mean_absolute_error: 4.346426, mean_q: 8.212352\n",
      "Episode 359 reward: 14.0\n",
      "  3811/100000: episode: 359, duration: 0.911s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.131 [-0.978, 0.545], loss: 0.831387, mean_absolute_error: 4.315536, mean_q: 8.155525\n",
      "Episode 360 reward: 14.0\n",
      "  3825/100000: episode: 360, duration: 0.947s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.130 [-0.980, 0.566], loss: 0.784994, mean_absolute_error: 4.311785, mean_q: 8.175000\n",
      "Episode 361 reward: 17.0\n",
      "  3842/100000: episode: 361, duration: 1.367s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.079 [-1.178, 0.820], loss: 0.797911, mean_absolute_error: 4.297298, mean_q: 8.140127\n",
      "Episode 362 reward: 15.0\n",
      "  3857/100000: episode: 362, duration: 1.094s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.110 [-1.166, 0.759], loss: 0.767873, mean_absolute_error: 4.379845, mean_q: 8.365724\n",
      "Episode 363 reward: 16.0\n",
      "  3873/100000: episode: 363, duration: 1.073s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.111 [-1.188, 0.755], loss: 0.786457, mean_absolute_error: 4.354371, mean_q: 8.267700\n",
      "Episode 364 reward: 19.0\n",
      "  3892/100000: episode: 364, duration: 1.495s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.097 [-1.195, 0.785], loss: 0.899860, mean_absolute_error: 4.378236, mean_q: 8.280692\n",
      "Episode 365 reward: 19.0\n",
      "  3911/100000: episode: 365, duration: 1.364s, episode steps: 19, steps per second: 14, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-1.175, 0.777], loss: 0.905002, mean_absolute_error: 4.438922, mean_q: 8.411226\n",
      "Episode 366 reward: 19.0\n",
      "  3930/100000: episode: 366, duration: 1.251s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.933, 0.596], loss: 0.676007, mean_absolute_error: 4.348955, mean_q: 8.269015\n",
      "Episode 367 reward: 18.0\n",
      "  3948/100000: episode: 367, duration: 1.402s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.909, 0.540], loss: 0.724816, mean_absolute_error: 4.513296, mean_q: 8.610352\n",
      "Episode 368 reward: 23.0\n",
      "  3971/100000: episode: 368, duration: 1.537s, episode steps: 23, steps per second: 15, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.088 [-1.158, 0.806], loss: 0.886264, mean_absolute_error: 4.384775, mean_q: 8.281147\n",
      "Episode 369 reward: 16.0\n",
      "  3987/100000: episode: 369, duration: 0.983s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.081 [-1.282, 0.638], loss: 1.177843, mean_absolute_error: 4.382876, mean_q: 8.158087\n",
      "Episode 370 reward: 17.0\n",
      "  4004/100000: episode: 370, duration: 1.049s, episode steps: 17, steps per second: 16, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.112 [-0.980, 0.579], loss: 0.787144, mean_absolute_error: 4.356051, mean_q: 8.246515\n",
      "Episode 371 reward: 14.0\n",
      "  4018/100000: episode: 371, duration: 0.932s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.028, 0.567], loss: 0.844511, mean_absolute_error: 4.403670, mean_q: 8.373652\n",
      "Episode 372 reward: 18.0\n",
      "  4036/100000: episode: 372, duration: 1.135s, episode steps: 18, steps per second: 16, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.055, 0.611], loss: 0.663792, mean_absolute_error: 4.496011, mean_q: 8.607559\n",
      "Episode 373 reward: 17.0\n",
      "  4053/100000: episode: 373, duration: 1.128s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.092 [-1.153, 0.807], loss: 0.772877, mean_absolute_error: 4.423720, mean_q: 8.376260\n",
      "Episode 374 reward: 28.0\n",
      "  4081/100000: episode: 374, duration: 1.802s, episode steps: 28, steps per second: 16, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.084 [-1.155, 0.813], loss: 0.856055, mean_absolute_error: 4.527378, mean_q: 8.596204\n",
      "Episode 375 reward: 20.0\n",
      "  4101/100000: episode: 375, duration: 1.591s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.121 [-0.929, 0.546], loss: 0.644498, mean_absolute_error: 4.536532, mean_q: 8.684608\n",
      "Episode 376 reward: 27.0\n",
      "  4128/100000: episode: 376, duration: 1.853s, episode steps: 27, steps per second: 15, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.106 [-0.934, 0.627], loss: 0.823917, mean_absolute_error: 4.549659, mean_q: 8.634276\n",
      "Episode 377 reward: 25.0\n",
      "  4153/100000: episode: 377, duration: 1.635s, episode steps: 25, steps per second: 15, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.110 [-0.875, 0.544], loss: 0.797031, mean_absolute_error: 4.529610, mean_q: 8.588132\n",
      "Episode 378 reward: 33.0\n",
      "  4186/100000: episode: 378, duration: 2.360s, episode steps: 33, steps per second: 14, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.092 [-1.091, 0.737], loss: 1.120528, mean_absolute_error: 4.560446, mean_q: 8.519656\n",
      "Episode 379 reward: 30.0\n",
      "  4216/100000: episode: 379, duration: 2.151s, episode steps: 30, steps per second: 14, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.132 [-0.883, 0.540], loss: 0.889948, mean_absolute_error: 4.588281, mean_q: 8.646709\n",
      "Episode 380 reward: 31.0\n",
      "  4247/100000: episode: 380, duration: 2.056s, episode steps: 31, steps per second: 15, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.098 [-0.874, 0.553], loss: 0.967859, mean_absolute_error: 4.620733, mean_q: 8.693396\n",
      "Episode 381 reward: 37.0\n",
      "  4284/100000: episode: 381, duration: 2.521s, episode steps: 37, steps per second: 15, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.085 [-0.975, 0.590], loss: 1.078187, mean_absolute_error: 4.609604, mean_q: 8.628318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 382 reward: 30.0\n",
      "  4314/100000: episode: 382, duration: 1.996s, episode steps: 30, steps per second: 15, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.109 [-0.832, 0.536], loss: 1.071667, mean_absolute_error: 4.697449, mean_q: 8.820950\n",
      "Episode 383 reward: 27.0\n",
      "  4341/100000: episode: 383, duration: 1.740s, episode steps: 27, steps per second: 16, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.128 [-0.672, 0.375], loss: 0.782242, mean_absolute_error: 4.700507, mean_q: 8.928839\n",
      "Episode 384 reward: 48.0\n",
      "  4389/100000: episode: 384, duration: 3.206s, episode steps: 48, steps per second: 15, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.102 [-0.948, 0.574], loss: 1.044527, mean_absolute_error: 4.764983, mean_q: 8.970333\n",
      "Episode 385 reward: 38.0\n",
      "  4427/100000: episode: 385, duration: 2.467s, episode steps: 38, steps per second: 15, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.144 [-0.808, 0.367], loss: 1.077179, mean_absolute_error: 4.761297, mean_q: 8.946731\n",
      "Episode 386 reward: 45.0\n",
      "  4472/100000: episode: 386, duration: 2.921s, episode steps: 45, steps per second: 15, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.141 [-0.773, 0.344], loss: 0.979822, mean_absolute_error: 4.889431, mean_q: 9.246928\n",
      "Episode 387 reward: 52.0\n",
      "  4524/100000: episode: 387, duration: 3.298s, episode steps: 52, steps per second: 16, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.122 [-0.791, 0.420], loss: 1.092505, mean_absolute_error: 4.871976, mean_q: 9.173784\n",
      "Episode 388 reward: 54.0\n",
      "  4578/100000: episode: 388, duration: 3.639s, episode steps: 54, steps per second: 15, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.166 [-0.920, 0.184], loss: 0.786866, mean_absolute_error: 4.949808, mean_q: 9.435109\n",
      "Episode 389 reward: 62.0\n",
      "  4640/100000: episode: 389, duration: 4.223s, episode steps: 62, steps per second: 15, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.121 [-0.437, 0.975], loss: 0.958228, mean_absolute_error: 5.063625, mean_q: 9.624806\n",
      "Episode 390 reward: 54.0\n",
      "  4694/100000: episode: 390, duration: 3.645s, episode steps: 54, steps per second: 15, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.109 [-0.534, 0.861], loss: 1.002439, mean_absolute_error: 5.148928, mean_q: 9.788520\n",
      "Episode 391 reward: 64.0\n",
      "  4758/100000: episode: 391, duration: 4.445s, episode steps: 64, steps per second: 14, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.180 [-0.626, 0.752], loss: 1.198044, mean_absolute_error: 5.216611, mean_q: 9.796303\n",
      "Episode 392 reward: 126.0\n",
      "  4884/100000: episode: 392, duration: 8.225s, episode steps: 126, steps per second: 15, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.152 [-3.045, 2.602], loss: 0.976041, mean_absolute_error: 5.253266, mean_q: 9.969988\n",
      "Episode 393 reward: 14.0\n",
      "  4898/100000: episode: 393, duration: 0.903s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.088 [-1.518, 2.354], loss: 1.045188, mean_absolute_error: 5.444715, mean_q: 10.380472\n",
      "Episode 394 reward: 9.0\n",
      "  4907/100000: episode: 394, duration: 0.678s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.372, 2.147], loss: 1.037678, mean_absolute_error: 5.366008, mean_q: 10.226982\n",
      "Episode 395 reward: 13.0\n",
      "  4920/100000: episode: 395, duration: 0.864s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.087 [-1.155, 1.762], loss: 1.565155, mean_absolute_error: 5.524879, mean_q: 10.418125\n",
      "Episode 396 reward: 10.0\n",
      "  4930/100000: episode: 396, duration: 0.737s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.103 [-1.601, 2.339], loss: 1.798814, mean_absolute_error: 5.516770, mean_q: 10.336858\n",
      "Episode 397 reward: 19.0\n",
      "  4949/100000: episode: 397, duration: 1.235s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.105 [-0.956, 1.479], loss: 1.414476, mean_absolute_error: 5.427143, mean_q: 10.191147\n",
      "Episode 398 reward: 9.0\n",
      "  4958/100000: episode: 398, duration: 0.592s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.569, 2.449], loss: 1.476733, mean_absolute_error: 5.421983, mean_q: 10.139827\n",
      "Episode 399 reward: 11.0\n",
      "  4969/100000: episode: 399, duration: 0.697s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.093 [-1.805, 2.732], loss: 1.676102, mean_absolute_error: 5.487484, mean_q: 10.271976\n",
      "Episode 400 reward: 10.0\n",
      "  4979/100000: episode: 400, duration: 0.740s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.177 [-1.910, 3.120], loss: 0.975565, mean_absolute_error: 5.492609, mean_q: 10.429705\n",
      "Episode 401 reward: 9.0\n",
      "  4988/100000: episode: 401, duration: 0.649s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.383, 2.263], loss: 1.320908, mean_absolute_error: 5.503396, mean_q: 10.402941\n",
      "Episode 402 reward: 10.0\n",
      "  4998/100000: episode: 402, duration: 0.650s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.124 [-1.610, 2.540], loss: 1.292664, mean_absolute_error: 5.592595, mean_q: 10.600626\n",
      "Episode 403 reward: 9.0\n",
      "  5007/100000: episode: 403, duration: 0.647s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.783, 2.766], loss: 1.623194, mean_absolute_error: 5.641390, mean_q: 10.724301\n",
      "Episode 404 reward: 12.0\n",
      "  5019/100000: episode: 404, duration: 0.821s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.111 [-1.909, 2.985], loss: 1.378481, mean_absolute_error: 5.521164, mean_q: 10.497736\n",
      "Episode 405 reward: 10.0\n",
      "  5029/100000: episode: 405, duration: 0.683s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.117 [-1.984, 3.022], loss: 1.554795, mean_absolute_error: 5.613914, mean_q: 10.584135\n",
      "Episode 406 reward: 10.0\n",
      "  5039/100000: episode: 406, duration: 0.661s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.992, 2.986], loss: 1.484488, mean_absolute_error: 5.568141, mean_q: 10.532650\n",
      "Episode 407 reward: 9.0\n",
      "  5048/100000: episode: 407, duration: 0.621s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.718, 2.839], loss: 1.974890, mean_absolute_error: 5.646377, mean_q: 10.674095\n",
      "Episode 408 reward: 12.0\n",
      "  5060/100000: episode: 408, duration: 0.837s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.177, 2.077], loss: 1.461599, mean_absolute_error: 5.757373, mean_q: 10.960256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 409 reward: 10.0\n",
      "  5070/100000: episode: 409, duration: 0.692s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.917, 3.068], loss: 1.059602, mean_absolute_error: 5.611640, mean_q: 10.717160\n",
      "Episode 410 reward: 9.0\n",
      "  5079/100000: episode: 410, duration: 0.828s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.738, 2.821], loss: 1.463403, mean_absolute_error: 5.662666, mean_q: 10.797404\n",
      "Episode 411 reward: 9.0\n",
      "  5088/100000: episode: 411, duration: 0.658s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.772, 2.842], loss: 1.434700, mean_absolute_error: 5.738854, mean_q: 10.912879\n",
      "Episode 412 reward: 9.0\n",
      "  5097/100000: episode: 412, duration: 0.642s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.771, 2.744], loss: 1.209379, mean_absolute_error: 5.721577, mean_q: 10.824304\n",
      "Episode 413 reward: 10.0\n",
      "  5107/100000: episode: 413, duration: 0.671s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.604, 2.559], loss: 2.798770, mean_absolute_error: 5.915798, mean_q: 11.018301\n",
      "Episode 414 reward: 9.0\n",
      "  5116/100000: episode: 414, duration: 0.754s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [-1.787, 2.787], loss: 1.654610, mean_absolute_error: 5.827293, mean_q: 10.998298\n",
      "Episode 415 reward: 12.0\n",
      "  5128/100000: episode: 415, duration: 0.913s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.125 [-1.592, 2.601], loss: 2.656030, mean_absolute_error: 5.828338, mean_q: 10.904419\n",
      "Episode 416 reward: 8.0\n",
      "  5136/100000: episode: 416, duration: 0.547s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.598, 2.548], loss: 1.240924, mean_absolute_error: 5.772925, mean_q: 11.005512\n",
      "Episode 417 reward: 12.0\n",
      "  5148/100000: episode: 417, duration: 1.115s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.113 [-1.740, 2.694], loss: 2.277583, mean_absolute_error: 5.741611, mean_q: 10.710010\n",
      "Episode 418 reward: 9.0\n",
      "  5157/100000: episode: 418, duration: 0.769s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.784, 2.824], loss: 2.208372, mean_absolute_error: 5.663663, mean_q: 10.619999\n",
      "Episode 419 reward: 11.0\n",
      "  5168/100000: episode: 419, duration: 1.098s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.362, 2.226], loss: 2.115246, mean_absolute_error: 5.786684, mean_q: 10.865287\n",
      "Episode 420 reward: 15.0\n",
      "  5183/100000: episode: 420, duration: 1.235s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.101 [-1.561, 2.470], loss: 2.330848, mean_absolute_error: 5.865980, mean_q: 10.952525\n",
      "Episode 421 reward: 10.0\n",
      "  5193/100000: episode: 421, duration: 0.752s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.906, 3.042], loss: 2.348736, mean_absolute_error: 6.014920, mean_q: 11.238841\n",
      "Episode 422 reward: 10.0\n",
      "  5203/100000: episode: 422, duration: 0.838s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.105 [-1.603, 2.525], loss: 2.119658, mean_absolute_error: 5.784239, mean_q: 10.867319\n",
      "Episode 423 reward: 10.0\n",
      "  5213/100000: episode: 423, duration: 0.786s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.925, 3.010], loss: 1.515909, mean_absolute_error: 5.660663, mean_q: 10.689028\n",
      "Episode 424 reward: 10.0\n",
      "  5223/100000: episode: 424, duration: 0.729s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.931, 3.028], loss: 3.122671, mean_absolute_error: 5.884742, mean_q: 10.902843\n",
      "Episode 425 reward: 10.0\n",
      "  5233/100000: episode: 425, duration: 0.652s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.604, 2.463], loss: 2.106182, mean_absolute_error: 6.006025, mean_q: 11.230905\n",
      "Episode 426 reward: 10.0\n",
      "  5243/100000: episode: 426, duration: 0.724s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.995, 3.113], loss: 1.326342, mean_absolute_error: 5.729537, mean_q: 10.792773\n",
      "Episode 427 reward: 9.0\n",
      "  5252/100000: episode: 427, duration: 0.628s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.803, 2.746], loss: 0.940199, mean_absolute_error: 5.803294, mean_q: 11.032974\n",
      "Episode 428 reward: 10.0\n",
      "  5262/100000: episode: 428, duration: 0.698s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.997, 3.027], loss: 1.461715, mean_absolute_error: 5.802604, mean_q: 10.987668\n",
      "Episode 429 reward: 10.0\n",
      "  5272/100000: episode: 429, duration: 0.665s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.976, 2.999], loss: 1.532647, mean_absolute_error: 5.800990, mean_q: 10.971921\n",
      "Episode 430 reward: 11.0\n",
      "  5283/100000: episode: 430, duration: 0.720s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.141 [-1.754, 2.835], loss: 0.938401, mean_absolute_error: 5.939812, mean_q: 11.337867\n",
      "Episode 431 reward: 9.0\n",
      "  5292/100000: episode: 431, duration: 0.580s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.327, 2.271], loss: 1.552777, mean_absolute_error: 5.799460, mean_q: 10.945781\n",
      "Episode 432 reward: 10.0\n",
      "  5302/100000: episode: 432, duration: 0.649s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.982, 3.018], loss: 2.414764, mean_absolute_error: 6.112042, mean_q: 11.535596\n",
      "Episode 433 reward: 11.0\n",
      "  5313/100000: episode: 433, duration: 0.737s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.131 [-1.737, 2.782], loss: 2.594330, mean_absolute_error: 5.983475, mean_q: 11.244419\n",
      "Episode 434 reward: 10.0\n",
      "  5323/100000: episode: 434, duration: 0.716s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.107 [-1.603, 2.450], loss: 3.227870, mean_absolute_error: 6.038750, mean_q: 11.372883\n",
      "Episode 435 reward: 11.0\n",
      "  5334/100000: episode: 435, duration: 0.705s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-2.160, 3.300], loss: 2.656734, mean_absolute_error: 5.995788, mean_q: 11.304951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 436 reward: 10.0\n",
      "  5344/100000: episode: 436, duration: 0.700s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.966, 2.997], loss: 2.349017, mean_absolute_error: 6.158711, mean_q: 11.648821\n",
      "Episode 437 reward: 9.0\n",
      "  5353/100000: episode: 437, duration: 0.558s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.771, 2.869], loss: 3.930130, mean_absolute_error: 6.149429, mean_q: 11.340412\n",
      "Episode 438 reward: 10.0\n",
      "  5363/100000: episode: 438, duration: 0.637s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-2.004, 3.107], loss: 3.531091, mean_absolute_error: 6.102782, mean_q: 11.321573\n",
      "Episode 439 reward: 8.0\n",
      "  5371/100000: episode: 439, duration: 0.523s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.605, 2.516], loss: 1.028388, mean_absolute_error: 6.089602, mean_q: 11.528998\n",
      "Episode 440 reward: 9.0\n",
      "  5380/100000: episode: 440, duration: 0.624s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [-1.739, 2.901], loss: 2.748878, mean_absolute_error: 6.229115, mean_q: 11.634538\n",
      "Episode 441 reward: 11.0\n",
      "  5391/100000: episode: 441, duration: 0.973s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.112 [-1.796, 2.793], loss: 3.511539, mean_absolute_error: 6.193525, mean_q: 11.524905\n",
      "Episode 442 reward: 9.0\n",
      "  5400/100000: episode: 442, duration: 0.820s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.725, 2.849], loss: 3.785091, mean_absolute_error: 6.195930, mean_q: 11.480921\n",
      "Episode 443 reward: 9.0\n",
      "  5409/100000: episode: 443, duration: 0.745s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.728, 2.865], loss: 3.530730, mean_absolute_error: 6.087450, mean_q: 11.296696\n",
      "Episode 444 reward: 8.0\n",
      "  5417/100000: episode: 444, duration: 0.632s, episode steps: 8, steps per second: 13, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.533, 2.534], loss: 2.211913, mean_absolute_error: 5.930414, mean_q: 11.196654\n",
      "Episode 445 reward: 9.0\n",
      "  5426/100000: episode: 445, duration: 1.081s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.768, 2.796], loss: 2.954534, mean_absolute_error: 6.257384, mean_q: 11.769080\n",
      "Episode 446 reward: 10.0\n",
      "  5436/100000: episode: 446, duration: 0.914s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [-1.967, 2.987], loss: 2.596933, mean_absolute_error: 6.103051, mean_q: 11.424888\n",
      "Episode 447 reward: 12.0\n",
      "  5448/100000: episode: 447, duration: 1.329s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.125 [-1.938, 3.043], loss: 2.731509, mean_absolute_error: 6.263531, mean_q: 11.802541\n",
      "Episode 448 reward: 14.0\n",
      "  5462/100000: episode: 448, duration: 1.494s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.100 [-1.921, 3.035], loss: 3.200288, mean_absolute_error: 6.167259, mean_q: 11.546076\n",
      "Episode 449 reward: 10.0\n",
      "  5472/100000: episode: 449, duration: 0.969s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.977, 3.056], loss: 1.165292, mean_absolute_error: 5.876915, mean_q: 11.158744\n",
      "Episode 450 reward: 9.0\n",
      "  5481/100000: episode: 450, duration: 0.960s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.745, 2.841], loss: 2.989397, mean_absolute_error: 6.187017, mean_q: 11.538671\n",
      "Episode 451 reward: 10.0\n",
      "  5491/100000: episode: 451, duration: 0.967s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.917, 3.094], loss: 2.373239, mean_absolute_error: 6.160176, mean_q: 11.616584\n",
      "Episode 452 reward: 10.0\n",
      "  5501/100000: episode: 452, duration: 1.107s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.606, 2.572], loss: 4.524781, mean_absolute_error: 6.174547, mean_q: 11.383668\n",
      "Episode 453 reward: 11.0\n",
      "  5512/100000: episode: 453, duration: 1.018s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-2.182, 3.281], loss: 5.333279, mean_absolute_error: 6.400087, mean_q: 11.592646\n",
      "Episode 454 reward: 9.0\n",
      "  5521/100000: episode: 454, duration: 0.754s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.762, 2.847], loss: 1.934482, mean_absolute_error: 6.024925, mean_q: 11.311951\n",
      "Episode 455 reward: 10.0\n",
      "  5531/100000: episode: 455, duration: 0.888s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.956, 3.068], loss: 4.336673, mean_absolute_error: 6.319788, mean_q: 11.662660\n",
      "Episode 456 reward: 8.0\n",
      "  5539/100000: episode: 456, duration: 0.580s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.596, 2.580], loss: 2.447551, mean_absolute_error: 6.056681, mean_q: 11.319148\n",
      "Episode 457 reward: 11.0\n",
      "  5550/100000: episode: 457, duration: 1.075s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-2.148, 3.274], loss: 2.232654, mean_absolute_error: 6.142910, mean_q: 11.500680\n",
      "Episode 458 reward: 12.0\n",
      "  5562/100000: episode: 458, duration: 1.214s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.128 [-1.591, 2.639], loss: 2.422539, mean_absolute_error: 6.161777, mean_q: 11.498042\n",
      "Episode 459 reward: 9.0\n",
      "  5571/100000: episode: 459, duration: 0.813s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.806, 2.811], loss: 2.345146, mean_absolute_error: 6.228269, mean_q: 11.595959\n",
      "Episode 460 reward: 9.0\n",
      "  5580/100000: episode: 460, duration: 0.719s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.741, 2.811], loss: 1.692272, mean_absolute_error: 6.183553, mean_q: 11.707012\n",
      "Episode 461 reward: 11.0\n",
      "  5591/100000: episode: 461, duration: 0.863s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.116 [-1.605, 2.550], loss: 3.628001, mean_absolute_error: 6.237044, mean_q: 11.546243\n",
      "Episode 462 reward: 10.0\n",
      "  5601/100000: episode: 462, duration: 0.705s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.588, 2.506], loss: 2.328980, mean_absolute_error: 6.313103, mean_q: 11.821884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 463 reward: 10.0\n",
      "  5611/100000: episode: 463, duration: 0.590s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.106 [-1.557, 2.452], loss: 2.606580, mean_absolute_error: 6.171792, mean_q: 11.600653\n",
      "Episode 464 reward: 10.0\n",
      "  5621/100000: episode: 464, duration: 0.631s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.946, 3.041], loss: 2.990845, mean_absolute_error: 6.285449, mean_q: 11.780552\n",
      "Episode 465 reward: 10.0\n",
      "  5631/100000: episode: 465, duration: 0.638s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [-1.989, 2.959], loss: 2.784318, mean_absolute_error: 6.274082, mean_q: 11.819474\n",
      "Episode 466 reward: 10.0\n",
      "  5641/100000: episode: 466, duration: 0.734s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.103 [-1.969, 2.957], loss: 2.724008, mean_absolute_error: 6.256830, mean_q: 11.826414\n",
      "Episode 467 reward: 10.0\n",
      "  5651/100000: episode: 467, duration: 0.602s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.919, 3.014], loss: 1.795017, mean_absolute_error: 6.150225, mean_q: 11.662807\n",
      "Episode 468 reward: 11.0\n",
      "  5662/100000: episode: 468, duration: 0.713s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.108 [-1.788, 2.743], loss: 2.611143, mean_absolute_error: 6.320534, mean_q: 11.883982\n",
      "Episode 469 reward: 11.0\n",
      "  5673/100000: episode: 469, duration: 0.689s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.093 [-1.787, 2.734], loss: 2.499022, mean_absolute_error: 6.205060, mean_q: 11.690463\n",
      "Episode 470 reward: 10.0\n",
      "  5683/100000: episode: 470, duration: 0.797s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.776, 2.736], loss: 2.768476, mean_absolute_error: 6.394024, mean_q: 12.023687\n",
      "Episode 471 reward: 10.0\n",
      "  5693/100000: episode: 471, duration: 0.635s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.908, 3.126], loss: 3.275460, mean_absolute_error: 6.430208, mean_q: 12.022062\n",
      "Episode 472 reward: 8.0\n",
      "  5701/100000: episode: 472, duration: 0.527s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.136 [-1.380, 2.233], loss: 1.441226, mean_absolute_error: 6.211498, mean_q: 11.795401\n",
      "Episode 473 reward: 10.0\n",
      "  5711/100000: episode: 473, duration: 0.635s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.950, 3.017], loss: 4.239915, mean_absolute_error: 6.528117, mean_q: 12.049078\n",
      "Episode 474 reward: 10.0\n",
      "  5721/100000: episode: 474, duration: 0.606s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.930, 3.095], loss: 2.940737, mean_absolute_error: 6.163449, mean_q: 11.490316\n",
      "Episode 475 reward: 10.0\n",
      "  5731/100000: episode: 475, duration: 0.619s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.571, 2.640], loss: 2.692432, mean_absolute_error: 6.345373, mean_q: 11.869215\n",
      "Episode 476 reward: 11.0\n",
      "  5742/100000: episode: 476, duration: 0.660s, episode steps: 11, steps per second: 17, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-2.189, 3.283], loss: 2.588785, mean_absolute_error: 6.314673, mean_q: 11.859507\n",
      "Episode 477 reward: 9.0\n",
      "  5751/100000: episode: 477, duration: 0.537s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.726, 2.829], loss: 2.745733, mean_absolute_error: 6.199711, mean_q: 11.541272\n",
      "Episode 478 reward: 10.0\n",
      "  5761/100000: episode: 478, duration: 0.695s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.934, 3.073], loss: 2.819995, mean_absolute_error: 6.369359, mean_q: 11.958500\n",
      "Episode 479 reward: 9.0\n",
      "  5770/100000: episode: 479, duration: 0.537s, episode steps: 9, steps per second: 17, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.779, 2.766], loss: 3.405151, mean_absolute_error: 6.306933, mean_q: 11.680457\n",
      "Episode 480 reward: 10.0\n",
      "  5780/100000: episode: 480, duration: 0.823s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.927, 3.032], loss: 1.721847, mean_absolute_error: 6.177070, mean_q: 11.662135\n",
      "Episode 481 reward: 10.0\n",
      "  5790/100000: episode: 481, duration: 0.817s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.115 [-1.998, 3.032], loss: 3.034378, mean_absolute_error: 6.302120, mean_q: 11.766726\n",
      "Episode 482 reward: 9.0\n",
      "  5799/100000: episode: 482, duration: 0.757s, episode steps: 9, steps per second: 12, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.394, 2.282], loss: 2.759741, mean_absolute_error: 6.418910, mean_q: 11.956992\n",
      "Episode 483 reward: 10.0\n",
      "  5809/100000: episode: 483, duration: 0.735s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.116 [-1.988, 3.043], loss: 2.531894, mean_absolute_error: 6.198145, mean_q: 11.601174\n",
      "Episode 484 reward: 10.0\n",
      "  5819/100000: episode: 484, duration: 0.648s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.933, 3.131], loss: 3.191229, mean_absolute_error: 6.374331, mean_q: 11.889185\n",
      "Episode 485 reward: 9.0\n",
      "  5828/100000: episode: 485, duration: 0.646s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.774, 2.855], loss: 2.634682, mean_absolute_error: 6.400959, mean_q: 12.039047\n",
      "Episode 486 reward: 10.0\n",
      "  5838/100000: episode: 486, duration: 0.653s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.156 [-1.525, 2.599], loss: 2.459693, mean_absolute_error: 6.331712, mean_q: 11.910005\n",
      "Episode 487 reward: 9.0\n",
      "  5847/100000: episode: 487, duration: 0.638s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.741, 2.780], loss: 3.618268, mean_absolute_error: 6.402268, mean_q: 11.957576\n",
      "Episode 488 reward: 10.0\n",
      "  5857/100000: episode: 488, duration: 0.685s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.972, 3.061], loss: 3.394633, mean_absolute_error: 6.484141, mean_q: 12.092029\n",
      "Episode 489 reward: 10.0\n",
      "  5867/100000: episode: 489, duration: 0.633s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.576, 2.588], loss: 3.360554, mean_absolute_error: 6.428091, mean_q: 11.891724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 490 reward: 8.0\n",
      "  5875/100000: episode: 490, duration: 0.533s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.152 [-1.175, 1.993], loss: 4.424192, mean_absolute_error: 6.363877, mean_q: 11.577209\n",
      "Episode 491 reward: 11.0\n",
      "  5886/100000: episode: 491, duration: 0.696s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.145 [-1.736, 2.787], loss: 2.413799, mean_absolute_error: 6.383915, mean_q: 11.934017\n",
      "Episode 492 reward: 18.0\n",
      "  5904/100000: episode: 492, duration: 1.285s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.097 [-1.911, 3.075], loss: 2.425350, mean_absolute_error: 6.261332, mean_q: 11.668744\n",
      "Episode 493 reward: 18.0\n",
      "  5922/100000: episode: 493, duration: 1.381s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.070 [-1.943, 3.000], loss: 2.658348, mean_absolute_error: 6.375433, mean_q: 11.912549\n",
      "Episode 494 reward: 15.0\n",
      "  5937/100000: episode: 494, duration: 1.090s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.091 [-1.426, 2.336], loss: 2.474372, mean_absolute_error: 6.303979, mean_q: 11.829217\n",
      "Episode 495 reward: 17.0\n",
      "  5954/100000: episode: 495, duration: 1.251s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.069 [-1.718, 2.697], loss: 1.800249, mean_absolute_error: 6.139895, mean_q: 11.593553\n",
      "Episode 496 reward: 12.0\n",
      "  5966/100000: episode: 496, duration: 0.781s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.129 [-1.939, 3.066], loss: 3.941221, mean_absolute_error: 6.495628, mean_q: 11.979965\n",
      "Episode 497 reward: 17.0\n",
      "  5983/100000: episode: 497, duration: 1.112s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.085 [-1.336, 2.176], loss: 2.081495, mean_absolute_error: 6.308253, mean_q: 11.891685\n",
      "Episode 498 reward: 31.0\n",
      "  6014/100000: episode: 498, duration: 2.087s, episode steps: 31, steps per second: 15, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.323 [0.000, 1.000], mean observation: 0.055 [-2.136, 3.291], loss: 3.187859, mean_absolute_error: 6.280620, mean_q: 11.642027\n",
      "Episode 499 reward: 20.0\n",
      "  6034/100000: episode: 499, duration: 1.260s, episode steps: 20, steps per second: 16, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.005 [-2.376, 3.306], loss: 2.926322, mean_absolute_error: 6.389300, mean_q: 11.896338\n",
      "Episode 500 reward: 41.0\n",
      "  6075/100000: episode: 500, duration: 2.857s, episode steps: 41, steps per second: 14, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.317 [0.000, 1.000], mean observation: -0.030 [-2.846, 3.677], loss: 2.719706, mean_absolute_error: 6.256202, mean_q: 11.643607\n",
      "Episode 501 reward: 33.0\n",
      "  6108/100000: episode: 501, duration: 2.123s, episode steps: 33, steps per second: 16, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.303 [0.000, 1.000], mean observation: -0.024 [-2.487, 3.286], loss: 2.968869, mean_absolute_error: 6.298524, mean_q: 11.658777\n",
      "Episode 502 reward: 38.0\n",
      "  6146/100000: episode: 502, duration: 3.019s, episode steps: 38, steps per second: 13, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: -0.030 [-2.684, 3.537], loss: 2.543152, mean_absolute_error: 6.443800, mean_q: 12.063321\n",
      "Episode 503 reward: 43.0\n",
      "  6189/100000: episode: 503, duration: 3.130s, episode steps: 43, steps per second: 14, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.349 [0.000, 1.000], mean observation: -0.011 [-2.497, 3.317], loss: 2.930904, mean_absolute_error: 6.449355, mean_q: 11.997352\n",
      "Episode 504 reward: 38.0\n",
      "  6227/100000: episode: 504, duration: 2.643s, episode steps: 38, steps per second: 14, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.395 [0.000, 1.000], mean observation: -0.027 [-1.742, 2.120], loss: 2.020979, mean_absolute_error: 6.299092, mean_q: 11.835762\n",
      "Episode 505 reward: 35.0\n",
      "  6262/100000: episode: 505, duration: 2.966s, episode steps: 35, steps per second: 12, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.314 [0.000, 1.000], mean observation: -0.019 [-2.494, 3.283], loss: 2.259716, mean_absolute_error: 6.313913, mean_q: 11.796994\n",
      "Episode 506 reward: 40.0\n",
      "  6302/100000: episode: 506, duration: 3.356s, episode steps: 40, steps per second: 12, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: -0.084 [-3.059, 3.682], loss: 2.295277, mean_absolute_error: 6.322450, mean_q: 11.832426\n",
      "Episode 507 reward: 67.0\n",
      "  6369/100000: episode: 507, duration: 5.166s, episode steps: 67, steps per second: 13, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.095 [-0.897, 0.604], loss: 1.991787, mean_absolute_error: 6.219628, mean_q: 11.676496\n",
      "Episode 508 reward: 45.0\n",
      "  6414/100000: episode: 508, duration: 3.270s, episode steps: 45, steps per second: 14, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.086 [-2.929, 3.480], loss: 2.179299, mean_absolute_error: 6.259462, mean_q: 11.706029\n",
      "Episode 509 reward: 40.0\n",
      "  6454/100000: episode: 509, duration: 2.727s, episode steps: 40, steps per second: 15, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.325 [0.000, 1.000], mean observation: -0.080 [-2.728, 3.258], loss: 1.737357, mean_absolute_error: 6.305107, mean_q: 11.872121\n",
      "Episode 510 reward: 51.0\n",
      "  6505/100000: episode: 510, duration: 3.358s, episode steps: 51, steps per second: 15, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: -0.085 [-2.892, 3.303], loss: 1.983748, mean_absolute_error: 6.317683, mean_q: 11.890635\n",
      "Episode 511 reward: 42.0\n",
      "  6547/100000: episode: 511, duration: 2.909s, episode steps: 42, steps per second: 14, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: -0.018 [-2.322, 3.081], loss: 2.161864, mean_absolute_error: 6.234669, mean_q: 11.701259\n",
      "Episode 512 reward: 44.0\n",
      "  6591/100000: episode: 512, duration: 3.763s, episode steps: 44, steps per second: 12, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.341 [0.000, 1.000], mean observation: -0.118 [-2.723, 2.975], loss: 1.905313, mean_absolute_error: 6.254600, mean_q: 11.749241\n",
      "Episode 513 reward: 44.0\n",
      "  6635/100000: episode: 513, duration: 4.462s, episode steps: 44, steps per second: 10, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.124 [-0.953, 0.552], loss: 1.884776, mean_absolute_error: 6.351755, mean_q: 11.932721\n",
      "Episode 514 reward: 48.0\n",
      "  6683/100000: episode: 514, duration: 4.303s, episode steps: 48, steps per second: 11, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.123 [-0.680, 0.438], loss: 1.681214, mean_absolute_error: 6.303080, mean_q: 11.887798\n",
      "Episode 515 reward: 72.0\n",
      "  6755/100000: episode: 515, duration: 6.649s, episode steps: 72, steps per second: 11, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.057 [-2.641, 2.768], loss: 1.759274, mean_absolute_error: 6.219645, mean_q: 11.704999\n",
      "Episode 516 reward: 46.0\n",
      "  6801/100000: episode: 516, duration: 4.576s, episode steps: 46, steps per second: 10, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.111 [-0.869, 0.588], loss: 1.797046, mean_absolute_error: 6.207500, mean_q: 11.653321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 517 reward: 46.0\n",
      "  6847/100000: episode: 517, duration: 4.022s, episode steps: 46, steps per second: 11, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.107 [-0.795, 0.425], loss: 1.532827, mean_absolute_error: 6.228208, mean_q: 11.713450\n",
      "Episode 518 reward: 36.0\n",
      "  6883/100000: episode: 518, duration: 4.017s, episode steps: 36, steps per second: 9, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.144 [-0.875, 0.542], loss: 1.673975, mean_absolute_error: 6.265778, mean_q: 11.810287\n",
      "Episode 519 reward: 39.0\n",
      "  6922/100000: episode: 519, duration: 4.455s, episode steps: 39, steps per second: 9, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.124 [-0.908, 0.232], loss: 1.633194, mean_absolute_error: 6.208171, mean_q: 11.700151\n",
      "Episode 520 reward: 44.0\n",
      "  6966/100000: episode: 520, duration: 4.843s, episode steps: 44, steps per second: 9, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.106 [-0.985, 0.432], loss: 1.607143, mean_absolute_error: 6.247975, mean_q: 11.762673\n",
      "Episode 521 reward: 57.0\n",
      "  7023/100000: episode: 521, duration: 5.585s, episode steps: 57, steps per second: 10, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.876, 0.433], loss: 1.368445, mean_absolute_error: 6.242196, mean_q: 11.817068\n",
      "Episode 522 reward: 44.0\n",
      "  7067/100000: episode: 522, duration: 3.094s, episode steps: 44, steps per second: 14, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.132 [-0.687, 0.428], loss: 1.413343, mean_absolute_error: 6.288831, mean_q: 11.914298\n",
      "Episode 523 reward: 41.0\n",
      "  7108/100000: episode: 523, duration: 2.920s, episode steps: 41, steps per second: 14, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.131 [-0.727, 0.374], loss: 1.742780, mean_absolute_error: 6.303867, mean_q: 11.857298\n",
      "Episode 524 reward: 66.0\n",
      "  7174/100000: episode: 524, duration: 5.110s, episode steps: 66, steps per second: 13, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.103 [-0.739, 0.385], loss: 1.976620, mean_absolute_error: 6.319101, mean_q: 11.834715\n",
      "Episode 525 reward: 42.0\n",
      "  7216/100000: episode: 525, duration: 3.198s, episode steps: 42, steps per second: 13, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.148 [-0.748, 0.377], loss: 1.344916, mean_absolute_error: 6.335935, mean_q: 11.983150\n",
      "Episode 526 reward: 53.0\n",
      "  7269/100000: episode: 526, duration: 3.879s, episode steps: 53, steps per second: 14, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.124 [-0.889, 0.302], loss: 1.451333, mean_absolute_error: 6.365543, mean_q: 12.052161\n",
      "Episode 527 reward: 59.0\n",
      "  7328/100000: episode: 527, duration: 4.151s, episode steps: 59, steps per second: 14, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.142 [-0.799, 0.405], loss: 1.271346, mean_absolute_error: 6.387431, mean_q: 12.135458\n",
      "Episode 528 reward: 47.0\n",
      "  7375/100000: episode: 528, duration: 3.353s, episode steps: 47, steps per second: 14, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.143 [-0.686, 0.345], loss: 1.527524, mean_absolute_error: 6.459165, mean_q: 12.221996\n",
      "Episode 529 reward: 54.0\n",
      "  7429/100000: episode: 529, duration: 4.857s, episode steps: 54, steps per second: 11, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.130 [-0.700, 0.417], loss: 1.603224, mean_absolute_error: 6.544397, mean_q: 12.348372\n",
      "Episode 530 reward: 48.0\n",
      "  7477/100000: episode: 530, duration: 4.431s, episode steps: 48, steps per second: 11, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.152 [-0.769, 0.183], loss: 1.555044, mean_absolute_error: 6.577012, mean_q: 12.463924\n",
      "Episode 531 reward: 82.0\n",
      "  7559/100000: episode: 531, duration: 6.584s, episode steps: 82, steps per second: 12, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.107 [-0.469, 0.871], loss: 1.325034, mean_absolute_error: 6.595766, mean_q: 12.545854\n",
      "Episode 532 reward: 50.0\n",
      "  7609/100000: episode: 532, duration: 3.667s, episode steps: 50, steps per second: 14, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.126 [-0.406, 0.854], loss: 1.754487, mean_absolute_error: 6.647961, mean_q: 12.576916\n",
      "Episode 533 reward: 129.0\n",
      "  7738/100000: episode: 533, duration: 10.007s, episode steps: 129, steps per second: 13, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.124 [-0.791, 0.581], loss: 1.725026, mean_absolute_error: 6.749018, mean_q: 12.787038\n",
      "Episode 534 reward: 84.0\n",
      "  7822/100000: episode: 534, duration: 6.335s, episode steps: 84, steps per second: 13, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.069 [-0.466, 1.086], loss: 1.494724, mean_absolute_error: 6.797192, mean_q: 12.937885\n",
      "Episode 535 reward: 61.0\n",
      "  7883/100000: episode: 535, duration: 4.503s, episode steps: 61, steps per second: 14, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.143 [-0.506, 1.035], loss: 1.703384, mean_absolute_error: 6.909322, mean_q: 13.072694\n",
      "Episode 536 reward: 68.0\n",
      "  7951/100000: episode: 536, duration: 5.073s, episode steps: 68, steps per second: 13, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.160 [-0.795, 0.377], loss: 1.658841, mean_absolute_error: 7.020596, mean_q: 13.292012\n",
      "Episode 537 reward: 34.0\n",
      "  7985/100000: episode: 537, duration: 2.748s, episode steps: 34, steps per second: 12, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.131 [-0.415, 0.793], loss: 1.464079, mean_absolute_error: 7.036023, mean_q: 13.369474\n",
      "Episode 538 reward: 44.0\n",
      "  8029/100000: episode: 538, duration: 3.331s, episode steps: 44, steps per second: 13, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.102 [-0.408, 0.736], loss: 1.917819, mean_absolute_error: 7.077075, mean_q: 13.412771\n",
      "Episode 539 reward: 83.0\n",
      "  8112/100000: episode: 539, duration: 6.690s, episode steps: 83, steps per second: 12, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.081 [-0.558, 0.944], loss: 1.764001, mean_absolute_error: 7.099425, mean_q: 13.469403\n",
      "Episode 540 reward: 52.0\n",
      "  8164/100000: episode: 540, duration: 4.105s, episode steps: 52, steps per second: 13, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.159 [-0.621, 1.107], loss: 1.464975, mean_absolute_error: 7.181125, mean_q: 13.642034\n",
      "Episode 541 reward: 43.0\n",
      "  8207/100000: episode: 541, duration: 3.343s, episode steps: 43, steps per second: 13, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.137 [-0.753, 1.245], loss: 2.257578, mean_absolute_error: 7.320349, mean_q: 13.802592\n",
      "Episode 542 reward: 83.0\n",
      "  8290/100000: episode: 542, duration: 6.617s, episode steps: 83, steps per second: 13, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.109 [-0.666, 0.891], loss: 1.831975, mean_absolute_error: 7.294492, mean_q: 13.813322\n",
      "Episode 543 reward: 57.0\n",
      "  8347/100000: episode: 543, duration: 6.462s, episode steps: 57, steps per second: 9, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.186 [-0.366, 0.932], loss: 1.742656, mean_absolute_error: 7.410011, mean_q: 14.037144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 544 reward: 92.0\n",
      "  8439/100000: episode: 544, duration: 10.679s, episode steps: 92, steps per second: 9, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.102 [-0.525, 1.065], loss: 2.018662, mean_absolute_error: 7.449530, mean_q: 14.075413\n",
      "Episode 545 reward: 44.0\n",
      "  8483/100000: episode: 545, duration: 4.067s, episode steps: 44, steps per second: 11, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.133 [-0.591, 1.114], loss: 1.607473, mean_absolute_error: 7.461331, mean_q: 14.182372\n",
      "Episode 546 reward: 48.0\n",
      "  8531/100000: episode: 546, duration: 5.299s, episode steps: 48, steps per second: 9, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.151 [-0.397, 1.060], loss: 2.053773, mean_absolute_error: 7.612343, mean_q: 14.424191\n",
      "Episode 547 reward: 61.0\n",
      "  8592/100000: episode: 547, duration: 5.976s, episode steps: 61, steps per second: 10, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.112 [-0.540, 0.913], loss: 2.182694, mean_absolute_error: 7.643160, mean_q: 14.483923\n",
      "Episode 548 reward: 62.0\n",
      "  8654/100000: episode: 548, duration: 7.490s, episode steps: 62, steps per second: 8, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.077 [-0.686, 0.946], loss: 2.248395, mean_absolute_error: 7.549259, mean_q: 14.290736\n",
      "Episode 549 reward: 50.0\n",
      "  8704/100000: episode: 549, duration: 8.070s, episode steps: 50, steps per second: 6, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.165 [-0.393, 1.330], loss: 2.254914, mean_absolute_error: 7.790115, mean_q: 14.744977\n",
      "Episode 550 reward: 59.0\n",
      "  8763/100000: episode: 550, duration: 9.177s, episode steps: 59, steps per second: 6, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.147 [-0.520, 1.038], loss: 1.881582, mean_absolute_error: 7.782002, mean_q: 14.743654\n",
      "Episode 551 reward: 59.0\n",
      "  8822/100000: episode: 551, duration: 7.839s, episode steps: 59, steps per second: 8, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.031 [-0.405, 0.639], loss: 1.717866, mean_absolute_error: 7.781152, mean_q: 14.803032\n",
      "Episode 552 reward: 22.0\n",
      "  8844/100000: episode: 552, duration: 2.347s, episode steps: 22, steps per second: 9, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.578, 0.977], loss: 2.468666, mean_absolute_error: 8.034774, mean_q: 15.216514\n",
      "Episode 553 reward: 17.0\n",
      "  8861/100000: episode: 553, duration: 1.755s, episode steps: 17, steps per second: 10, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.120 [-0.371, 0.990], loss: 2.469937, mean_absolute_error: 7.997166, mean_q: 15.204425\n",
      "Episode 554 reward: 42.0\n",
      "  8903/100000: episode: 554, duration: 4.362s, episode steps: 42, steps per second: 10, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.042 [-0.429, 0.647], loss: 2.201953, mean_absolute_error: 7.977317, mean_q: 15.174231\n",
      "Episode 555 reward: 22.0\n",
      "  8925/100000: episode: 555, duration: 2.736s, episode steps: 22, steps per second: 8, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.088 [-0.416, 0.676], loss: 1.872876, mean_absolute_error: 7.880089, mean_q: 15.040043\n",
      "Episode 556 reward: 29.0\n",
      "  8954/100000: episode: 556, duration: 2.573s, episode steps: 29, steps per second: 11, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.076 [-0.376, 0.687], loss: 2.022964, mean_absolute_error: 8.042486, mean_q: 15.304300\n",
      "Episode 557 reward: 26.0\n",
      "  8980/100000: episode: 557, duration: 2.354s, episode steps: 26, steps per second: 11, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.103 [-0.544, 0.853], loss: 2.234638, mean_absolute_error: 8.063689, mean_q: 15.339579\n",
      "Episode 558 reward: 25.0\n",
      "  9005/100000: episode: 558, duration: 2.288s, episode steps: 25, steps per second: 11, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.106 [-0.365, 0.756], loss: 2.662857, mean_absolute_error: 7.984288, mean_q: 15.161117\n",
      "Episode 559 reward: 32.0\n",
      "  9037/100000: episode: 559, duration: 3.438s, episode steps: 32, steps per second: 9, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.083 [-0.373, 0.970], loss: 2.576867, mean_absolute_error: 8.189133, mean_q: 15.548834\n",
      "Episode 560 reward: 37.0\n",
      "  9074/100000: episode: 560, duration: 4.553s, episode steps: 37, steps per second: 8, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.128 [-0.383, 0.774], loss: 1.883671, mean_absolute_error: 8.128060, mean_q: 15.463655\n",
      "Episode 561 reward: 55.0\n",
      "  9129/100000: episode: 561, duration: 5.522s, episode steps: 55, steps per second: 10, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.106 [-0.551, 0.936], loss: 1.973514, mean_absolute_error: 8.162664, mean_q: 15.470806\n",
      "Episode 562 reward: 39.0\n",
      "  9168/100000: episode: 562, duration: 3.752s, episode steps: 39, steps per second: 10, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.126 [-0.839, 1.094], loss: 2.008464, mean_absolute_error: 8.312490, mean_q: 15.825712\n",
      "Episode 563 reward: 24.0\n",
      "  9192/100000: episode: 563, duration: 2.437s, episode steps: 24, steps per second: 10, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.088 [-0.431, 0.709], loss: 2.166604, mean_absolute_error: 8.286114, mean_q: 15.761932\n",
      "Episode 564 reward: 45.0\n",
      "  9237/100000: episode: 564, duration: 5.138s, episode steps: 45, steps per second: 9, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.123 [-0.767, 1.128], loss: 2.501340, mean_absolute_error: 8.354479, mean_q: 15.869278\n",
      "Episode 565 reward: 40.0\n",
      "  9277/100000: episode: 565, duration: 4.182s, episode steps: 40, steps per second: 10, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.169 [-0.353, 0.970], loss: 2.836225, mean_absolute_error: 8.375746, mean_q: 15.816900\n",
      "Episode 566 reward: 44.0\n",
      "  9321/100000: episode: 566, duration: 4.162s, episode steps: 44, steps per second: 11, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.196 [-0.375, 1.144], loss: 2.478494, mean_absolute_error: 8.467542, mean_q: 16.031807\n",
      "Episode 567 reward: 52.0\n",
      "  9373/100000: episode: 567, duration: 4.538s, episode steps: 52, steps per second: 11, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.163 [-0.520, 1.082], loss: 2.921911, mean_absolute_error: 8.524957, mean_q: 16.074093\n",
      "Episode 568 reward: 37.0\n",
      "  9410/100000: episode: 568, duration: 3.704s, episode steps: 37, steps per second: 10, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.100 [-0.441, 0.756], loss: 2.405869, mean_absolute_error: 8.495670, mean_q: 16.119989\n",
      "Episode 569 reward: 29.0\n",
      "  9439/100000: episode: 569, duration: 3.164s, episode steps: 29, steps per second: 9, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.071 [-0.585, 1.143], loss: 1.964619, mean_absolute_error: 8.391937, mean_q: 16.008425\n",
      "Episode 570 reward: 34.0\n",
      "  9473/100000: episode: 570, duration: 3.381s, episode steps: 34, steps per second: 10, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.071 [-0.364, 0.734], loss: 2.239770, mean_absolute_error: 8.546165, mean_q: 16.306133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 571 reward: 35.0\n",
      "  9508/100000: episode: 571, duration: 3.178s, episode steps: 35, steps per second: 11, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.064 [-0.586, 0.930], loss: 3.029704, mean_absolute_error: 8.721030, mean_q: 16.502420\n",
      "Episode 572 reward: 22.0\n",
      "  9530/100000: episode: 572, duration: 2.030s, episode steps: 22, steps per second: 11, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.408, 0.794], loss: 3.812685, mean_absolute_error: 8.674222, mean_q: 16.343565\n",
      "Episode 573 reward: 22.0\n",
      "  9552/100000: episode: 573, duration: 1.857s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.104 [-0.418, 0.686], loss: 2.434846, mean_absolute_error: 8.481234, mean_q: 16.152578\n",
      "Episode 574 reward: 26.0\n",
      "  9578/100000: episode: 574, duration: 2.191s, episode steps: 26, steps per second: 12, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.081 [-0.438, 0.650], loss: 2.255633, mean_absolute_error: 8.717910, mean_q: 16.641130\n",
      "Episode 575 reward: 23.0\n",
      "  9601/100000: episode: 575, duration: 1.879s, episode steps: 23, steps per second: 12, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.089 [-0.752, 1.368], loss: 3.696632, mean_absolute_error: 8.727538, mean_q: 16.488060\n",
      "Episode 576 reward: 20.0\n",
      "  9621/100000: episode: 576, duration: 1.645s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.381, 0.907], loss: 4.158331, mean_absolute_error: 8.807898, mean_q: 16.557468\n",
      "Episode 577 reward: 37.0\n",
      "  9658/100000: episode: 577, duration: 3.016s, episode steps: 37, steps per second: 12, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.076 [-0.587, 0.791], loss: 2.907272, mean_absolute_error: 8.779042, mean_q: 16.627453\n",
      "Episode 578 reward: 24.0\n",
      "  9682/100000: episode: 578, duration: 2.079s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.590, 0.876], loss: 3.213804, mean_absolute_error: 8.956649, mean_q: 16.965902\n",
      "Episode 579 reward: 48.0\n",
      "  9730/100000: episode: 579, duration: 3.964s, episode steps: 48, steps per second: 12, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.203 [-0.461, 1.110], loss: 2.457757, mean_absolute_error: 8.752771, mean_q: 16.587393\n",
      "Episode 580 reward: 48.0\n",
      "  9778/100000: episode: 580, duration: 5.158s, episode steps: 48, steps per second: 9, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.164 [-0.365, 1.003], loss: 4.385276, mean_absolute_error: 8.904067, mean_q: 16.698481\n",
      "done, took 767.862 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 38.000, steps: 38\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "from rl.callbacks import Callback\n",
    "\n",
    "class EpisodeRewardCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super(EpisodeRewardCallback, self).__init__()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        episode_reward = logs['episode_reward']\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1} reward: {episode_reward}\")\n",
    "\n",
    "# Create an instance of the callback\n",
    "episode_callback = EpisodeRewardCallback()\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "#np.random.seed(123)\n",
    "#env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=int(1e5), window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "\n",
    "dqn.fit(env, nb_steps=int(1e5), visualize=True, verbose=2,callbacks=[episode_callback])\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All episode rewards: [82.0, 33.0, 52.0, 41.0, 56.0, 40.0, 43.0, 42.0, 38.0, 30.0, 44.0, 47.0, 54.0, 49.0, 31.0, 24.0, 58.0, 55.0, 25.0, 19.0, 16.0, 20.0, 16.0, 18.0, 24.0, 12.0, 18.0, 13.0, 15.0, 15.0, 17.0, 13.0, 16.0, 12.0, 12.0, 14.0, 15.0, 10.0, 15.0, 15.0, 11.0, 13.0, 11.0, 13.0, 11.0, 10.0, 12.0, 11.0, 10.0, 13.0, 11.0, 13.0, 15.0, 12.0, 10.0, 12.0, 11.0, 11.0, 9.0, 15.0, 10.0, 13.0, 14.0, 12.0, 13.0, 12.0, 13.0, 10.0, 11.0, 12.0, 10.0, 9.0, 11.0, 9.0, 10.0, 11.0, 13.0, 11.0, 11.0, 12.0, 12.0, 12.0, 13.0, 12.0, 11.0, 11.0, 16.0, 16.0, 13.0, 16.0, 14.0, 14.0, 16.0, 13.0, 10.0, 13.0, 15.0, 12.0, 12.0, 14.0, 14.0, 15.0, 16.0, 12.0, 18.0, 18.0, 16.0, 16.0, 11.0, 14.0, 14.0, 18.0, 14.0, 12.0, 12.0, 12.0, 18.0, 18.0, 19.0, 24.0, 17.0, 13.0, 20.0, 13.0, 17.0, 20.0, 19.0, 14.0, 13.0, 15.0, 18.0, 17.0, 18.0, 17.0, 22.0, 18.0, 24.0, 22.0, 17.0, 30.0, 20.0, 28.0, 14.0, 21.0, 18.0, 35.0, 43.0, 29.0, 69.0, 40.0, 35.0, 29.0, 31.0, 36.0, 37.0, 65.0, 36.0, 53.0, 27.0, 77.0, 55.0, 106.0, 30.0, 79.0, 22.0, 27.0, 25.0, 20.0, 16.0, 19.0, 20.0, 22.0, 14.0, 18.0, 18.0, 12.0, 14.0, 18.0, 16.0, 11.0, 17.0, 12.0, 12.0, 10.0, 13.0, 14.0, 9.0, 11.0, 12.0, 13.0, 11.0, 9.0, 10.0, 10.0, 9.0, 12.0, 10.0, 10.0, 10.0, 13.0, 10.0, 10.0, 10.0, 12.0, 10.0, 10.0, 10.0, 11.0, 9.0, 10.0, 9.0, 10.0, 9.0, 10.0, 12.0, 10.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 8.0, 11.0, 8.0, 11.0, 9.0, 10.0, 9.0, 12.0, 8.0, 11.0, 9.0, 10.0, 10.0, 12.0, 10.0, 8.0, 9.0, 9.0, 9.0, 10.0, 10.0, 13.0, 10.0, 12.0, 8.0, 9.0, 12.0, 9.0, 12.0, 14.0, 11.0, 11.0, 18.0, 14.0, 13.0, 13.0, 26.0, 26.0, 55.0, 15.0, 22.0, 22.0, 16.0, 26.0, 26.0, 80.0, 58.0, 35.0, 29.0, 48.0, 23.0, 35.0, 49.0, 28.0, 26.0, 27.0, 30.0, 24.0, 23.0, 38.0, 82.0, 112.0, 27.0, 33.0, 30.0, 47.0, 31.0, 131.0, 48.0, 47.0, 40.0, 43.0, 46.0, 72.0, 61.0, 67.0, 50.0, 40.0, 43.0, 54.0, 118.0, 62.0, 86.0, 38.0, 52.0, 31.0, 94.0, 56.0, 74.0, 68.0, 156.0, 46.0, 41.0, 43.0, 43.0, 131.0, 47.0, 48.0, 71.0, 42.0, 67.0, 74.0, 32.0, 101.0, 56.0, 69.0, 68.0, 37.0, 160.0, 55.0, 60.0, 84.0, 61.0, 54.0, 44.0, 66.0, 93.0, 61.0, 73.0, 72.0, 71.0, 69.0, 49.0, 67.0, 74.0, 200.0, 57.0, 32.0, 64.0, 78.0, 34.0, 43.0, 118.0, 87.0, 71.0, 71.0, 149.0, 43.0, 60.0, 65.0, 76.0, 78.0, 67.0, 106.0, 64.0, 99.0, 102.0, 91.0, 70.0, 91.0, 74.0, 66.0, 73.0, 69.0, 44.0, 93.0, 79.0, 83.0, 45.0, 200.0, 98.0, 90.0, 107.0, 68.0, 136.0, 55.0, 90.0, 74.0, 75.0, 80.0, 64.0, 86.0, 122.0, 97.0, 81.0, 101.0, 92.0, 77.0, 200.0, 111.0, 124.0, 71.0, 52.0, 72.0, 108.0, 181.0, 118.0, 107.0, 95.0, 80.0, 111.0, 118.0, 94.0, 97.0, 100.0, 158.0, 97.0, 97.0, 179.0, 200.0, 200.0, 167.0, 120.0, 103.0, 118.0, 49.0, 188.0, 87.0, 54.0, 57.0, 137.0, 41.0, 45.0, 67.0, 74.0, 65.0, 122.0, 66.0, 51.0, 163.0, 111.0, 59.0, 44.0, 68.0, 74.0, 45.0, 55.0, 155.0, 74.0, 131.0, 187.0, 75.0, 200.0, 67.0, 51.0, 57.0, 38.0, 93.0, 51.0, 92.0, 77.0, 70.0, 200.0, 55.0, 88.0, 67.0, 146.0, 56.0, 64.0, 77.0, 48.0, 58.0, 90.0, 58.0, 134.0, 168.0, 78.0, 136.0, 57.0, 193.0, 113.0, 55.0, 186.0, 200.0, 67.0, 56.0, 72.0, 69.0, 60.0, 110.0, 70.0, 149.0, 94.0, 80.0, 89.0, 83.0, 99.0, 200.0, 82.0, 62.0, 166.0, 152.0, 141.0, 118.0, 85.0, 75.0, 79.0, 141.0, 198.0, 200.0, 64.0, 69.0, 60.0, 58.0, 66.0, 48.0, 73.0, 61.0, 74.0, 168.0, 76.0, 94.0, 200.0, 104.0, 69.0, 107.0, 68.0, 70.0, 85.0, 81.0, 79.0, 69.0, 79.0, 68.0, 103.0, 95.0, 102.0, 63.0, 71.0, 69.0, 93.0, 117.0, 200.0, 163.0, 87.0, 102.0, 96.0, 200.0, 125.0, 122.0, 106.0, 116.0, 200.0, 111.0, 193.0, 200.0, 200.0, 95.0, 200.0, 116.0, 134.0, 136.0, 193.0, 171.0, 110.0, 200.0, 197.0, 95.0, 121.0, 151.0, 101.0, 200.0, 101.0, 112.0, 137.0, 150.0, 114.0, 200.0, 120.0, 159.0, 139.0, 200.0, 132.0, 138.0, 146.0, 143.0, 164.0, 151.0, 166.0, 200.0, 131.0, 200.0, 200.0, 200.0, 200.0, 194.0, 172.0, 155.0, 200.0, 170.0, 173.0, 187.0, 200.0, 200.0, 200.0, 200.0, 200.0, 197.0, 200.0, 200.0, 200.0, 200.0, 200.0, 171.0, 200.0, 200.0, 200.0, 176.0, 200.0, 200.0, 195.0, 200.0, 175.0, 191.0, 200.0, 200.0, 184.0, 180.0, 174.0, 192.0, 182.0, 180.0, 194.0, 198.0, 200.0, 200.0, 200.0, 200.0, 200.0, 191.0, 173.0, 176.0, 174.0, 200.0, 200.0, 200.0, 182.0, 196.0, 200.0, 180.0, 200.0, 200.0, 182.0, 184.0, 200.0, 200.0, 200.0, 200.0, 200.0, 189.0, 189.0, 193.0, 200.0, 200.0, 200.0, 183.0, 195.0, 200.0, 184.0, 195.0, 200.0, 194.0, 177.0, 192.0, 172.0, 200.0, 168.0, 170.0, 200.0, 179.0, 200.0, 200.0, 183.0, 187.0, 200.0, 188.0, 200.0, 200.0, 183.0, 179.0, 177.0, 200.0, 194.0, 200.0, 200.0, 200.0, 190.0, 189.0, 200.0, 200.0, 196.0, 189.0, 200.0, 198.0, 194.0, 200.0, 200.0, 200.0, 200.0, 200.0, 181.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 194.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 184.0, 200.0, 200.0, 200.0, 197.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"All episode rewards:\", episode_callback.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(running_mean(episode_callback.episode_rewards,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(rwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA algorithm\n",
    "def SARSA_algorithm(num_episodes):\n",
    "    allowed_actions=[0,1]\n",
    "    # Variables: alpha, gamma y epsilon.\n",
    "    alpha = 0.3\n",
    "    gamma = 1\n",
    "    epsilon = 0.1\n",
    "    cont = 0\n",
    "    #Para 5000 episodios de entrenamiento\n",
    "    for i in range(num_episodes):\n",
    "        # Inicializa las variables para cada episodio\n",
    "        state=env.reset()\n",
    "        num_steps = 0\n",
    "        state = discretize(state[0])\n",
    "\n",
    "\n",
    "        # Seleccion accion \"a\" de forma epsilon-greedy\n",
    "        if epsilon< np.random.uniform():        \n",
    "            act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "            action = allowed_actions[np.argmax(act_arg)]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Inicia el episodio\n",
    "        continue_episode = True\n",
    "        while continue_episode:        \n",
    "            # Obtengo s'\n",
    "            new_state, reward, done, _,_=env.step(action)\n",
    "            new_state = discretize(new_state)\n",
    "\n",
    "            # Revisa que new_state no sea un estado terminal\n",
    "            if done:\n",
    "                # Valor q(s',a') terminal\n",
    "                q_value_next_step = 0\n",
    "            else:\n",
    "                #Obtengo a' de s' con epsilon greedy\n",
    "                if epsilon< np.random.uniform():        \n",
    "                    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "                    new_action = allowed_actions[np.argmax(act_arg)]\n",
    "                else:\n",
    "                    new_action = env.action_space.sample()\n",
    "\n",
    "                # Valor q(s',a') no terminal\n",
    "                q_value_next_step = Q_table[tuple(new_state),new_action]\n",
    "\n",
    "\n",
    "            # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*q(s',a') - q(s,a))\n",
    "            Q_table[tuple(state), action] += alpha*(reward + gamma*q_value_next_step - Q_table[tuple(state),action])\n",
    "\n",
    "            # asigna a = a' y s = s'\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "\n",
    "\n",
    "            # Parte que termina el episodio si se llega a algun estado terminal\n",
    "            if done:\n",
    "                continue_episode = False\n",
    "        cont+=1\n",
    "        if cont%100==0:\n",
    "            print(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 400 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(400)\n",
    "SARSA_algorithm(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_actions= [0,1]\n",
    "prom=[]\n",
    "prom_rand=[]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 4000 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(4000)\n",
    "SARSA_algorithm(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom=[]\n",
    "prom_rand=[]\n",
    "allowed_actions= [0,1]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
