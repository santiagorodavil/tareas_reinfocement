{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8fcea4-7982-43e1-b52c-a98d5574c7b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tarea 3 Reinforcement Learning\n",
    "### (Montecarlo, SARSA, Q-Learning)\n",
    "Juan Pablo Reyes Fajardo\n",
    "\n",
    "Santiago Rodríguez Ávila\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da0d5-cdc1-4825-90c0-6ad7a3e2c003",
   "metadata": {},
   "source": [
    "Para esta tarea se quiere estudiar el comportamiento de los algoritmos de aprendizaje por refuerzo de: Montecarlo, SARSA y Q-learning. Se observarán características, comportamiento y composición de estos. Se hablará de políticas on y off (que el agente sigue la misma política a evaluar o una diferente), todo esto partiendo del escenario de la tarea anterior, el juego de escaleras y serpientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2308a3-7d67-4fac-a21c-c81bce01f8cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Planteamiento del MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbbf2f-641a-406d-ba1c-67609a91a3f2",
   "metadata": {},
   "source": [
    "En esta parte, se pedía hacer las respectivas modificaciones con el fin de que el módulo de python creado tuviera lo siguiente:\n",
    "* Dada una acción en un estado, retornar la recompensa y el estado resultante en esa acción. Considere el caso especial del estado terminal.\n",
    "    \n",
    "* Que sea posible ejecutar una política arbitraria.\n",
    "\n",
    "Para esto, se creará un escenario y se utilizará para hacer las pruebas respectivas que se pedían anteriormente. Se procede a crear el escenario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f87e32-e6dc-47f7-a488-b4d0e32683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from escalerasyserpientes import EscalerasSerpientes\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ec77cf-fbea-471d-9f39-0b3eb1463381",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EscalerasSerpientes([80,100],[23,37,45,67,89],-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b5567-809b-415a-837b-52842c8eed36",
   "metadata": {},
   "source": [
    "Luego de crearlo, se mostrará la forma en que el MDP retorna la información luego de realizar un paso, el primer caso será para mostrar la forma de retorno, el segundo va a ser teniendo en cuenta algún estado terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a882c9f5-d2a8-4346-965e-c82462b185c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, -0.01, 5, False)\n"
     ]
    }
   ],
   "source": [
    "# Muestra de como funciona el MDP\n",
    "state_prueba=16\n",
    "action_prueba=5\n",
    "print(es.step(state_prueba, action_prueba, random=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3954126-d5ba-42ec-85fb-792bf933f9d3",
   "metadata": {},
   "source": [
    "La estructura de retorno del MDP es la siguiente:\n",
    "Casilla de llegada, valor de la recompensa obtenida en el paso, acción tomada en ese estado (valor del dado), variable que muestra si el estado al que se llegó es un estado terminal. En ese orden de ideas, la prueba teniendo en cuenta algún estado terminal sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b8008a-e7fa-4f4a-bccd-775c3c5a51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Azul', 1.0, None, True)\n"
     ]
    }
   ],
   "source": [
    "# Muestra de como funciona el MDP en estado terminal\n",
    "state_prueba=100\n",
    "action_prueba=-4\n",
    "print(es.step(state_prueba, action_prueba, random=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f399a77-5c12-4f88-b2a7-77fe360cb860",
   "metadata": {},
   "source": [
    "Aquí se puede ver que cuando el agente llega a un estado terminal e intenta avanzar, el escenario no le permite saltar hacia ningún lado, en este caso, se recibe \"Azul\"(estado terminal bueno), la recompensa del estado terminal y la variable que informa que se llegó a un estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d45b0-3e0e-436e-b0cf-39a2ae74aa1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Algotimo Montecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe92caa-08a9-4a08-973f-92c26b00ba00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algoritmo SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1889a-e463-42bb-afb4-450d6fcf7036",
   "metadata": {},
   "source": [
    "En esta parte de la tarea se va a analizar y desarrollar el algoritmo SARSA. Como se pudo ver en clase, este es un algoritmo *on-policy*, esto quiere decir que el algoritmo aprende directamente de la política actual del agente. Otra cosa que vale la pena nombrar es que este algoritmo utiliza la **función Q** para actualizar el valor del par estado, acción a evaluar. La norma de actualización de SARSA es la siguiente:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha*(R+\\gamma*Q(s',a') - Q(s,a))$$\n",
    "\n",
    "Donde $s'$ es el estado siguiente al que se llega luego de elegir $a$ de forma $\\epsilon-greedy$ , y $a'$ es la acción $\\epsilon-greedy$ elegida desde el estado $s'$. Finalmente, se repite esta regla de actualización actualizando los valores de forma: $s\\leftarrow s'$ y $a\\leftarrow a'$ hasta que se llegue a un estado terminal y ahí se termina el episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c308966-a519-441d-a87a-b5739771bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA algorithm\n",
    "\n",
    "# Variables: alpha, gamma y epsilon.\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#Para 10000 episodios de entrenamiento\n",
    "for i in range(10000):\n",
    "    # Inicializa las variables para cada episodio\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    # Seleccion accion \"a\" de forma epsilon-greedy\n",
    "    if epsilon< np.random.uniform():        \n",
    "        act_arg = np.array([es.q_values[(state, act)] for act in es.allowed_actions[state]])\n",
    "        action = es.allowed_actions[state][np.argmax(act_arg)]\n",
    "    else:\n",
    "        actions = es.allowed_actions[state]\n",
    "        action = actions[np.random.randint(0,2)]\n",
    "    \n",
    "    # Inicia el episodio\n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "        \n",
    "        # Obtengo s'\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        # Revisa que new_state no sea un estado terminal\n",
    "        if type(new_state) is str:\n",
    "            # Valor q(s',a') terminal\n",
    "            q_value_next_step = 0\n",
    "        else:\n",
    "            #Obtengo a' de s' con epsilon greedy\n",
    "            if np.random.uniform()< epsilon:     \n",
    "                actions = es.allowed_actions[new_state]\n",
    "                new_action = actions[np.random.randint(0,2)]\n",
    "            else:\n",
    "                act_arg = np.array([es.q_values[(new_state, act)] for act in es.allowed_actions[state]])\n",
    "                new_action = es.allowed_actions[new_state][np.argmax(act_arg)]\n",
    "\n",
    "            # Valor q(s',a') no terminal\n",
    "            q_value_next_step = es.q_values[(new_state,new_action)]\n",
    "        \n",
    "        \n",
    "        # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*q(s',a') - q(s,a))\n",
    "        es.q_values[(state, action)] += alpha*(reward + gamma*q_value_next_step - es.q_values[(state,action)])\n",
    "        \n",
    "        # asigna a = a' y s = s'\n",
    "        state = new_state\n",
    "        action = new_action\n",
    "      \n",
    "\n",
    "        # Parte que termina el episodio si se llega a algun estado terminal\n",
    "        if done:\n",
    "            continue_episode = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0df1d8f-17ae-447a-aa77-ca9a3f1793d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Ad', 2: 'At', 3: 'Ad', 4: 'Ad', 5: 'Ad', 6: 'Ad', 7: 'At', 8: 'Ad', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'Ad', 16: 'Ad', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: 'Ad', 22: 'At', 23: 'Ad', 24: 'At', 25: 'At', 26: 'Ad', 27: 'Ad', 28: 'At', 29: 'At', 30: 'Ad', 31: 'Ad', 32: 'At', 33: 'At', 34: 'Ad', 35: 'Ad', 36: 'At', 37: 'Ad', 38: 'Ad', 39: 'At', 40: 'At', 41: 'Ad', 42: 'Ad', 43: 'Ad', 44: 'Ad', 45: 'Ad', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'Ad', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'At', 57: 'At', 58: 'Ad', 59: 'Ad', 60: 'At', 61: 'At', 62: 'Ad', 63: 'Ad', 64: 'Ad', 65: 'At', 66: 'Ad', 67: 'At', 68: 'Ad', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'Ad', 73: 'Ad', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'Ad', 78: 'Ad', 79: 'Ad', 80: 'Ad', 81: 'At', 82: 'At', 83: 'Ad', 84: 'At', 85: 'Ad', 86: 'At', 87: 'At', 88: 'At', 89: 'Ad', 90: 'At', 91: 'Ad', 92: 'Ad', 93: 'Ad', 94: 'Ad', 95: 'Ad', 96: 'At', 97: 'Ad', 98: 'Ad', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "SARSA_policy = {}\n",
    "for i in es.states:\n",
    "    max_val = np.argmax([es.q_values[(i,act)] for act in es.allowed_actions[i]])\n",
    "    SARSA_policy[i] = es.allowed_actions[i][max_val]\n",
    "print(SARSA_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed77570-350d-4562-ade4-0cd30b0ff6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de politica en SARSA\n",
    "num_episodios = 1000\n",
    "win_per = 0\n",
    "prom_step = 0\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    states = []\n",
    "    \n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "            \n",
    "        # Toma la accion de la politica\n",
    "        action = SARSA_policy[state]\n",
    "        # Da un paso en direccion de la politica\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        state = new_state\n",
    "        states.append(state)\n",
    "        num_steps+=1\n",
    "        \n",
    "        if done or num_steps >500:\n",
    "            continue_episode = False\n",
    "            if state == 'Azul':\n",
    "                win_per+=1\n",
    "    #print(states)\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)\n",
    "    \n",
    "#win_per = win_per*100/num_episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3863396f-190e-4b19-9621-7f5373d0f07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio pasos por episodio:  37.465000000000046\n",
      "Porcentaje victorias:  84.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Promedio pasos por episodio: \",prom_step)\n",
    "print(\"Porcentaje victorias: \", win_per*100/num_episodios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac4434-724c-4115-92d2-91cadaa223d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0273a5-bb04-4703-9fb8-9d31cfcc698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning algorithm\n",
    "\n",
    "# Variables: alpha , gamma y epsilon.\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "#Para 1000 episodio\n",
    "for i in range(10000):\n",
    "    # Inicializa las variables para cada episodio\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    \n",
    "    # Inicia el episodio\n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "\n",
    "        # Tomar acción de forma epsilon-greedy\n",
    "        if np.random.uniform()<epsilon:\n",
    "            # paso aleatorio\n",
    "            A = np.random.randint(0,2)\n",
    "            actions = es.allowed_actions[1]\n",
    "            action = actions[A]\n",
    "\n",
    "        else:\n",
    "            # paso con accion greedy\n",
    "            act_arg = np.array([es.q_values[(state, act)] for act in es.allowed_actions[state]])\n",
    "            action = es.allowed_actions[state][np.argmax(act_arg)]\n",
    "        \n",
    "        # Obtengo s'\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "\n",
    "        # Valor de max q(s',a). Si es terminal el estado, el valor es 0\n",
    "        if type(new_state) is str:\n",
    "            # Valor max q(s',a) terminal\n",
    "            max_q_val = 0\n",
    "        else:\n",
    "            # Valor max q(s',a) no terminal\n",
    "            action_arg = np.array([es.q_values[(new_state, act)] for act in es.allowed_actions[state]])\n",
    "            new_action = es.allowed_actions[new_state][np.argmax(action_arg)]\n",
    "            max_q_val = es.q_values[(new_state,new_action)]\n",
    "        \n",
    "        # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*max q(s',a) - q(s,a))\n",
    "        es.q_values[(state, action)] += alpha*(reward + gamma*max_q_val - es.q_values[(state,action)])\n",
    "        \n",
    "        # asigna s = s'\n",
    "        state = new_state\n",
    "     \n",
    "\n",
    "        # Parte que termina el episodio si se llega a algun estado terminal\n",
    "        if done:\n",
    "            continue_episode = False\n",
    "\n",
    "                \n",
    "    # Promedio de pasos por episodio\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7298b6-359b-4d5c-ae7a-d298c55f294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'At', 2: 'At', 3: 'At', 4: 'At', 5: 'Ad', 6: 'At', 7: 'At', 8: 'Ad', 9: 'Ad', 10: 'Ad', 11: 'Ad', 12: 'Ad', 13: 'Ad', 14: 'Ad', 15: 'At', 16: 'Ad', 17: 'At', 18: 'At', 19: 'At', 20: 'At', 21: 'Ad', 22: 'At', 23: 'Ad', 24: 'Ad', 25: 'Ad', 26: 'At', 27: 'Ad', 28: 'Ad', 29: 'Ad', 30: 'At', 31: 'At', 32: 'At', 33: 'At', 34: 'At', 35: 'At', 36: 'At', 37: 'Ad', 38: 'Ad', 39: 'At', 40: 'Ad', 41: 'Ad', 42: 'Ad', 43: 'Ad', 44: 'Ad', 45: 'Ad', 46: 'Ad', 47: 'Ad', 48: 'Ad', 49: 'Ad', 50: 'Ad', 51: 'Ad', 52: 'Ad', 53: 'Ad', 54: 'Ad', 55: 'Ad', 56: 'At', 57: 'At', 58: 'Ad', 59: 'Ad', 60: 'At', 61: 'At', 62: 'Ad', 63: 'At', 64: 'Ad', 65: 'At', 66: 'Ad', 67: 'At', 68: 'Ad', 69: 'Ad', 70: 'Ad', 71: 'Ad', 72: 'At', 73: 'Ad', 74: 'Ad', 75: 'Ad', 76: 'Ad', 77: 'Ad', 78: 'Ad', 79: 'Ad', 80: 'Ad', 81: 'At', 82: 'At', 83: 'Ad', 84: 'At', 85: 'At', 86: 'At', 87: 'At', 88: 'At', 89: 'Ad', 90: 'Ad', 91: 'Ad', 92: 'Ad', 93: 'Ad', 94: 'Ad', 95: 'Ad', 96: 'At', 97: 'Ad', 98: 'Ad', 99: 'Ad', 100: 'Ad'}\n"
     ]
    }
   ],
   "source": [
    "Qlearning_policy = {}\n",
    "for i in es.states:\n",
    "    #print(i)\n",
    "    max_val = np.argmax([es.q_values[(i,act)] for act in es.allowed_actions[i]])\n",
    "    Qlearning_policy[i] = es.allowed_actions[i][max_val]\n",
    "print(Qlearning_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99fa169-dd53-4988-9fd2-c980e4d055f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de polica Qlearning\n",
    "num_episodios = 1000\n",
    "win_per = 0\n",
    "prom_step = 0\n",
    "\n",
    "for i in range(num_episodios):\n",
    "    num_steps = 0\n",
    "    state = es.states[0]\n",
    "    states = []\n",
    "    \n",
    "    continue_episode = True\n",
    "    while continue_episode:\n",
    "        # cambia al estado de cima escalera o cola de serpiente\n",
    "        if state in es.snakes:\n",
    "            state = es.state_values[es.snakes[state]]\n",
    "        elif state in es.stairs:\n",
    "            state = es.state_values[es.stairs[state]]\n",
    "            \n",
    "        # Toma la accion de la politica\n",
    "        action = Qlearning_policy[state]\n",
    "        # Da un paso en direccion de la politica\n",
    "        new_state, reward, _, done = es.step(state, action, random=True)\n",
    "        \n",
    "        state = new_state\n",
    "        states.append(state)\n",
    "        num_steps+=1\n",
    "        #print(action, state)\n",
    "        \n",
    "        if done or num_steps >500:\n",
    "            continue_episode = False\n",
    "            if state == 'Azul':\n",
    "                win_per+=1\n",
    "    #print(states)\n",
    "    prom_step += 1/(i+1) *(num_steps - prom_step)\n",
    "    \n",
    "#win_per = win_per*100/num_episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61abd953-8099-4ef8-baf4-595e229ba189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio pasos por episodio: 73.346\n",
      "Porcentaje victorias: 93.7\n"
     ]
    }
   ],
   "source": [
    "print('Promedio pasos por episodio:',prom_step)\n",
    "print('Porcentaje victorias:' ,win_per*100/num_episodios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
