{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247c5a90-c5d3-4661-9d2c-88f2ea2f537a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Reinforcement Learning: Taller 4**\n",
    "## Estudiantes: Juan Pablo Reyes Fajardo y Santiago Rodríguez Ávila "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd367f16-d8ba-4db1-98f4-b147dc419455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885daafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de6862",
   "metadata": {},
   "source": [
    "# 1. RL Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion Auxiliar para diccionarios\n",
    "def key_max(d):\n",
    "        return max(d.items(), key=operator.itemgetter(1))\n",
    "def key_min(d):\n",
    "        return min(d.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80afee",
   "metadata": {},
   "source": [
    "Política $\\epsilon$ - Greedy \n",
    "\n",
    "(Útil más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q_, state,epsilon=0.1):\n",
    "    rand = np.random.uniform()\n",
    "    if rand>epsilon:\n",
    "        return key_max(Q_[state])[0],1-epsilon\n",
    "    else:\n",
    "        return key_min(Q_[state])[0],epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b04298",
   "metadata": {},
   "source": [
    "## Discretización\n",
    "\n",
    "Inicialmente se realizan múltiples experimentos para determinar límites razonables para las variables a discretizar (aquellas cuyo espacio de observación es infinito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ff1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocidades_absolutas_maximas={\"Lineal\":[],\"Angular\":[]}\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "abs_lineal=abs(observation[1])\n",
    "abs_angular=abs(observation[3])\n",
    "\n",
    "for _ in range(int(1e6)):\n",
    "    \n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if abs(observation[1])>abs_lineal:\n",
    "        abs_lineal=abs(observation[1])\n",
    "    if abs(observation[3])>abs_angular:\n",
    "        abs_angular=abs(observation[3])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        velocidades_absolutas_maximas[\"Lineal\"].append(abs_lineal)\n",
    "        velocidades_absolutas_maximas[\"Angular\"].append(abs_angular)\n",
    "\n",
    "env.close()\n",
    "\n",
    "vel_lin_abs_max=np.mean(velocidades_absolutas_maximas[\"Lineal\"])\n",
    "vel_ang_abs_max=np.mean(velocidades_absolutas_maximas[\"Angular\"])\n",
    "print(f'Promedio de velocidad lineal absoluta máxima: \\\n",
    "      {vel_lin_abs_max} \\\n",
    "      \\nPromedio de velocidad angular absoluta máxima: \\\n",
    "      {vel_ang_abs_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64390d",
   "metadata": {},
   "source": [
    "Discretización de estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e262a-a666-493f-b6e9-b952ebe4100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Límites del espacio de observación del MDP real\n",
    "cart_high_var = env.observation_space.high\n",
    "cart_low_var = env.observation_space.low\n",
    "\n",
    "# Espacios de observación discretizados\n",
    "observation_space_discrete_400=[np.linspace(cart_low_var[0], cart_high_var[0], num= 5),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 4),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 5),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 4)]\\\n",
    "\n",
    "observation_space_discrete_4096=[np.linspace(cart_low_var[0], cart_high_var[0], num= 8),\\\n",
    "                            np.linspace(-vel_lin_abs_max, vel_lin_abs_max, num= 8),\\\n",
    "                            np.linspace(cart_low_var[2], cart_high_var[2], num = 8),\\\n",
    "                            np.linspace(-vel_ang_abs_max, vel_ang_abs_max, num= 8)]\n",
    "\n",
    "# Uso de iteradores para obtener todos los estados a partir del espacio de obsevación discreto\n",
    "states_400=list(itertools.product(*observation_space_discrete_400))\n",
    "states_4096=list(itertools.product(*observation_space_discrete_4096))\n",
    "\n",
    "def init_Q_400():\n",
    "    Q_table={}\n",
    "    for i in states_400:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "\n",
    "def init_Q_4096():\n",
    "    Q_table={}\n",
    "    for i in states_4096:\n",
    "        Q_table[i] = {0:0,1:0}\n",
    "    return Q_table\n",
    "         \n",
    "def discretize_400(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_400[i]]\n",
    "        discretizacion[i] = observation_space_discrete_400[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)\n",
    "\n",
    "def discretize_4096(new_state):\n",
    "    # car pos, car vel, pole angle, pole vel\n",
    "    discretizacion=[0]*4\n",
    "    for i in range(len(discretizacion)):\n",
    "        dif = [(abs(x - new_state[i])) for x in observation_space_discrete_4096[i]]\n",
    "        discretizacion[i] = observation_space_discrete_4096[i][dif.index(min(dif))]\n",
    "        \n",
    "    return tuple(discretizacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615aea94",
   "metadata": {},
   "source": [
    "## Estimación de Q con Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6afd3e",
   "metadata": {},
   "source": [
    "### 400 Estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857dcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_400(gamma,alpha):\n",
    "    terminated=False\n",
    "    observation, info = env.reset()\n",
    "    observation=discretize_400(observation)\n",
    "    while not terminated:\n",
    "        \n",
    "        action,_=eps_greedy(Q,observation,0.1)\n",
    "        \n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        observation_=discretize_400(observation_)\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode='human')\n",
    "Q=init_Q_400()\n",
    "policy=dict.fromkeys(states_400, 0)\n",
    "for i in range(0,10):\n",
    "    Q_Learning_400(0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12558f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning_4096(gamma,alpha):\n",
    "    terminated=False\n",
    "    observation, info = env.reset()\n",
    "    observation=discretize_4096(observation)\n",
    "    while not terminated:\n",
    "        \n",
    "        action,_=eps_greedy(Q,observation,0.1)\n",
    "        \n",
    "        observation_, reward, terminated, truncated, info = env.step(action)\n",
    "        observation_=discretize_4096(observation_)\n",
    "        \n",
    "        Q_=key_max(Q[observation_])[1]\n",
    "\n",
    "        Q[observation][action]+=alpha*(reward+gamma*Q_-Q[observation][action])\n",
    "        observation=observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abd7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c95cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "Q=init_Q_4096()\n",
    "policy=dict.fromkeys(states_4096, 0)\n",
    "    \n",
    "for i in tqdm(range(int(1e6))):\n",
    "    Q_Learning_4096(0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state_4096 in states_4096:\n",
    "    policy[state_4096],_=eps_greedy(Q,tuple(state_4096),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "\n",
    "\n",
    "rwds=[]\n",
    "for _ in range(100):\n",
    "    observation, info = env.reset()\n",
    "    r=0\n",
    "    while True:\n",
    "        action = policy[discretize_4096(observation)]\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        r+=reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            rwds.append(r)\n",
    "            break\n",
    "env.close()\n",
    "print(np.mean(rwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabb12ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100 steps ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44036\\1937588419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mvisualkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayered_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         inputs, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1304\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m         )\n\u001b[0;32m   1306\u001b[0m         \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m         )\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2694\u001b[0m                 \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2696\u001b[1;33m                 \u001b[0mexception_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2697\u001b[0m             )\n\u001b[0;32m   2698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\RL2\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    737\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m                             \u001b[1;33m+\u001b[0m \u001b[1;34m\" but got array with shape \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m                             \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                         )\n\u001b[0;32m    741\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected flatten_1_input to have shape (1, 4) but got array with shape (1, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(1,) + env.observation_space.shape),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(2, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=100, visualize=True, verbose=2)\n",
    "\n",
    "visualkeras.layered_view(model)\n",
    "\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71308fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36332\\2881083884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python --version'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NumPy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensorflow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "!python --version\n",
    "print('NumPy', np.__version__)\n",
    "print('Tensorflow', tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dca9137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\juanp\\anaconda3\\lib\\site-packages (2.12.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Input\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88514866-70f6-4ecf-81fc-d7ef7778551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA algorithm\n",
    "def SARSA_algorithm(num_episodes):\n",
    "    allowed_actions=[0,1]\n",
    "    # Variables: alpha, gamma y epsilon.\n",
    "    alpha = 0.3\n",
    "    gamma = 1\n",
    "    epsilon = 0.1\n",
    "    cont = 0\n",
    "    #Para 5000 episodios de entrenamiento\n",
    "    for i in range(num_episodes):\n",
    "        # Inicializa las variables para cada episodio\n",
    "        state=env.reset()\n",
    "        num_steps = 0\n",
    "        state = discretize(state[0])\n",
    "\n",
    "\n",
    "        # Seleccion accion \"a\" de forma epsilon-greedy\n",
    "        if epsilon< np.random.uniform():        \n",
    "            act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "            action = allowed_actions[np.argmax(act_arg)]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Inicia el episodio\n",
    "        continue_episode = True\n",
    "        while continue_episode:        \n",
    "            # Obtengo s'\n",
    "            new_state, reward, done, _,_=env.step(action)\n",
    "            new_state = discretize(new_state)\n",
    "\n",
    "            # Revisa que new_state no sea un estado terminal\n",
    "            if done:\n",
    "                # Valor q(s',a') terminal\n",
    "                q_value_next_step = 0\n",
    "            else:\n",
    "                #Obtengo a' de s' con epsilon greedy\n",
    "                if epsilon< np.random.uniform():        \n",
    "                    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "                    new_action = allowed_actions[np.argmax(act_arg)]\n",
    "                else:\n",
    "                    new_action = env.action_space.sample()\n",
    "\n",
    "                # Valor q(s',a') no terminal\n",
    "                q_value_next_step = Q_table[tuple(new_state),new_action]\n",
    "\n",
    "\n",
    "            # Calculo de actualizacion q(s,a) <- q(s,a) + alpha*(R + gamma*q(s',a') - q(s,a))\n",
    "            Q_table[tuple(state), action] += alpha*(reward + gamma*q_value_next_step - Q_table[tuple(state),action])\n",
    "\n",
    "            # asigna a = a' y s = s'\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "\n",
    "\n",
    "            # Parte que termina el episodio si se llega a algun estado terminal\n",
    "            if done:\n",
    "                continue_episode = False\n",
    "        cont+=1\n",
    "        if cont%100==0:\n",
    "            print(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175e199-6c30-42f9-9445-b1fff097fda4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 400 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc496d6-4697-4c4b-99f3-3fd3d6c38ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d518651-f58f-43e2-a852-ac711092fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(400)\n",
    "SARSA_algorithm(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d541f-311e-4792-8d36-aeda32925164",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_actions= [0,1]\n",
    "prom=[]\n",
    "prom_rand=[]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d842e-c241-4e90-a9d9-6090d2fb5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914b0c8-5082-4b70-85fe-9f08941b6861",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SARSA con 4000 estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971337c3-768a-4d8d-b63f-d2438646e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_states()\n",
    "set_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdedad1-20eb-4d8d-b297-1df3f77f6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_values = num_states(4000)\n",
    "SARSA_algorithm(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0343b9-09a1-4d69-b6ef-493b6207eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prom=[]\n",
    "prom_rand=[]\n",
    "allowed_actions= [0,1]\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    state = state[0]\n",
    "    state = discretize(state)\n",
    "    act_arg = np.array([Q_table[tuple(state), act] for act in range(2)])\n",
    "    action = allowed_actions[np.argmax(act_arg)]\n",
    "    done = False\n",
    "    n=1\n",
    "    while not done:\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "        discrete_state = discretize(new_state)\n",
    "        act_arg = np.array([Q_table[tuple(discrete_state), act] for act in range(2)])\n",
    "        action = allowed_actions[np.argmax(act_arg)]\n",
    "    prom.append(G_pi)\n",
    "print('---'*5)\n",
    "for i in range(100):\n",
    "    G_pi = 0\n",
    "    state =env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, _,_=env.step(action)\n",
    "        G_pi= G_pi + reward\n",
    "    prom_rand.append(G_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d949d9-15ed-4c55-a0ec-90bb45d23a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retorno obtenido con la funcion q:\",sum(prom)/len(prom))\n",
    "print(\"Retorno obtenido de acciones al azar:\",sum(prom_rand)/len(prom_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb52627-a4a1-46ec-9b80-21647b9c7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
